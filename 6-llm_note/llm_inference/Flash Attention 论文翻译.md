## 1ï¼Œä»‹ç»

Transformer æ¨¡å‹ç›®å‰å·²ç»æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†å’Œå›¾åƒåˆ†ç±»ç­‰é¢†åŸŸä¸­ä½¿ç”¨æœ€å¹¿æ³›çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚è™½ç„¶ Transformer æ¨¡å‹å·²ç»å˜å¾—è¶Šæ¥è¶Šå¤§å’Œæ·±ï¼Œä½†æ˜¯å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¾ç„¶è¿˜æ˜¯æœ‰å›°éš¾ï¼Œæœ€ä¸»è¦çš„åŸå› æ˜¯å…¶æ ¸å¿ƒæ¨¡å—-è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é—´å’Œå†…å­˜å¤æ‚åº¦æ˜¯åºåˆ—é•¿åº¦çš„äºŒæ¬¡æ–¹ã€‚ä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯ï¼Œè®©æ³¨æ„åŠ›æ›´å¿«ã€å†…å­˜æ•ˆç‡æ›´é«˜æ˜¯å¦å¯ä»¥å¸®åŠ© Transformer æ¨¡å‹è§£å†³é•¿åºåˆ—çš„è¿è¡Œæ—¶å’Œå†…å­˜æŒ‘æˆ˜ã€‚

> $Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

è®¸å¤šè¿‘ä¼¼è‡ªæ³¨æ„åŠ›æ–¹æ³•æ—¨åœ¨é™ä½æ³¨æ„åŠ›çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œæ¯”å¦‚ç¨€ç–é€¼è¿‘ sparse-approximation [51, 74]ã€ä½ç§©é€¼è¿‘ low-rank approximation [12, 50, 84]ä»¥åŠå®ƒä»¬çš„ç»„åˆ[3, 9, 92]ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å°† Flops é™ä½åˆ°ä¸åºåˆ—é•¿åº¦çº¿æ€§æˆ–æ¥è¿‘çº¿æ€§çš„æ°´å¹³ï¼Œä½†å¹¶æœªåœ¨å®é™…è¿è¡Œæ—¶é—´ä¸Šæ˜¾è‘—æé€Ÿï¼Œä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸æ¯”ä¹Ÿæ²¡æœ‰è¢«å¹¿æ³›é‡‡ç”¨ã€‚ä¸€ä¸ªä¸»è¦åŸå› æ˜¯å®ƒä»¬ä¾§é‡äºå‡å°‘æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPs ä¸å®é™…è¿è¡Œæ—¶é—´ä¸ä¸€å®šç›¸å…³ï¼‰ï¼Œè€Œå¿½ç•¥å†…å­˜è®¿é—®ä»£ä»·ï¼ˆ`MAC`ï¼‰ã€‚

> å‡å°‘ç®—å­å¯¹ FLOPs ä¸ä¸€å®šä¼šå‡å°‘è¿è¡Œæ—¶é—´ï¼Œè¿˜è¦å…³æ³¨å…¶å†…å­˜è®¿é—®ä»£ä»· MACï¼Œè¿™ä¸ªç»“è®ºåœ¨ shufflenetv2 è®ºæ–‡å°±æ——å¸œé²œæ˜æŒ‡å‡ºäº†ã€‚

åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªæ–°åŸåˆ™-**è®©æ³¨æ„åŠ›ç®—æ³•å…·å¤‡ IO æ„ŸçŸ¥æ€§** IO-aware [1]ï¼Œå³è€ƒè™‘å¯¹ä¸åŒé€Ÿåº¦çš„å†…å­˜çš„è¯»å–å’Œå†™å…¥æ“ä½œï¼ˆGPU èŠ¯ç‰‡ä¸Šçš„å¿«é€Ÿ SRAM å’Œç›¸å¯¹è¾ƒæ…¢çš„ GPU é«˜å¸¦å®½å†…å­˜ï¼‰ã€‚**åœ¨ç°ä»£ GPU ä¸­ï¼Œè®¡ç®—é€Ÿåº¦å·²ç»è¶…è¿‡äº†å†…å­˜è¯»å†™é€Ÿåº¦ [61, 62, 63]ï¼Œè€Œ Transformers ä¸­çš„å¤§å¤šæ•°æ“ä½œéƒ½å—åˆ°å†…å­˜è®¿é—®çš„é™åˆ¶[43]ã€‚**

![figure](../../images/flash_attention/figure1.png)

å¯¹äºç±»ä¼¼äºå†…å­˜å—é™æ“ä½œçš„ä»»åŠ¡ï¼ŒIO æ„ŸçŸ¥ç®—æ³•éå¸¸é‡è¦ï¼Œå› ä¸ºæ•°æ®çš„è¯»å–å’Œå†™å…¥ä¼šå ç”¨è¿è¡Œæ—¶çš„å¤§éƒ¨åˆ†æ—¶é—´ï¼Œæ¯”å¦‚æ•°æ®åº“è¿æ¥[71]ã€å›¾åƒå¤„ç†[70]ã€æ•°å€¼çº¿æ€§ä»£æ•°[4]ç­‰ç­‰[40, 85]ã€‚ç„¶è€Œï¼Œé€šå¸¸ç”¨äºæ·±åº¦å­¦ä¹ çš„å¸¸è§ Python æ¥å£ï¼Œå¦‚ PyTorch å’Œ Tensorflowï¼Œä¸å…è®¸å¯¹å†…å­˜è®¿é—®è¿›è¡Œç²¾ç»†æ§åˆ¶ã€‚

æˆ‘ä»¬æå‡ºäº† FlashAttentionï¼Œä¸€ç§æ–°çš„æ³¨æ„åŠ›ç®—æ³•ï¼Œå®ƒå¯ä»¥åœ¨æå°‘çš„å†…å­˜è®¿é—®æ¬¡æ•°ä¸‹è®¡ç®—å‡ºå‡†ç¡®çš„æ³¨æ„åŠ›ã€‚æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯**é¿å…ä» HBMï¼ˆæ˜¾å­˜ï¼‰è¯»å–å’Œå†™å…¥æ³¨æ„åŠ›çŸ©é˜µï¼Œ**è¿™éœ€è¦ï¼ˆiï¼‰åœ¨ä¸è®¿é—®æ•´ä¸ªè¾“å…¥çš„æƒ…å†µä¸‹è®¡ç®— softmax ç¼©å‡ï¼ˆreductionï¼‰ï¼ˆiiï¼‰ä¸å­˜å‚¨ç”¨äºåå‘ä¼ é€’çš„å¤§å‹ä¸­é—´æ³¨æ„åŠ›çŸ©é˜µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç§å·²ç»è¢«å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯æ¥è§£å†³è¿™äº›æŒ‘æˆ˜**ï¼š**

1. æˆ‘ä»¬é‡æ–°æ„é€ äº†æ³¨æ„åŠ›è®¡ç®—ï¼Œå°†è¾“å…¥åˆ†å‰²æˆå—å¹¶å¯¹è¾“å…¥å—è¿›è¡Œå¤šæ¬¡å¤„ç†ï¼Œé€æ­¥æ‰§è¡Œsoftmax ç¼©å‡ï¼ˆreductionï¼‰ï¼ˆä¹Ÿç§°ä¸ºåˆ‡ç‰‡ï¼‰ã€‚è¿™æ ·å¯ä»¥åœ¨ä¸éœ€è¦è®¿é—®æ•´ä¸ªè¾“å…¥çš„æƒ…å†µä¸‹è®¡ç®—softmaxå½’çº¦ã€‚
2. æˆ‘ä»¬åœ¨å‰å‘ä¼ æ’­ä¸­å­˜å‚¨äº†softmax å½’ä¸€åŒ–å› å­ï¼Œä»¥ä¾¿åœ¨åå‘ä¼ æ’­ä¸­å¿«é€Ÿé‡æ–°è®¡ç®—èŠ¯ç‰‡ä¸Šçš„æ³¨æ„åŠ›ã€‚è¿™æ¯”ä»HBMè¯»å–ä¸­é—´æ³¨æ„åŠ›çŸ©é˜µçš„æ ‡å‡†æ–¹æ³•æ›´å¿«ã€‚

æˆ‘ä»¬ä½¿ç”¨ CUDA å®ç°äº†FlashAttentionï¼Œä»¥å®ç°å¯¹å†…å­˜è®¿é—®çš„ç²¾ç»†æ§åˆ¶ï¼Œå¹¶å°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œèåˆåˆ°ä¸€ä¸ªGPUå†…æ ¸ä¸­ã€‚å³ä½¿ç”±äºé‡æ–°è®¡ç®—è€Œå¢åŠ äº†æµ®ç‚¹è¿ç®—é‡ï¼Œç”±äºå¤§å¤§å‡å°‘äº† HBM è®¿é—®æ¬¡æ•°ï¼Œæˆ‘ä»¬çš„ç®—æ³•æ¯”æ ‡å‡†çš„æ³¨æ„åŠ›æ›´å¿«ï¼ˆä¾‹å¦‚ï¼Œåœ¨GPT-2ä¸Šé«˜è¾¾7.6å€ï¼Œå‚è§å›¾1å³ä¾§ï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨çš„å†…å­˜æ›´å°‘ï¼Œä¸åºåˆ—é•¿åº¦å‘ˆçº¿æ€§å…³ç³»ã€‚

æˆ‘ä»¬åˆ†æäº† FlashAttention çš„ IO å¤æ‚åº¦ [1]ï¼Œè¯æ˜å®ƒéœ€è¦ $O(N^2d^2M^{-1})$ æ¬¡ HBM è®¿é—®ï¼Œå…¶ä¸­ $d$ æ˜¯å¤´éƒ¨å°ºå¯¸ï¼Œ$M$ æ˜¯ SRAM çš„å¤§å°ï¼Œè€Œæ ‡å‡†æ³¨æ„åŠ›çš„å¤æ‚åº¦ $O(N*d + N^2)$ã€‚  å¯¹äºå…¸å‹çš„ $d$ å’Œ $M$ å€¼ï¼ŒFlashAttention ç›¸å¯¹äºæ ‡å‡†æ³¨æ„åŠ›éœ€è¦çš„ HBM è®¿é—®æ¬¡æ•°è¦å°‘å¾—å¤šï¼ˆæœ€å¤šå°‘ 9 å€ï¼Œå¦‚å›¾ 2 æ‰€ç¤º)ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¸‹ç•Œï¼Œè¡¨æ˜åœ¨æ‰€æœ‰SRAM å¤§å°ä¸Šï¼Œæ²¡æœ‰ä»»ä½•ç²¾ç¡®çš„æ³¨æ„åŠ›ç®—æ³•å¯ä»¥åœ¨ HBM è®¿é—®æ¬¡æ•°ä¸Šæ¸è¿‘åœ°æ”¹è¿›ã€‚

> å…³äºæ—¶é—´å¤æ‚åº¦çš„è®¡ç®—ï¼Œé™„å½•æœ‰å…¬å¼æ¨å¯¼ã€‚

æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒFlashAttention å¯ä»¥ä½œä¸ºä¸€ä¸ªæœ‰ç”¨çš„åŸè¯­ï¼ˆuseful primitiveï¼‰ï¼Œé€šè¿‡å…‹æœå†…å­˜è®¿é—®å¼€é”€çš„é—®é¢˜æ¥å®ç°è¿‘ä¼¼æ³¨æ„ç®—æ³•çš„æ½œåŠ›ã€‚ ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬å®ç°äº†å—ç¨€ç– FlashAttentionï¼Œè¿™æ˜¯ä¸€ç§ç¨€ç–æ³¨æ„åŠ›ç®—æ³•ï¼Œæ¯” FlashAttention å¿« 2-4 å€ï¼Œå¯æ‰©å±•åˆ° 64k çš„åºåˆ—é•¿åº¦ã€‚ æˆ‘ä»¬è¯æ˜å—ç¨€ç– FlashAttention å…·æœ‰æ¯” FlashAttention æ›´å¥½çš„ IO å¤æ‚æ€§ï¼Œå…¶ç¨‹åº¦ä¸ç¨€ç–åº¦æˆæ­£æ¯”ã€‚ æˆ‘ä»¬åœ¨ç¬¬ 5 èŠ‚ä¸­è®¨è®ºå¯¹å…¶ä»–æ“ä½œçš„è¿›ä¸€æ­¥æ‰©å±•ï¼ˆå¤š GPU æ³¨æ„åŠ›ã€å†…æ ¸å›å½’ã€å—ç¨€ç–çŸ©é˜µä¹˜æ³•ï¼‰ã€‚æˆ‘ä»¬å°† FlashAttention ç®—æ³•ä»£ç å¼€æºï¼Œä»¥ä¾¿æ›´è½»æ¾åœ°åœ¨æ­¤åŸè¯­ä¸Šè¿›è¡Œæ„å»ºã€‚

æˆ‘ä»¬é€šè¿‡å®è¯éªŒè¯äº†FlashAttentionå¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œå¹¶é€šè¿‡å»ºæ¨¡æ›´é•¿çš„ä¸Šä¸‹æ–‡æ¥æé«˜æ¨¡å‹è´¨é‡ã€‚ä¸ä¹‹å‰çš„æ³¨æ„åŠ›å®ç°ç›¸æ¯”ï¼Œæˆ‘ä»¬è¿˜å¯¹ FlashAttention å’Œå—ç¨€ç– FlashAttention çš„è¿è¡Œæ—¶å’Œå†…å­˜å ç”¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚

1. **æ›´å¿«çš„æ¨¡å‹è®­ç»ƒ**ã€‚æˆ‘ä»¬è®­ç»ƒçš„ BERT-largeï¼ˆåºåˆ—é•¿åº¦ 512ï¼‰æ¯” MLPerf 1.1 [58] çš„è®­ç»ƒé€Ÿåº¦è®°å½•å¿«15%ï¼Œæ¯” HuggingFace [87] å’Œ Megatron-LM [77]çš„åŸºå‡†å®ç°å¿« `3` å€ï¼Œè€Œå¯¹äºé•¿åºåˆ— arenaï¼ˆåºåˆ—é•¿åº¦ 1K-4Kï¼‰ï¼Œé€Ÿåº¦æ¯”åŸºçº¿å¿«2.4å€ã€‚
2. **æ›´é«˜è´¨é‡çš„æ¨¡å‹**ã€‚FlashAttentionå¯ä»¥å°† Transformers æ‰©å±•åˆ°æ›´é•¿çš„åºåˆ—ï¼Œä»è€Œæé«˜æ¨¡å‹è´¨é‡å¹¶å®ç°æ–°çš„åŠŸèƒ½ã€‚
3. **attention çš„åŸºå‡†æµ‹è¯•**ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFlashAttention åœ¨å¸¸è§åºåˆ—é•¿åº¦ï¼ˆä»128 åˆ° 2Kï¼‰ä¸Šæ¯”æ ‡å‡†çš„æ³¨æ„åŠ›å®ç°å¿« 3 å€ï¼Œå¯æ‰©å±•åˆ° 64Kã€‚åœ¨åºåˆ—é•¿åº¦æœ€å¤§ä¸º512 çš„æƒ…å†µä¸‹ï¼ŒFlashAttention æ—¢æ¯”ä»»ä½•ç°æœ‰çš„æ³¨æ„åŠ›æ–¹æ³•æ›´å¿«ï¼Œä¹Ÿæ›´èŠ‚çœå†…å­˜ï¼Œä½†å¯¹äºè¶…è¿‡ `1K` çš„åºåˆ—é•¿åº¦ï¼Œä¸€äº›è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒLinformerï¼‰å¼€å§‹å˜å¾—æ›´å¿«ã€‚å¦ä¸€æ–¹é¢ï¼Œå—ç¨€ç– FlashAttention æ¯”æˆ‘ä»¬æ‰€çŸ¥çš„æ‰€æœ‰ç°æœ‰è¿‘ä¼¼æ³¨æ„åŠ›æ–¹æ³•éƒ½æ›´å¿«ã€‚

## 2ï¼ŒèƒŒæ™¯

æˆ‘ä»¬æä¾›äº†ä¸€äº›å…³äºç°ä»£ç¡¬ä»¶ï¼ˆGPUï¼‰ä¸Šå¸¸è§æ·±åº¦å­¦ä¹ æ“ä½œçš„æ€§èƒ½ç‰¹å¾çš„èƒŒæ™¯ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜**æè¿°äº†æ³¨æ„åŠ›çš„æ ‡å‡†å®ç°æ–¹å¼**ã€‚

### 2.1ï¼Œç¡¬ä»¶æ€§èƒ½

1. **GPU å†…å­˜å±‚æ¬¡ç»“æ„**ã€‚ï¼ˆå¦‚å›¾1å·¦ä¾§æ‰€ç¤ºï¼‰åŒ…æ‹¬å¤šç§ä¸åŒå¤§å°å’Œé€Ÿåº¦çš„å†…å­˜å½¢å¼ï¼Œè¾ƒå°çš„å†…å­˜é€Ÿåº¦æ›´å¿«ã€‚ä»¥A100 GPUä¸ºä¾‹ï¼Œå®ƒå…·æœ‰40-80GBçš„é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰ï¼Œ**å¸¦å®½ä¸º 1.5-2.0 TB/s**ï¼Œå¹¶ä¸”æ¯ä¸ª 108 ä¸ªæµå¼å¤šå¤„ç†å™¨éƒ½æœ‰ 192KB çš„èŠ¯ç‰‡ä¸ŠSRAMï¼Œå…¶å¸¦å®½çº¦ä¸º19TB/s [44, 45]ã€‚èŠ¯ç‰‡ä¸Šçš„SRAMæ¯”HBMå¿«ä¸€ä¸ªæ•°é‡çº§ï¼Œä½†åœ¨å¤§å°ä¸Šè¦å°å¾—å¤šã€‚ç”±äºè®¡ç®—ç›¸å¯¹äºå†…å­˜é€Ÿåº¦æ›´å¿«[61, 62, 63]ï¼Œæ“ä½œè¶Šæ¥è¶Šå—åˆ°å†…å­˜ï¼ˆHBMï¼‰è®¿é—®çš„é™åˆ¶ã€‚å› æ­¤ï¼Œåˆ©ç”¨å¿«é€Ÿçš„SRAMå˜å¾—æ›´åŠ é‡è¦ã€‚
2. **æ‰§è¡Œæ¨¡å‹ï¼ˆ**Execution Modelï¼‰ã€‚GPUæœ‰å¤§é‡çº¿ç¨‹æ¥æ‰§è¡Œæ“ä½œï¼ˆç§°ä¸ºå†…æ ¸ï¼‰ã€‚æ¯ä¸ªå†…æ ¸å°†è¾“å…¥ä»HBMåŠ è½½åˆ°å¯„å­˜å™¨å’ŒSRAMä¸­ï¼Œè¿›è¡Œè®¡ç®—ï¼Œç„¶åå°†è¾“å‡ºå†™å…¥HBMã€‚
3. **æ€§èƒ½ç‰¹å¾**ï¼ˆPerformance characteristics.ï¼‰ã€‚æ ¹æ®è®¡ç®—å’Œå†…å­˜è®¿é—®çš„å¹³è¡¡ï¼Œæ“ä½œå¯ä»¥è¢«åˆ†ç±»ä¸º**è®¡ç®—å—é™æˆ–å†…å­˜å—é™**ã€‚è¿™é€šå¸¸é€šè¿‡**ç®—æœ¯å¼ºåº¦**[85] æ¥è¡¡é‡ï¼Œå³æ¯å­—èŠ‚å†…å­˜è®¿é—®çš„ç®—æœ¯æ“ä½œæ•°ã€‚

- **è®¡ç®—å—é™**ï¼šæ“ä½œæ‰€éœ€çš„æ—¶é—´å–å†³äºæœ‰å¤šå°‘ç®—æœ¯æ“ä½œï¼Œè€Œè®¿é—® HBM çš„æ—¶é—´è¦å°å¾—å¤šã€‚å…¸å‹çš„ç¤ºä¾‹åŒ…æ‹¬å…·æœ‰**å¤§å†…éƒ¨ç»´åº¦çš„çŸ©é˜µä¹˜æ³•**å’Œå…·æœ‰å¤§é‡é€šé“çš„**å·ç§¯æ“ä½œ**ã€‚
- **å†…å­˜å—é™**ï¼šæ“ä½œæ‰€éœ€çš„æ—¶é—´å–å†³äºå†…å­˜è®¿é—®çš„æ¬¡æ•°ï¼Œè€Œåœ¨è®¡ç®—æ–¹é¢æ‰€èŠ±è´¹çš„æ—¶é—´è¦å°å¾—å¤šã€‚ç¤ºä¾‹åŒ…æ‹¬å¤§å¤šæ•°å…¶ä»–æ“ä½œï¼šé€å…ƒç´ æ“ä½œï¼ˆä¾‹å¦‚æ¿€æ´»å‡½æ•° activationã€ä¸¢å¼ƒæ“ä½œ dropoutï¼‰ä»¥åŠè§„çº¦æ“ä½œï¼ˆä¾‹å¦‚æ±‚å’Œ  sumã€softmaxã€æ‰¹å½’ä¸€åŒ– batch normã€å±‚å½’ä¸€åŒ– layer normï¼‰ã€‚

**å†…æ ¸èåˆ**ã€‚**åŠ é€Ÿå†…å­˜å—é™æ“ä½œçš„æœ€å¸¸è§æ–¹æ³•æ˜¯å†…æ ¸èåˆ**ï¼šå¦‚æœå¯¹ç›¸åŒè¾“å…¥åº”ç”¨äº†å¤šä¸ªæ“ä½œï¼Œé‚£ä¹ˆå¯ä»¥ä» HBM åŠ è½½ä¸€æ¬¡è¾“å…¥ï¼Œè€Œä¸æ˜¯æ¯ä¸ªæ“ä½œéƒ½åŠ è½½å¤šæ¬¡ã€‚ç¼–è¯‘å™¨å¯ä»¥è‡ªåŠ¨èåˆè®¸å¤šé€å…ƒç´ æ“ä½œ[53, 65, 75]ã€‚ç„¶è€Œï¼Œåœ¨æ¨¡å‹è®­ç»ƒçš„èƒŒæ™¯ä¸‹ï¼Œä¸­é—´å€¼ä»ç„¶éœ€è¦å†™å…¥HBMä»¥ä¾›åå‘ä¼ æ’­ä¿å­˜ï¼Œé™ä½äº†æœ´ç´ å†…æ ¸èåˆçš„æ•ˆæœã€‚

### 2.2ï¼Œæ ‡å‡† attention å®ç°

ç»™å®šè¾“å…¥åºåˆ— $Q, K, V \in R^{N\times d}$ï¼Œå…¶ä¸­ $N$ æ˜¯è¾“å…¥åºåˆ—çš„é•¿åº¦çš„ï¼Œ$d$ æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤´çš„é•¿åº¦ã€‚æˆ‘ä»¬æƒ³è¦è®¡ç®—æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºçŸ©é˜µ $O \in R^{N\times d}$: 

$$S = QK^T \in R^{N\times N}, P = softmax(S) \in R^{N\times N}, O = PV\in R^{N\times d}$$ã€‚

è¿™é‡Œçš„ softmax æ˜¯æŒ‰è¡Œåº”ç”¨çš„ã€‚

æ ‡å‡†çš„æ³¨æ„åŠ›å®ç°ä¼šå°†çŸ©é˜µ S å’Œ P å†™å…¥åˆ° HBMï¼Œè¿™éœ€è¦ O(N^2)çš„å†…å­˜ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼ŒN å’Œ d ç›¸å¯¹è¾ƒå¤§ï¼ˆä¾‹å¦‚ï¼Œå¯¹äºGPT2ï¼ŒN = 1024ï¼Œd = 64ï¼‰ã€‚æˆ‘ä»¬åœ¨ç®—æ³• 0 ä¸­æè¿°äº†æ ‡å‡†çš„æ³¨æ„åŠ›å®ç°ã€‚ç”±äºä¸€äº›æˆ–å¤§å¤šæ•°æ“ä½œæ˜¯å†…å­˜å—é™çš„ï¼ˆä¾‹å¦‚ softmaxï¼‰ï¼Œå¤§é‡çš„å†…å­˜è®¿é—®ä¼šå¯¼è‡´å¢™é’Ÿæ—¶é—´ï¼ˆwall-clock timeï¼‰å˜æ…¢ã€‚

è¿™ä¸ªé—®é¢˜åœ¨åº”ç”¨äºæ³¨æ„åŠ›çŸ©é˜µçš„å…¶ä»–é€å…ƒç´ æ“ä½œæ—¶ä¼šåŠ å‰§ï¼Œä¾‹å¦‚åº”ç”¨äº S çš„æ©ç  mask æˆ–åº”ç”¨äº P çš„ä¸¢å¼ƒ dropout æ“ä½œã€‚å› æ­¤ï¼Œå·²ç»æœ‰å¾ˆå¤šå°è¯•å°†å¤šä¸ªé€å…ƒç´ æ“ä½œèåˆåœ¨ä¸€èµ·ï¼Œæ¯”å¦‚å°†æ©ç ä¸ softmax èåˆåœ¨ä¸€èµ·[77]ã€‚

åœ¨ç¬¬3.2èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºæ ‡å‡†çš„æ³¨æ„åŠ›å®ç°åœ¨åºåˆ—é•¿åº¦ N æ–¹é¢æ‰§è¡Œ HBM è®¿é—®ï¼ŒåŒæ—¶æ¯”è¾ƒæ ‡å‡†æ³¨æ„åŠ›å’Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆFlashAttentionï¼‰çš„ FLOPs æ•°é‡å’Œ HBM è®¿é—®æ•°é‡ã€‚

![æ ‡å‡† attentionç®—æ³•](../../images/flash_attention/standard_attention_imple.png)

## 3ï¼ŒFlashAttention: Algorithm, Analysis, and Extensions

### 3.1ï¼ŒAn Efficient Attention Algorithm With Tiling and Recomputation

ç»™å®š HBM ä¸Šçš„è¾“å…¥ $Q,K,V \in R^{N\times d}$ï¼Œç›®æ ‡æ˜¯è®¡ç®—æ³¨æ„åŠ›è¾“å‡º $O \in R^{N\times d}$ å¹¶å°†å…¶å†™å…¥åˆ° HBM ä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯å‡å°‘ HBM çš„è®¿é—®æ¬¡æ•°ã€‚

æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§æŠ€æœ¯ï¼ˆ**tilingï¼Œrecomputation**ï¼‰æ¥å…‹æœåœ¨ sub-quadratic HBM è®¿é—®ä¸­è®¡ç®—ç²¾ç¡®æ³¨æ„åŠ›çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨ç®—æ³• 1ä¸­æè¿°äº†è¿™ä¸¤ç§æŠ€æœ¯ã€‚ä¸»è¦æ€æƒ³å°±æ˜¯å°† $Q,K,V$ çŸ©é˜µåˆ’åˆ†æˆå—ï¼Œä»æ…¢é€Ÿ HBM åŠ è½½åˆ°å¿«é€Ÿ SRAM ä¸­ï¼Œç„¶ååˆ†åˆ«è®¡ç®—è¿™äº›å—çš„æ³¨æ„åŠ›è¾“å‡ºï¼Œæœ€åï¼Œå°†æ¯ä¸ªå—çš„è¾“å‡ºæŒ‰æ­£ç¡®çš„å½’ä¸€åŒ–å› å­ç¼©æ”¾ä¹‹åç›¸åŠ ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†æ­£ç¡®çš„ç»“æœã€‚

**1ï¼ŒTiling.** æˆ‘ä»¬**æŒ‰å—è®¡ç®—æ³¨æ„åŠ›ã€‚**Softmax å°† K çš„åˆ—è€¦åˆåœ¨ä¸€èµ·ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç¼©æ”¾ [51, 60, 66] æ¥åˆ†è§£å¤§çš„ softmaxã€‚ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå‘é‡ $x\in R^B$ çš„ softmax ç»“æœè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

$$m(x) := \underset{i}{max} \; x_i, \ f(x) := [e^{x_1 - m(x)} ... e^{x_B - m(x)}], \ell(x) := \sum_i f(x)_i, softmax(x) := \frac{f(x)}{\ell(x)} .$$

å¯¹äºå‘é‡ $x^{(1)}, x^{(2)} \in R^{B}$ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†è§£ï¼ˆdecomposeï¼‰è®¡ç®—ä¸¤ä¸ªå‘é‡ç»„åˆç»“æœçš„ softmaxï¼Œå³ $softmax(x)\ , x = x^{(1)}, x^{(2)} \in R^{B} \in R^{2B}$ã€‚

$$m(x) := m([x^{(1)} x^{(2)}]) = max(m(x^{(1)}), m(x^{(2)})), f(x) := [e^{m(x^{(1)}) - m(x)} f(x^{(1)}) e^{m(x^{(2)}) - m(x)} f(x^{(2)}) ],$$

$$\ell(x) = \ell([x^{(1)} x^{(2)}]) = [e^{m(x^{(1)}) - m(x)} \ell(x^{(1)}) e^{m(x^{(2)}) - m(x)} \ell(x^{(2)}) ],softmax(x) = \frac{f(x)}{\ell(x)}$$

> è¿™å…¬å¼æ‹†è§£çš„æœ‰ç‚¹éš¾æ‡‚å•Šã€‚

å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è·Ÿè¸ªä¸€äº›é¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯ $(m(x), \ell(x))$ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€æ¬¡è®¡ç®—ä¸€ä¸ªå—çš„ softmaxã€‚**å› æ­¤ï¼Œæˆ‘ä»¬å°†è¾“å…¥ $Qã€Kã€V$ åˆ†æˆå—ï¼ˆç®—æ³•1ç¬¬3è¡Œï¼‰ï¼ŒåŒæ—¶è®¡ç®— softmax å€¼å’Œé¢å¤–çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç®—æ³•1ç¬¬10è¡Œï¼‰ï¼Œç„¶åç»„åˆç»“æœï¼ˆç®—æ³•1ç¬¬12è¡Œï¼‰**ã€‚

**2ï¼ŒRecomputation.** æˆ‘ä»¬çš„ç›®æ ‡ä¹‹ä¸€æ˜¯ä¸å­˜å‚¨ $O(N^2)$ çš„ä¸­é—´å€¼ç”¨æ¥åå‘ä¼ æ’­ã€‚åå‘ä¼ æ’­é€šå¸¸éœ€è¦çŸ©é˜µ $S, P \in R^{N\times N}$ï¼Œç”¨æ¥è®¡ç®— $Q, K, V$ çš„æ¢¯åº¦ã€‚ä½†æ˜¯ï¼Œé€šè¿‡å­˜å‚¨è¾“å‡º $O$ å’Œ $softmax$ å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯ $(m, \ell)$ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ `SRAM` ä¸­çš„ $Qã€Kã€V$ å—çš„å‘åä¼ é€’ä¸­è½»æ¾é‡æ–°è®¡ç®—æ³¨æ„åŠ›çŸ©é˜µ $S$ å’Œ $P$ã€‚è¿™ç§æ–¹æ³•ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§é€‰æ‹©æ€§çš„æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆ gradient checkpointing [10, 34]ï¼‰ã€‚è™½ç„¶**æ¢¯åº¦æ£€æŸ¥ç‚¹å·²ç»è¢«æå‡ºæ¥å‡å°‘æ‰€éœ€çš„æœ€å¤§å†…å­˜**[66]ï¼Œä½†æ‰€æœ‰çš„å®ç°ï¼ˆæˆ‘ä»¬çŸ¥é“çš„ï¼‰éƒ½å¿…é¡»åœ¨é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå³ä½¿æœ‰æ›´å¤šçš„ FLOPsï¼Œæˆ‘ä»¬çš„é‡æ–°è®¡ç®—ä¹Ÿä¼šç”±äº HBM è®¿é—®æ¬¡æ•°çš„å‡å°‘è€ŒåŠ é€Ÿå‘åä¼ é€’ã€‚å®Œæ•´çš„åå‘ä¼ æ’­æè¿°åœ¨é™„å½•Bä¸­ã€‚

å®ç°ç»†èŠ‚ï¼š**å†…æ ¸èåˆ**ã€‚`Tiling` ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸€ä¸ª CUDA å†…æ ¸ä¸­å®ç°æˆ‘ä»¬çš„ç®—æ³•ï¼Œä» HBM åŠ è½½è¾“å…¥ï¼Œæ‰§è¡Œæ‰€æœ‰è®¡ç®—æ­¥éª¤ï¼ˆçŸ©é˜µä¹˜æ³•ã€softmaxã€å¯é€‰çš„æ©ç å’Œä¸¢å¼ƒã€çŸ©é˜µä¹˜æ³•ï¼‰ï¼Œç„¶åå°†ç»“æœå†™å› HBMï¼ˆé™„å½•Bä¸­æœ‰æ©ç å’Œä¸¢å¼ƒï¼‰ã€‚è¿™é¿å…äº†åå¤ä» HBM è¯»å–å’Œå†™å…¥è¾“å…¥å’Œè¾“å‡ºçš„æ“ä½œã€‚

flash attention ç®—æ³•å®ç°æ­¥éª¤å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![flash attention ç®—æ³•æ­¥éª¤](../../images/flash_attention/flash_attention_algorithm1.png)

ã€å®šç† 1ã€‘ ç®—æ³• 1 æ³¨æ„åŠ›è¾“å‡ºçŸ©é˜µ $O = softmax(QK^T)V$ è¦æ±‚ $O(N^2d)$ çš„ FLOPsï¼Œå¹¶ä¸”é™¤äº†è¾“å…¥å’Œè¾“å‡ºå†…å­˜ä¹‹å¤–ï¼Œéœ€è¦é¢å¤–çš„ $O(N)$ å†…å­˜ã€‚

### 3.2ï¼ŒAnalysis: IO Complexity of FlashAttentio

æˆ‘ä»¬åˆ†æäº† FlashAttention çš„ `IO` å¤æ‚æ€§ï¼Œç»“æœæ˜¾ç¤ºï¼Œä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸æ¯” HBM è®¿é—®æ¬¡æ•°æ˜¾è‘—å‡å°‘ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªä¸‹ç•Œï¼Œè¯æ˜æ²¡æœ‰ç²¾ç¡®çš„æ³¨æ„åŠ›ç®—æ³•å¯ä»¥åœ¨æ‰€æœ‰ SRAM å¤§å°ä¸Šæ¸è¿‘åœ°æ”¹å–„ HBM è®¿é—®æ¬¡æ•°ã€‚è¯æ˜åœ¨é™„å½• C ä¸­ã€‚

![figure2](../../images/flash_attention/figure2.png)

ã€å®šç† 2ã€‘å‡è®¾ $N$ æ˜¯è¾“å…¥åºåˆ—çš„é•¿åº¦ï¼Œ$d$ æ˜¯æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œ$M$ æ˜¯ `SRAM` å¤§å°ï¼Œä¸” $d \leq M\leq Nd$ã€‚æ ‡å‡† attention çš„ `HBM` è®¿é—®æ¬¡æ•°æ˜¯ $O(Nd+N^2)$ï¼Œè€Œ flashattention åªéœ€è¦ $O(N^2d^2M^{-1})$ã€‚

å¯¹äºå…¸å‹å€¼çš„æƒ…å†µï¼Œ$d$ (64 -128)ï¼Œ$M$ å¤§æ¦‚æ˜¯ 100KBï¼Œå¾ˆæ˜æ˜¾ $d^2$ è¿œè¿œå°äº $M$ã€‚å› æ­¤ï¼ŒFlashAttention æ‰€éœ€çš„ HBM è®¿é—®æ¬¡æ•°æ¯”æ ‡å‡†å®ç°å°‘å¾ˆå¤šå€ï¼Œè¿™ä¼šå¸¦æ¥**æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜å ç”¨**ï¼Œæˆ‘ä»¬åœ¨ 4.3 èŠ‚ä¸­å¯¹æ­¤å®šç†è¿›è¡Œäº†å®éªŒéªŒè¯ã€‚

 è¯æ˜çš„ä¸»è¦æ€æƒ³æ˜¯ï¼Œå‡è®¾ `SRAM` å¤§å°ä¸º $M$ï¼Œåˆ™å¯ä»¥åŠ è½½ $K, V$ å¤§å°ä¸º $\Theta (M)$ çš„å—ï¼ˆç®—æ³• 1 ç¬¬ 6 è¡Œï¼‰ã€‚å¯¹äºæ¯ä¸ª $K$ å’Œ $V$ å—ï¼Œæˆ‘ä»¬è¿­ä»£æ‰€æœ‰çš„ $Q$ å—ï¼ˆç®—æ³• 1 ç¬¬ 8 è¡Œï¼‰æ¥è®¡ç®—ä¸­é—´å€¼ï¼Œè¿™å¯¼è‡´äº† $\Theta(NdM^{-1})$ ä¼ é€’åˆ° $Q$ã€‚æ¯æ¬¡ä¼ é€’ï¼ˆ`pass`ï¼‰éƒ½ä¼šåŠ è½½ $\Theta(N d)$ ä¸ªå…ƒç´ ï¼Œç›¸å½“äº $\Theta (N^2d^2M^{-1})$ HBM è®¿é—®æ¬¡æ•°ã€‚æˆ‘ä»¬åŒæ ·è¯æ˜äº†æ ‡å‡†æ³¨æ„åŠ›ç®—æ³•çš„åå‘ä¼ æ’­éœ€è¦ $Nd + N^2$ HBM è®¿é—®ï¼Œè€Œ FlashAttention çš„åå‘ä¼ æ’­éœ€è¦ $\Theta(N^2d^2M^{-1})$ HBM è®¿é—®ï¼ˆé™„å½• Bï¼‰ã€‚

æˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªä¸‹ç•Œï¼š**åœ¨è®¡ç®—ç²¾ç¡®æ³¨æ„åŠ›æ—¶ï¼Œä¸èƒ½åœ¨æ‰€æœ‰ $ğ‘€$ï¼ˆSRAMå¤§å°ï¼‰çš„å€¼ä¸Šæ¸è¿‘åœ°æ”¹å–„ HBMè®¿é—®æ¬¡æ•°**ã€‚

ã€å®šç† 3ã€‘å‡è®¾ $N$ æ˜¯è¾“å…¥åºåˆ—çš„é•¿åº¦ï¼Œ$d$ æ˜¯æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œ$M$ æ˜¯ `SRAM` å¤§å°ï¼Œä¸” $d \leq M\leq Nd$ã€‚ä¸å­˜åœ¨ä¸€ç§ç®—æ³•å¯ä»¥åœ¨èŒƒå›´ $[d, Nd]$ ä¸­çš„æ‰€æœ‰ $M$ ä¸Šä½¿ç”¨ $O(N^2d^2M^{âˆ’1})$ HBM è®¿é—®æ¥è®¡ç®—ç²¾ç¡®çš„æ³¨æ„åŠ›ã€‚

è¿™ä¸ªè¯æ˜ä¾èµ–äºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå¯¹äº$M = \Theta(Nd)$ï¼Œä»»ä½•ç®—æ³•éƒ½å¿…é¡»æ‰§è¡Œ $\Omega(N^2d^2M^{âˆ’1}) = \Omega(Nd)$ HBM è®¿é—®ã€‚è¿™ç§å…³äº $M$ å­èŒƒå›´çš„ä¸‹ç•Œåœ¨æµå¼ç®—æ³•æ–‡çŒ®ä¸­å¾ˆå¸¸è§[88]ã€‚æˆ‘ä»¬å°†è¯æ˜å…³äº $M$ çš„å‚æ•°åŒ–å¤æ‚æ€§[27]çš„ä¸‹ç•Œä½œä¸ºä»¤äººå…´å¥‹çš„æœªæ¥å·¥ä½œã€‚

### 3.3ï¼ŒExtension: Block-Sparse FlashAttention

æˆ‘ä»¬è¿˜å°† FlashAttention æ‰©å±•åˆ°è¿‘ä¼¼æ³¨æ„åŠ›ï¼šæˆ‘ä»¬æå‡ºäº†å—ç¨€ç– FlashAttentionï¼Œå…¶ IO å¤æ‚æ€§æ¯”FlashAttention å°ï¼Œä¸ç¨€ç–åº¦æˆæ¯”ä¾‹ã€‚

ç»™å®šè¾“å…¥ $Q,K,V \in R^{N\times d}$ å’Œæ©ç çŸ©é˜µï¼ˆmask matrixï¼‰$\tilde{m}\in {0,1}^{N\times N}$ï¼Œæƒ³è¦è®¡ç®—:

$$S = QK^T \in R^{N\times N}, P = softmax(S\bigodot 1_{\tilde{m}}) \in \mathbb{R}^{N\times N}, O = PV\in \mathbb{R}^{N\times d}$$ï¼Œ
$$
\left\{\begin{matrix}
S\bigodot 1_{\tilde{M}} = S_{kl} \quad \tilde{M}_{kl} = 1 \\ \nonumber
-\infty \quad M_{kl} = 0 \end{matrix}\right.
$$
æˆ‘ä»¬è¦æ±‚ $\tilde{M}$ å…·æœ‰å—å½¢å¼ï¼šå¯¹äºæŸäº›å—å¤§å° $B_rï¼ŒB_c$ï¼Œå¯¹äºæ‰€æœ‰çš„ $k, l$ï¼Œéƒ½æœ‰ $\tilde{M}_{kl} = M_{ij}$ï¼Œå…¶ä¸­ $i = \left \lfloor k/B_r \right \rfloor, j = \left \lfloor l/B_c \right \rfloor$ï¼Œå¯¹äºæŸäº› $M\in {0, 1}^{N/B_r\times N/B_c}$ã€‚

ç»™å®šé¢„å®šä¹‰çš„å—ç¨€ç–æ©ç çŸ©é˜µ $M\in {0, 1}^{N/B_r\times N/B_c}$ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°è°ƒæ•´ç®—æ³•1ï¼Œåªè®¡ç®—æ³¨æ„åŠ›çŸ©é˜µçš„éé›¶å—ã€‚è¯¥ç®—æ³•ä¸ç®—æ³• 1 ç›¸åŒï¼Œåªæ˜¯æˆ‘ä»¬ä¼šè·³è¿‡é›¶å—éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨é™„å½•Bä¸­çš„ç®—æ³•5ä¸­é‡ç°äº†ç®—æ³•æè¿°ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†å—ç¨€ç– FlashAttention çš„ IO å¤æ‚æ€§ã€‚

ã€å®šç† 4ã€‘å‡è®¾ $N$ æ˜¯è¾“å…¥åºåˆ—çš„é•¿åº¦ï¼Œ$d$ æ˜¯æ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œ$M$ æ˜¯ `SRAM` å¤§å°ï¼Œä¸” $d \leq M\leq Nd$ã€‚å—-ç¨€ç–çš„ FlashAttention (ç®—æ³• 5) çš„ HBM è®¿é—®æ¬¡æ•°æ˜¯ $O(Nd + N^2d^2M^{âˆ’1}s)$ã€‚

å…¶ä¸­ $s$ æ˜¯å—ç¨€ç–æ©ç ä¸­éé›¶å—çš„æ¯”ä¾‹ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ**åº”ç”¨å—ç¨€ç–æ€§ç›´æ¥æ”¹å–„äº† IO å¤æ‚æ€§ä¸­è¾ƒå¤§é¡¹çš„å¤æ‚åº¦**ï¼Œå¤æ‚åº¦ä¸ç¨€ç–åº¦æˆæ¯”ä¾‹ã€‚å¯¹äºå¤§çš„åºåˆ—é•¿åº¦ $N$ï¼Œ$s$ é€šå¸¸è®¾ç½®ä¸º $N^{-1/2}$ [11] æˆ– $N^{-1}log N$ [3, 17, 92]ï¼Œå¯¼è‡´ $\Theta(N\sqrt{N})$ æˆ– $\Theta(N logN)$çš„ IO å¤æ‚æ€§ã€‚åœ¨ä¸‹æ¸¸å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å›ºå®šçš„è´è¶ç¨€ç–æ¨¡å¼ [17]ï¼Œå·²ç»è¯æ˜èƒ½å¤Ÿé€¼è¿‘ä»»æ„çš„ç¨€ç–æ€§ [16]ã€‚åœ¨å›¾ 2ï¼ˆå³ä¾§ï¼‰ä¸­ï¼Œæˆ‘ä»¬éªŒè¯éšç€ç¨€ç–æ€§çš„å¢åŠ ï¼Œå—ç¨€ç– FlashAttention çš„è¿è¡Œæ—¶é—´æˆæ¯”ä¾‹æé«˜ã€‚åœ¨ `LRA` åŸºå‡†æµ‹è¯•ä¸­ï¼Œå—ç¨€ç– FlashAttention å®ç°äº†2.8å€çš„åŠ é€Ÿï¼ŒåŒæ—¶æ€§èƒ½ä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸å½“ï¼ˆç¬¬4èŠ‚ï¼‰ã€‚

## 4ï¼Œå®éªŒ

### 4.1ï¼Œä½¿ç”¨ FlashAttention çš„æ›´å¿«æ¨¡å‹

### 4.2ï¼Œä½¿ç”¨é•¿åºåˆ—çš„æ›´å¥½æ¨¡å‹

### 4.3ï¼ŒåŸºå‡†æ³¨æ„åŠ›

## 5ï¼Œå±€é™æ€§å’Œæœªæ¥æ–¹å‘

1. **ç¼–è¯‘ä¸º CUDA**ã€‚

   æˆ‘ä»¬å½“å‰æ„å»º IO æ„ŸçŸ¥çš„æ³¨æ„åŠ›å®ç°çš„æ–¹æ³•éœ€è¦ä¸ºæ¯ä¸ªæ–°çš„æ³¨æ„åŠ›å®ç°ç¼–å†™ä¸€ä¸ªæ–°çš„ CUDA å†…æ ¸ã€‚ è¿™éœ€è¦ä½¿ç”¨æ¯” PyTorch ä½å¾—å¤šçš„è¯­è¨€ç¼–å†™æ³¨æ„åŠ›ç®—æ³•ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„å·¥ç¨‹å·¥ä½œã€‚ å®ç°ä¹Ÿå¯èƒ½æ— æ³•è·¨ GPU æ¶æ„è½¬ç§»ã€‚ è¿™äº›é™åˆ¶è¡¨æ˜éœ€è¦ä¸€ç§æ–¹æ³•æ¥æ”¯æŒç”¨é«˜çº§è¯­è¨€ï¼ˆä¾‹å¦‚ PyTorchï¼‰ç¼–å†™æ³¨æ„åŠ›ç®—æ³•ï¼Œå¹¶ç¼–è¯‘ä¸º CUDA ä¸­çš„ IO æ„ŸçŸ¥å®ç°ï¼Œç±»ä¼¼äºå›¾åƒå¤„ç†ä¸­çš„ Halide ç­‰å·¥ä½œ [70]ã€‚

2. **IO æ„ŸçŸ¥æ·±åº¦å­¦ä¹ **ã€‚ æˆ‘ä»¬ç›¸ä¿¡ï¼Œ**IO æ„ŸçŸ¥æ–¹æ³•**å¯ä»¥æ‰©å±•åˆ°æ³¨æ„åŠ›ä»¥å¤–çš„é¢†åŸŸã€‚è™½ç„¶æ³¨æ„åŠ›æ˜¯å˜æ¢å™¨ä¸­æœ€å ç”¨å†…å­˜çš„è®¡ç®—ï¼Œä½†æ·±åº¦ç½‘ç»œä¸­çš„æ¯ä¸€å±‚éƒ½ä¼šæ¥è§¦ GPU HBMã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿæ¿€å‘å¯¹å…¶ä»–æ¨¡å—è¿›è¡Œ IO æ„ŸçŸ¥å®ç°çš„çµæ„Ÿã€‚æˆ‘ä»¬åœ¨é™„å½• D ä¸­è®¨è®ºäº†è¿™äº›æ½œåœ¨çš„æ‰©å±•ã€‚

3. **å¤š GPU IO æ„ŸçŸ¥æ–¹æ³•**ã€‚ æˆ‘ä»¬çš„ IO æ„ŸçŸ¥æ³¨æ„åŠ›å®ç°åœ¨å•ä¸ª GPU ä¸Šè®¡ç®—æ³¨æ„åŠ›çš„å¸¸æ•°èŒƒå›´å†…æ˜¯æœ€ä½³çš„ã€‚ ç„¶è€Œï¼Œæ³¨æ„åŠ›è®¡ç®—å¯ä»¥è·¨å¤šä¸ª GPU å¹¶è¡Œ[72]ã€‚ ä½¿ç”¨å¤šä¸ª GPU ä¸º IO åˆ†æå¢åŠ äº†ä¸€ä¸ªé¢å¤–çš„å±‚ - è€ƒè™‘ GPU ä¹‹é—´çš„æ•°æ®ä¼ è¾“ã€‚ æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿæ¿€å‘æœªæ¥åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„ç ”ç©¶å·¥ä½œã€‚

## Aï¼Œç›¸å…³å·¥ä½œ



## Bï¼Œç®—æ³•ç»†èŠ‚

æˆ‘ä»¬é¦–å…ˆæ¨å¯¼äº†æ³¨æ„åŠ›çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼Œå¹¶å±•ç¤ºå®ƒä»¬å¯ä»¥ä»¥ä¸€ç§å†…å­˜é«˜æ•ˆçš„æ–¹å¼è¿›è¡Œè®¡ç®—ï¼ˆéœ€è¦é¢å¤–çš„å†…å­˜ä¸åºåˆ—é•¿åº¦å‘ˆçº¿æ€§å…³ç³»ï¼Œè€Œä¸æ˜¯äºŒæ¬¡å…³ç³»ï¼‰ã€‚å°½ç®¡å®ƒä»¬å‡å°‘äº†æ‰€éœ€çš„é¢å¤–å†…å­˜é‡ï¼Œä½†ä»æ ¹æœ¬ä¸Šæ¥è¯´ï¼Œå®ƒä»¬ä»ç„¶ä¼šäº§ç”ŸäºŒæ¬¡çš„ HBM è®¿é—®ï¼Œå¯¼è‡´æ‰§è¡Œé€Ÿåº¦è¾ƒæ…¢ã€‚æˆ‘ä»¬æè¿°äº† `FlashAttention` ç®—æ³•ï¼Œç”¨äºåœ¨ `GPU` ä¸Šå®ç°å‰å‘ä¼ é€’å’Œåå‘ä¼ é€’ï¼Œå‡å°‘äº† `HBM` è®¿é—®ï¼Œä»è€Œæ—¢æé«˜äº†è¿è¡Œæ—¶é€Ÿåº¦ï¼Œåˆå‡å°äº†å†…å­˜å ç”¨ã€‚

### B.1ï¼Œå†…å­˜é«˜æ•ˆçš„å‰å‘ä¼ æ’­

åœ¨ä½¿æ³¨æ„åŠ›å†…å­˜é«˜æ•ˆåŒ–æ–¹é¢çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å°† $K$ çš„åˆ—ï¼ˆå’Œ $V$ çš„åˆ—ï¼‰è€¦åˆçš„ `softmax`ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åˆ†åˆ«è®¡ç®—`softmax` å½’ä¸€åŒ–å¸¸æ•°ï¼Œä»¥è§£è€¦è¿™äº›åˆ—ã€‚è¿™ç§æŠ€æœ¯å·²ç»åœ¨æ–‡çŒ®ä¸­è¢«ä½¿ç”¨ï¼Œç”¨æ¥è¯æ˜æ³¨æ„åŠ›è®¡ç®—ä¸éœ€è¦äºŒæ¬¡é¢å¤–å†…å­˜ï¼ˆå°½ç®¡ `HBM` è®¿é—®çš„æ•°é‡ä»ç„¶æ˜¯äºŒæ¬¡çš„ï¼Œå¯¼è‡´è¿è¡Œæ—¶è¾ƒæ…¢ï¼‰ã€‚

ç»™å®šè¾“å…¥ $Q,K,V \in R^{N\times d}$ï¼Œç›®æ ‡æ˜¯è®¡ç®—æ³¨æ„åŠ›è¾“å‡º $O \in R^{N\times d}$: 

$$S = QK^T \in R^{N\times N}, P = softmax(S) \in R^{N\times N}, O = PV\in R^{N\times d}$$ã€‚

å‡è®¾ $q_i$ å’Œ $k_j$ æ˜¯ $Q$ å’Œ $K$ çŸ©é˜µçš„ç¬¬ $i$ å’Œç¬¬ $j$ åˆ—ã€‚å®šä¹‰ `softmax` çš„å½’ä¸€åŒ–å¸¸æ•°å¦‚ä¸‹:

> å› ä¸ºå–çš„æ˜¯åŸå§‹ $Q$ å’Œ $K$ çŸ©é˜µçš„åˆ—ï¼Œæ‰€ä»¥åŸæ¥çš„ $QK^T$ è¦è½¬å˜æˆ $q_ik_j^T$ï¼Œå³åˆ—å˜è¡Œï¼Œè¡Œå˜åˆ—ã€‚

$$
L_i = \sum_j e^{q_i^T k_j}
$$

è®¾ $v_j$ ä¸ºçŸ©é˜µ $V$ çš„ç¬¬ $j$ åˆ—ï¼Œåˆ™è¾“å‡ºçŸ©é˜µçš„ç¬¬ $i$ åˆ—ä¸ºï¼š
$$
o_i = P_{i:}V = \sum_j P_{ij}v_j = \sum_j \frac{e^{q_i^T k_j}}{L_i}v_j
$$

> å› ä¸º $v_j$ ä¸ºçŸ©é˜µ $V$ çš„ç¬¬ $j$ åˆ—ï¼Œæ‰€ä»¥å¯¹åº”çš„å¯¹æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µä¹Ÿè¦å–ç¬¬ $j$ åˆ—ã€‚

æˆ‘ä»¬çœ‹åˆ°ï¼Œä¸€æ—¦è®¡ç®—å‡º $L_i$ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡é‡å¤å¯¹ $\frac{e^{q_i^T k_j}}{L_i}v_j$ æ±‚å’Œæ¥è®¡ç®— $o_i$ï¼Œè€Œæ— éœ€é¢å¤–çš„å†…å­˜ã€‚ å› æ­¤ï¼Œå‰å‘ä¼ æ’­å¯ä»¥ç”¨ $O(n)$ é¢å¤–å†…å­˜æ¥è®¡ç®—ï¼š

1. æ ¹æ®æ–¹ç¨‹å¼ï¼ˆ1ï¼‰è®¡ç®—æ‰€æœ‰ $i$ çš„ $L_i$ï¼Œéœ€è¦ $O(n)$ é¢å¤–å†…å­˜ã€‚
2. æ ¹æ®æ–¹ç¨‹å¼ï¼ˆ2ï¼‰è®¡ç®—æ‰€æœ‰ $i$ çš„ $o_i$ï¼Œéœ€è¦ $O(d)$ é¢å¤–å†…å­˜ã€‚

### B.2ï¼Œå†…å­˜é«˜æ•ˆçš„åå‘ä¼ æ’­

### B.3 FlashAttention: å‰å‘ä¼ æ’­

![full_flash_attention](../../images/flash_attention/full_flash_attention.png)

æˆ‘ä»¬ä¿å­˜è¾“å‡º $O$ã€`softmax` ç»Ÿè®¡ä¿¡æ¯ $\ell$ å’Œ $m$ï¼Œä»¥åŠåå‘ä¼ æ’­çš„ä¼ªéšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€ $R$ã€‚

å®Œæ•´çš„ `FlashAttention` å‰å‘ä¼ æ’­ç®—æ³•å¦‚ä¸‹:

![å®Œæ•´çš„ FlashAttention å‰å‘ä¼ æ’­ç®—æ³•](../../images/flash_attention/full_flash_attention_pipeline.png)

åŸºäº `openai` `trion` åº“å®ç°çš„æ”¯æŒ `NoPad` çš„ `FlashAttention` ç®—å­å¦‚ä¸‹ï¼š

```python
if triton.__version__ >= "2.1.0":
    @triton.jit
    def _fwd_kernel(
        Q, K, V, sm_scale, B_Start_Loc, B_Seqlen,  # B_LOC å†…éƒ¨è®°å½•æ¯ä¸ªbatch è¾“å…¥çš„çœŸå®ä½ç½®ï¼Œ B_SEQ_len è®°å½•å½“å‰è¾“å…¥çš„çœŸå®é•¿åº¦
        Out,
        stride_qbs, stride_qh, stride_qd,
        stride_kbs, stride_kh, stride_kd,
        stride_vbs, stride_vh, stride_vd,
        stride_obs, stride_oh, stride_od,
        BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,
        BLOCK_N: tl.constexpr,
    ):
        cur_batch = tl.program_id(0)
        cur_head = tl.program_id(1)
        start_m = tl.program_id(2)

        cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)
        cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)

        block_start_loc = BLOCK_M * start_m

        # initialize offsets
        offs_n = tl.arange(0, BLOCK_N)
        offs_d = tl.arange(0, BLOCK_DMODEL)
        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
        off_q = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd
        off_k = offs_n[None, :] * stride_kbs + cur_head * stride_kh + offs_d[:, None] * stride_kd
        off_v = offs_n[:, None] * stride_vbs + cur_head * stride_vh + offs_d[None, :] * stride_vd

        q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)

        k_ptrs = K + off_k
        v_ptrs = V + off_v

        # initialize pointer to m and l
        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")
        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)
        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)

        block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)

        for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):
            start_n = tl.multiple_of(start_n, BLOCK_N)
            # -- compute qk ----
            k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,
                        mask=(start_n + offs_n[None, :]) < cur_batch_seq_len, other=0.0)
            # mask = tl.load(mask_ptrs + start_n, mask=start_n + offs_n < cur_batch_end_loc, other=0.0)

            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
            qk += tl.dot(q, k)
            qk *= sm_scale
            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float("-inf"))

            # -- compute m_ij, p, l_ij
            m_ij = tl.max(qk, 1)
            p = tl.exp(qk - m_ij[:, None])
            l_ij = tl.sum(p, 1)
            # -- update m_i and l_i
            m_i_new = tl.maximum(m_i, m_ij)
            alpha = tl.exp(m_i - m_i_new)
            beta = tl.exp(m_ij - m_i_new)
            l_i_new = alpha * l_i + beta * l_ij
            # -- update output accumulator --
            # scale p
            p_scale = beta / l_i_new
            p = p * p_scale[:, None]
            # scale acc
            acc_scale = l_i / l_i_new * alpha
            acc = acc * acc_scale[:, None]
            # update acc
            v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,
                        mask=(start_n + offs_n[:, None]) < cur_batch_seq_len, other=0.0)

            p = p.to(v.dtype)
            acc += tl.dot(p, v)
            # update m_i and l_i
            l_i = l_i_new
            m_i = m_i_new
        # initialize pointers to output
        off_o = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od
        out_ptrs = Out + off_o
        tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)
        return
```

## Cï¼Œè¯æ˜

ä¸»è¦çš„ `FLOPs` æ¥æºäº**çŸ©é˜µä¹˜æ³•**ï¼Œåœ¨å†…å¾ªç¯çš„çŸ©é˜µä¹˜æ³•ä¸­ï¼š

1. ç®—æ³• 1 ç¬¬ 9 è¡Œï¼Œè®¡ç®— $Q_iK_j^T \in R^{B_r\times B_c}$ ï¼Œ$Q \in R^{B_r \times d}$ã€$K_j \in R^{B_c \times d}$ï¼ŒFLOPs ä¸º $O(B_rB_cd)$
2. ç®—æ³•ç¬¬ 12 è¡Œï¼Œè®¡ç®— $\tilde{P}_{ij}V_j \in R^{B_r \times d}$ï¼Œè¿™é‡Œ $\tilde{P} \in R^{B_r\times B_c}$ã€$V_j \in R^{B_c \times d}$ï¼ŒFLOPs ä¸º $O(B_rB_cd)$ã€‚

å†…éƒ¨å¾ªç¯æ‰§è¡Œæ¬¡æ•°ï¼š$T_cT_r = \left \lceil \frac{N}{B_c} \right \rceil \left \lceil \frac{N}{B_r} \right \rceil$ï¼Œç”±æ­¤å¯å¾—æ€»çš„ FLOPs ä¸ºï¼š

$$O(\frac{N^2}{B_r\times B_c}B_rB_cd) = O(N^2d)$$

**å—å¤§å°**ä¸ºï¼š$B_c = \frac{M}{4d}$ï¼Œ$B_r = min(\frac{M}{4d}, d)$ã€‚

å°±æ‰€éœ€çš„é¢å¤–å†…å­˜è€Œè¨€ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬éœ€è¦ $O(N)$ å†…å­˜æ¥å­˜å‚¨ç»Ÿè®¡æ•°æ® $(\ell, m)$ã€‚

