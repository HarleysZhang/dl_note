- [ä¸€ï¼ŒTransformers æœ¯è¯­](#ä¸€transformers-æœ¯è¯­)
  - [1.1ï¼Œtokenã€tokenization å’Œ tokenizer](#11tokentokenization-å’Œ-tokenizer)
  - [1.2ï¼Œinput IDs](#12input-ids)
  - [1.3ï¼Œattention mask](#13attention-mask)
  - [1.4ï¼Œdecoder models](#14decoder-models)
  - [1.5ï¼Œæ¶æ„ä¸å‚æ•°](#15æ¶æ„ä¸å‚æ•°)
- [äºŒï¼ŒTransformers åŠŸèƒ½](#äºŒtransformers-åŠŸèƒ½)
  - [API æ¦‚è¿°](#api-æ¦‚è¿°)
- [ä¸‰ï¼Œå¿«é€Ÿä¸Šæ‰‹](#ä¸‰å¿«é€Ÿä¸Šæ‰‹)
  - [3.1ï¼Œtransformer æ¨¡å‹ç±»åˆ«](#31transformer-æ¨¡å‹ç±»åˆ«)
  - [3.2ï¼ŒPipeline](#32pipeline)
  - [3.3ï¼ŒAutoClass](#33autoclass)
    - [3.3.1ï¼ŒAutoTokenizer](#331autotokenizer)
  - [3.3.2ï¼ŒAutoModel](#332automodel)
- [å‚è€ƒé“¾æ¥](#å‚è€ƒé“¾æ¥)

## ä¸€ï¼ŒTransformers æœ¯è¯­

### 1.1ï¼Œtokenã€tokenization å’Œ tokenizer

`token` å¯ä»¥ç†è§£ä¸ºæœ€å°è¯­ä¹‰å•å…ƒï¼Œç¿»è¯‘çš„è¯å¯ä»¥æ˜¯è¯å…ƒã€ä»¤ç‰Œã€è¯ï¼Œä¹Ÿå¯ä»¥æ˜¯ word/char/subwordï¼Œå•ç†è§£å°±æ˜¯å•è¯å’Œæ ‡ç‚¹ã€‚

`tokenization` æ˜¯æŒ‡**åˆ†è¯**è¿‡ç¨‹ï¼Œç›®çš„æ˜¯å°†è¾“å…¥åºåˆ—åˆ’åˆ†æˆä¸€ä¸ªä¸ªè¯å…ƒï¼ˆ`token`ï¼‰ï¼Œä¿è¯å„ä¸ªè¯å…ƒæ‹¥æœ‰ç›¸å¯¹å®Œæ•´å’Œç‹¬ç«‹çš„è¯­ä¹‰ï¼Œä»¥ä¾›åç»­ä»»åŠ¡ï¼ˆæ¯”å¦‚å­¦ä¹  embedding æˆ–ä½œä¸º LLM çš„è¾“å…¥ï¼‰ä½¿ç”¨ã€‚

åœ¨ transformers åº“ä¸­ï¼Œ`tokenizer` å°±æ˜¯å®ç° `tokenization` çš„å¯¹è±¡ï¼Œæ¯ä¸ª tokenizer ä¼šæœ‰ä¸åŒçš„ vocabularyã€‚åœ¨ä»£ç ä¸­ï¼Œtokenizer ç”¨ä»¥å°†è¾“å…¥æ–‡æœ¬åºåˆ—åˆ’åˆ†æˆ tokenizer vocabulary ä¸­å¯ç”¨çš„ `tokens`ã€‚

ä¸¾ä¸¤ä¸ª tokenization ä¾‹å­ï¼š

- â€œVRAMâ€ é€šå¸¸ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œæ‰€ä»¥å…¶é€šå¸¸ä¼šè¢«åˆ’åˆ†æˆ â€œVâ€, â€œRAâ€ and â€œMâ€ è¿™æ ·çš„ `tokens`ã€‚
- æˆ‘æ˜¯ä¸­å›½äºº->['æˆ‘', 'æ˜¯', 'ä¸­å›½äºº']

### 1.2ï¼Œinput IDs

`LLM` å”¯ä¸€å¿…é¡»çš„è¾“å…¥æ˜¯ `input ids`ï¼Œæœ¬è´¨æ˜¯ `tokens` ç´¢å¼•ï¼ˆtoken indices in tokenizer vocabularyï¼‰ï¼Œå³æ•°å­— ID æ•°ç»„ï¼Œä»è€Œç¬¦åˆæ¨¡å‹è¾“å…¥çš„è¦æ±‚ã€‚

- å°†è¾“å…¥æ–‡æœ¬åºåˆ—è½¬æ¢æˆ tokensï¼Œå³ tokenized è¿‡ç¨‹ï¼›
- å°†è¾“å…¥æ–‡æœ¬åºåˆ—è½¬æ¢æˆ input idsï¼Œå³è¾“å…¥ç¼–ç è¿‡ç¨‹ï¼Œæ•°å€¼å¯¹åº”çš„æ˜¯ tokenizer è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ï¼Œ

Transformer åº“å®ç°äº†ä¸åŒæ¨¡å‹çš„ tokenizerã€‚ä¸‹é¢ä»£ç å±•ç¤ºäº†å°†è¾“å…¥åºåˆ—è½¬æ¢æˆ tokens å’Œ input_ids çš„ç»“æœã€‚

```python
from transformers import BertTokenizer

sequence = "A Titan RTX has 24GB of VRAM"
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased") 

tokenized_sequence = tokenizer.tokenize(sequence) # å°†è¾“å…¥åºåˆ—è½¬æ¢æˆtokensï¼Œtokenized è¿‡ç¨‹
inputs = tokenizer(sequence) # å°†è¾“å…¥åºåˆ—è½¬åŒ–æˆç¬¦åˆæ¨¡å‹è¾“å…¥è¦æ±‚çš„ input_idsï¼Œç¼–ç è¿‡ç¨‹
encoded_sequence = inputs["input_ids"]

print(tokenized_sequence)
print(encoded_sequence)
print("[INFO]: length of tokenized_sequence and encoded_sequence:", len(tokenized_sequence), len(encoded_sequence))

"""
['A', 'Titan', 'RT', '##X', 'has', '24', '##GB', 'of', 'VR', '##AM']
[101, 138, 28318, 56898, 12674, 10393, 10233, 32469, 10108, 74727, 36535, 102]
[INFO]: length of tokenized_sequence and encoded_sequence: 10 12
"""
```

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè°ƒç”¨ tokenizer() å‡½æ•°è¿”å›çš„æ˜¯å­—å…¸å¯¹è±¡ï¼ŒåŒ…å«ç›¸åº”æ¨¡å‹æ­£å¸¸å·¥ä½œæ‰€éœ€çš„æ‰€æœ‰å‚æ•°ï¼Œtoken indices åœ¨é”® `input_ids` å¯¹åº”çš„é”®å€¼ä¸­ã€‚åŒæ—¶ï¼Œ**tokenizer ä¼šè‡ªåŠ¨å¡«å…… "special tokens"**ï¼ˆå¦‚æœç›¸å…³æ¨¡å‹ä¾èµ–å®ƒä»¬ï¼‰ï¼Œè¿™ä¹Ÿæ˜¯ tokenized_sequence å’Œ encoded_sequence åˆ—è¡¨ä¸­é•¿åº¦ä¸ä¸€è‡´çš„åŸå› ã€‚

```python
decoded_sequence = tokenizer.decode(encoded_sequence)
print(decoded_sequence)
"""
[CLS] A Titan RTX has 24GB of VRAM [SEP]
"""
```

### 1.3ï¼Œattention mask

æ³¨æ„æ©ç ï¼ˆ`attention mask`ï¼‰æ˜¯ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œä¸€èˆ¬åœ¨å°†è¾“å…¥åºåˆ—è¿›è¡Œ**æ‰¹å¤„ç†**æ—¶ä½¿ç”¨ã€‚ä½œç”¨æ˜¯å‘Šè¯‰æˆ‘ä»¬å“ªäº› `tokens` åº”è¯¥è¢«å…³æ³¨ï¼Œå“ªäº›ä¸ç”¨ã€‚å› ä¸ºå¦‚æœè¾“å…¥çš„åºåˆ—æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªåºåˆ—é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œé€šå¸¸æ˜¯é€šè¿‡å¡«å……çš„æ–¹å¼æŠŠä»–ä»¬å¤„ç†æˆåŒä¸€é•¿åº¦ã€‚åŸå§‹ token id æ˜¯æˆ‘ä»¬éœ€è¦å…³æ³¨çš„ï¼Œå¡«å……çš„ id æ˜¯ä¸ç”¨å…³æ³¨çš„ã€‚

attention mask æ˜¯äºŒè¿›åˆ¶å¼ é‡ç±»å‹ï¼Œå€¼ä¸º `1` çš„ä½ç½®ç´¢å¼•å¯¹åº”çš„åŸå§‹ `token` è¡¨ç¤ºåº”è¯¥æ³¨æ„çš„å€¼ï¼Œè€Œ `0` è¡¨ç¤ºå¡«å……å€¼ã€‚

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from transformers import AutoTokenizer

sentence_list = ["We are very happy to show you the ğŸ¤— Transformers library.",
            "Deepspeed is faster"]
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

padded_sequences = tokenizer(sentence_list, padding=True, return_tensors="pt")

print(padded_sequences["input_ids"])
print(padded_sequences["attention_mask"])

"""
tensor([[101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
        [101, 15526, 65998, 54436, 10127, 51524, 102, 0, 0, 0, 0, 0, 0, 0]])
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])
"""
```

![attention_mask](../../images/transformers_basic/attention_mask.png)

### 1.4ï¼Œdecoder models

decoder æ¨¡å‹ä¹Ÿç§°ä¸ºè‡ªå›å½’ï¼ˆauto-regressiveï¼‰æ¨¡å‹ã€causal language modelsï¼Œå…¶æŒ‰é¡ºåºé˜…è¯»è¾“å…¥æ–‡æœ¬å¹¶å¿…é¡»é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œåœ¨è®­ç»ƒä¸­ä¼šé˜…è¯»**æ·»åŠ æ©ç çš„å¥å­**ã€‚

### 1.5ï¼Œæ¶æ„ä¸å‚æ•°

- **æ¶æ„**ï¼šæ¨¡å‹çš„éª¨æ¶ï¼ŒåŒ…å«æ¯ä¸ªå±‚çš„ç±»åˆ«åŠå®šä¹‰ã€å„ä¸ªå±‚çš„è¿æ¥æ–¹å¼ç­‰ç­‰å†…å®¹ã€‚
- **Checkpoints**ï¼šç»™å®šæ¶æ„ä¸­ä¼šè¢«åŠ è½½çš„æƒé‡ã€‚
- **æ¨¡å‹**ï¼šä¸€ä¸ªç¬¼ç»Ÿçš„æœ¯è¯­ï¼Œæ²¡æœ‰â€œæ¶æ„â€æˆ–â€œå‚æ•°â€é‚£ä¹ˆç²¾ç¡®ï¼šå®ƒå¯ä»¥æŒ‡ä¸¤è€…ã€‚

## äºŒï¼ŒTransformers åŠŸèƒ½

[Transformers](https://github.com/huggingface/transformers) åº“æä¾›åˆ›å»º transformer æ¨¡å‹å’ŒåŠ è½½ä½¿ç”¨å…±äº«æ¨¡å‹çš„åŠŸèƒ½ï¼›å¦å¤–ï¼Œ[æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰](https://huggingface.co/models)åŒ…å«æ•°åƒä¸ªå¯ä»¥ä»»æ„ä¸‹è½½å’Œä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ”¯æŒç”¨æˆ·ä¸Šä¼ æ¨¡å‹åˆ° Hubã€‚

### API æ¦‚è¿°

 Transformers åº“çš„ `API` ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸‰ç§ï¼š

1. **MAIN CLASSES**ï¼šä¸»è¦åŒ…æ‹¬é…ç½®(configuration)ã€æ¨¡å‹(model)ã€åˆ†è¯å™¨(tokenizer)å’Œæµæ°´çº¿(pipeline)è¿™å‡ ä¸ªæœ€é‡è¦çš„ç±»ã€‚
2. **MODELS**ï¼šåº“ä¸­å’Œæ¯ä¸ªæ¨¡å‹å®ç°æœ‰å…³çš„ç±»å’Œå‡½æ•°ã€‚
3. **INTERNAL HELPERS**ï¼šå†…éƒ¨ä½¿ç”¨çš„å·¥å…·ç±»å’Œå‡½æ•°ã€‚

## ä¸‰ï¼Œå¿«é€Ÿä¸Šæ‰‹

### 3.1ï¼Œtransformer æ¨¡å‹ç±»åˆ«

Transformer æ¨¡å‹æ¶æ„ä¸»è¦ç”±ä¸¤ä¸ªéƒ¨ä»¶ç»„æˆï¼š

- **Encoder (å·¦ä¾§)**: ç¼–ç å™¨æ¥æ”¶è¾“å…¥å¹¶æ„å»ºå…¶è¡¨ç¤ºï¼ˆå…¶ç‰¹å¾ï¼‰ã€‚è¿™æ„å‘³ç€å¯¹æ¨¡å‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥ä»è¾“å…¥ä¸­è·å¾—ç†è§£ã€‚
- **Decoder (å³ä¾§)**: è§£ç å™¨ä½¿ç”¨ç¼–ç å™¨çš„è¡¨ç¤ºï¼ˆç‰¹å¾ï¼‰ä»¥åŠå…¶ä»–è¾“å…¥æ¥ç”Ÿæˆç›®æ ‡åºåˆ—ã€‚è¿™æ„å‘³ç€è¯¥æ¨¡å‹å·²é’ˆå¯¹ç”Ÿæˆè¾“å‡ºè¿›è¡Œäº†ä¼˜åŒ–ã€‚

![transformer blocks](../../images/transformers_basic/transformers_blocks.svg)

ä¸Šè¿°ä¸¤ä¸ªéƒ¨ä»¶ä¸­çš„æ¯ä¸€ä¸ªéƒ½å¯ä»¥ä½œä¸ºæ¨¡å‹æ¶æ„ç‹¬ç«‹ä½¿ç”¨ï¼Œå…·ä½“å–å†³äºä»»åŠ¡ï¼š

- **Encoder-only models**: ä¹Ÿå«è‡ªåŠ¨ç¼–ç  Transformer æ¨¡å‹ï¼Œå¦‚ BERT-like ç³»åˆ—æ¨¡å‹ï¼Œé€‚ç”¨äºéœ€è¦ç†è§£è¾“å…¥çš„ä»»åŠ¡ã€‚å¦‚å¥å­åˆ†ç±»å’Œå‘½åå®ä½“è¯†åˆ«ã€‚
- **Decoder-only models**: ä¹Ÿå«è‡ªå›å½’ Transformer æ¨¡å‹ï¼Œå¦‚ GPT-like ç³»åˆ—æ¨¡å‹ã€‚é€‚ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œå¦‚**æ–‡æœ¬ç”Ÿæˆ**ã€‚
- **Encoder-decoder models** æˆ–è€… **sequence-to-sequence models**: ä¹Ÿè¢«ç§°ä½œåºåˆ—åˆ°åºåˆ—çš„ Transformer æ¨¡å‹ï¼Œå¦‚ BART/T5-like ç³»åˆ—æ¨¡å‹ã€‚é€‚ç”¨äºéœ€è¦æ ¹æ®è¾“å…¥è¿›è¡Œç”Ÿæˆçš„ä»»åŠ¡ï¼Œå¦‚ç¿»è¯‘æˆ–æ‘˜è¦ã€‚

ä¸‹è¡¨æ€»ç»“äº†ç›®å‰çš„ transformers æ¶æ„æ¨¡å‹ç±»åˆ«ã€ç¤ºä¾‹ä»¥åŠé€‚ç”¨ä»»åŠ¡ï¼š

| æ¨¡å‹          | ç¤ºä¾‹                                       | ä»»åŠ¡                                     |
| ------------- | ------------------------------------------ | ---------------------------------------- |
| ç¼–ç å™¨        | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€ä»æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ |
| è§£ç å™¨        | CTRL, GPT, GPT-2, Transformer XL           | æ–‡æœ¬ç”Ÿæˆ                                 |
| ç¼–ç å™¨-è§£ç å™¨ | BART, T5, Marian, mBART                    | æ–‡æœ¬æ‘˜è¦ã€ç¿»è¯‘ã€ç”Ÿæˆé—®é¢˜çš„å›ç­”           |

### 3.2ï¼ŒPipeline

Transformers åº“æ”¯æŒé€šè¿‡ pipeline() å‡½æ•°è®¾ç½® `task` ä»»åŠ¡ç±»å‹å‚æ•°ï¼Œæ¥è·‘é€šä¸åŒæ¨¡å‹çš„æ¨ç†ï¼Œå¯å®ç°ä¸€è¡Œä»£ç è·‘é€šè·¨ä¸åŒæ¨¡æ€çš„å¤šç§ä»»åŠ¡ï¼Œå…¶æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨å¦‚ä¸‹ï¼š

| **ä»»åŠ¡**     | **æè¿°**                                                 | **æ¨¡æ€**        | **Pipeline**                                  |
| ------------ | -------------------------------------------------------- | --------------- | --------------------------------------------- |
| æ–‡æœ¬åˆ†ç±»     | ä¸ºç»™å®šçš„æ–‡æœ¬åºåˆ—åˆ†é…ä¸€ä¸ªæ ‡ç­¾                             | NLP             | pipeline(task="sentiment-analysis")           |
| æ–‡æœ¬ç”Ÿæˆ     | æ ¹æ®ç»™å®šçš„æç¤ºç”Ÿæˆæ–‡æœ¬                                   | NLP             | pipeline(task="text-generation")              |
| å‘½åå®ä½“è¯†åˆ« | ä¸ºåºåˆ—é‡Œçš„æ¯ä¸ªtokenåˆ†é…ä¸€ä¸ªæ ‡ç­¾(äºº, ç»„ç»‡, åœ°å€ç­‰ç­‰)      | NLP             | pipeline(task="ner")                          |
| é—®ç­”ç³»ç»Ÿ     | é€šè¿‡ç»™å®šçš„ä¸Šä¸‹æ–‡å’Œé—®é¢˜, åœ¨æ–‡æœ¬ä¸­æå–ç­”æ¡ˆ                 | NLP             | pipeline(task="question-answering")           |
| æ©ç›–å¡«å……     | é¢„æµ‹å‡ºæ­£ç¡®çš„åœ¨åºåˆ—ä¸­è¢«æ©ç›–çš„token                        | NLP             | pipeline(task="fill-mask")                    |
| æ–‡æœ¬æ‘˜è¦     | ä¸ºæ–‡æœ¬åºåˆ—æˆ–æ–‡æ¡£ç”Ÿæˆæ€»ç»“                                 | NLP             | pipeline(task="summarization")                |
| æ–‡æœ¬ç¿»è¯‘     | å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘ä¸ºå¦ä¸€ç§è¯­è¨€                         | NLP             | pipeline(task="translation")                  |
| å›¾åƒåˆ†ç±»     | ä¸ºå›¾åƒåˆ†é…ä¸€ä¸ªæ ‡ç­¾                                       | Computer vision | pipeline(task="image-classification")         |
| å›¾åƒåˆ†å‰²     | ä¸ºå›¾åƒä¸­æ¯ä¸ªç‹¬ç«‹çš„åƒç´ åˆ†é…æ ‡ç­¾(æ”¯æŒè¯­ä¹‰ã€å…¨æ™¯å’Œå®ä¾‹åˆ†å‰²) | Computer vision | pipeline(task="image-segmentation")           |
| ç›®æ ‡æ£€æµ‹     | é¢„æµ‹å›¾åƒä¸­ç›®æ ‡å¯¹è±¡çš„è¾¹ç•Œæ¡†å’Œç±»åˆ«                         | Computer vision | pipeline(task="object-detection")             |
| éŸ³é¢‘åˆ†ç±»     | ç»™éŸ³é¢‘æ–‡ä»¶åˆ†é…ä¸€ä¸ªæ ‡ç­¾                                   | Audio           | pipeline(task="audio-classification")         |
| è‡ªåŠ¨è¯­éŸ³è¯†åˆ« | å°†éŸ³é¢‘æ–‡ä»¶ä¸­çš„è¯­éŸ³æå–ä¸ºæ–‡æœ¬                             | Audio           | pipeline(task="automatic-speech-recognition") |
| è§†è§‰é—®ç­”     | ç»™å®šä¸€ä¸ªå›¾åƒå’Œä¸€ä¸ªé—®é¢˜ï¼Œæ­£ç¡®åœ°å›ç­”æœ‰å…³å›¾åƒçš„é—®é¢˜         | Multimodal      | pipeline(task="vqa")                          |

![Hub models](../../images/transformers_basic/transformers_model_hub.png)



ä»¥ä¸‹ä»£ç æ˜¯é€šè¿‡ pipeline å‡½æ•°å®ç°å¯¹æ–‡æœ¬çš„æƒ…ç»ªåˆ†ç±»ã€‚

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
print(classifier("I've been waiting for a HuggingFace course my whole life."))
# [{'label': 'POSITIVE', 'score': 0.9598049521446228}]
```

åœ¨ `NLP` é—®é¢˜ä¸­ï¼Œé™¤äº†ä½¿ç”¨ `pipeline()`  ä»»åŠ¡ä¸­é»˜è®¤çš„æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®š `model` å’Œ `tokenizer` å‚æ•°æ¥è‡ªåŠ¨æŸ¥æ‰¾ç›¸å…³æ¨¡å‹ã€‚

### 3.3ï¼ŒAutoClass

Pipeline() å‡½æ•°èƒŒåå®é™…æ˜¯é€šè¿‡ â€œAutoClassâ€ ç±»ï¼Œå®ç°**é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„è‡ªåŠ¨æŸ¥æ‰¾å…¶æ¶æ„**çš„å¿«æ·æ–¹å¼ã€‚é€šè¿‡ä¸ºä»»åŠ¡é€‰æ‹©åˆé€‚çš„ `AutoClass` å’Œå®ƒå…³è”çš„é¢„å¤„ç†ç±»ï¼Œæ¥é‡ç°ä½¿ç”¨ `pipeline()` çš„ç»“æœã€‚

#### 3.3.1ï¼ŒAutoTokenizer

åˆ†è¯å™¨ï¼ˆ`tokenizer`ï¼‰çš„ä½œç”¨æ˜¯è´Ÿè´£é¢„å¤„ç†æ–‡æœ¬ï¼Œå°†è¾“å…¥æ–‡æœ¬ï¼ˆinput promptï¼‰è½¬æ¢ä¸º**æ•°å­—æ•°ç»„**ï¼ˆarray of numbersï¼‰æ¥ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚`tokenization` è¿‡ç¨‹ä¸»è¦çš„è§„åˆ™åŒ…æ‹¬ï¼šå¦‚ä½•æ‹†åˆ†å•è¯å’Œä»€ä¹ˆæ ·çº§åˆ«çš„å•è¯åº”è¯¥è¢«æ‹†åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ä¾‹åŒ– tokenizer å’Œ model å¿…é¡»æ˜¯åŒä¸€ä¸ªæ¨¡å‹åç§°æˆ–è€… `checkpoints` è·¯å¾„ã€‚

å¯¹äº `LLM` ï¼Œé€šå¸¸è¿˜æ˜¯ä½¿ç”¨ `AutoModel` å’Œ `AutoTokenizer` æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå®ƒå…³è”çš„åˆ†è¯å™¨ã€‚

```py
from transformers import AutoModel, AutoTokenizer
tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path)
model = AutoModel.from_pretrained(model_name_or_path, torch_dtype=torch.float16)
```

ä¸€èˆ¬ä½¿ç”¨ `AutoTokenizer` åŠ è½½åˆ†è¯å™¨ï¼ˆ`tokenizer`ï¼‰:

```python
from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(encoding)

"""
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
"""
```

`tokenizer` çš„è¿”å›æ˜¯åŒ…å«äº†å¦‚ä¸‹â€œé”®â€çš„å­—å…¸ï¼š

- [input_ids](https://huggingface.co/docs/transformers/v4.29.1/zh/glossary#input-ids): ç”¨æ•°å­—è¡¨ç¤ºçš„ `token`ã€‚
- [attention_mask](https://huggingface.co/docs/transformers/v4.29.1/zh/.glossary#attention-mask): åº”è¯¥å…³æ³¨å“ªäº› `token` çš„æŒ‡ç¤ºã€‚

tokenizer() å‡½æ•°è¿˜**æ”¯æŒåˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶å¯å¡«å……å’Œæˆªæ–­æ–‡æœ¬, è¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹æ¬¡**ï¼š

```python
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
```

### 3.3.2ï¼ŒAutoModel

Transformers æä¾›äº†ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ–¹å¼æ¥åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å®ä¾‹ï¼Œå³å¯ä»¥åƒåŠ è½½ `AutoTokenizer` ä¸€æ ·åŠ è½½ `AutoModel`ï¼Œæˆ‘ä»¬æ‰€éœ€è¦æä¾›çš„å¿…é¡»å‚æ•°åªæœ‰æ¨¡å‹åç§°æˆ–è€… `checkpoints` è·¯å¾„ã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹æ‰€ç¤º:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch import nn

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name) # ä¼šä¸‹è½½ vocab.txt è¯è¡¨

pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)

pt_model = AutoModelForSequenceClassification.from_pretrained(model_name) # ä¼šä¸‹è½½ pytorch_model.bin æ¨¡å‹æƒé‡

pt_outputs = pt_model(**pt_batch) # ** å¯è§£åŒ… pt_batch å­—å…¸
pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1) # åœ¨ logitsä¸Šåº”ç”¨softmaxå‡½æ•°æ¥æŸ¥è¯¢æ¦‚ç‡

print(pt_predictions)
print(pt_model.config.id2label) # {0: '1 star', 1: '2 stars', 2: '3 stars', 3: '4 stars', 4: '5 stars'}
```

ç¨‹åºè¿è¡Œç»“æœè¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºã€‚

> tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
>         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)

## å‚è€ƒé“¾æ¥

1. [HuggingFace Transformers å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/transformers/v4.29.1/zh/quicktour)
2. [NLP Course](https://huggingface.co/learn/nlp-course/zh-CN/chapter1/1)
3. [NLPé¢†åŸŸä¸­çš„tokenå’Œtokenizationåˆ°åº•æŒ‡çš„æ˜¯ä»€ä¹ˆ](https://www.zhihu.com/question/64984731)
