- [Abstract](#abstract)
- [摘要](#摘要)
- [1. Introduction](#1-introduction)
- [介绍](#介绍)
- [2. Delve into LLM Inference and Deployment](#2-delve-into-llm-inference-and-deployment)
  - [2.1. LLM Inference](#21-llm-inference)
  - [2.2. Roofline Model](#22-roofline-model)
  - [2.3. LLM-Viewer](#23-llm-viewer)
- [2. 深入研究 LLM 推理和部署](#2-深入研究-llm-推理和部署)
  - [2.1 LLM 推理](#21-llm-推理)
  - [2.2. Roofline 模型](#22-roofline-模型)
- [3. Model Compression](#3-model-compression)
  - [3.1. Quantization](#31-quantization)
    - [3.1.1 A Use Case of LLM-Viewer:Roofline Analysis for Quantization](#311-a-use-case-of-llm-viewerroofline-analysis-for-quantization)
    - [3.1.2 Quantization for Compressing Pre-trained](#312-quantization-for-compressing-pre-trained)
      - [LLMs In Quantization-Aware Training (QAT)](#llms-in-quantization-aware-training-qat)
      - [Post-Training Quantization (PTQ)](#post-training-quantization-ptq)
    - [3.1.3 Quantization for Parameter Efficient FineTuning (Q-PEFT)](#313-quantization-for-parameter-efficient-finetuning-q-peft)
    - [3.1.4 Discussion on LLM Quantiztaion](#314-discussion-on-llm-quantiztaion)
- [3. 模型压缩](#3-模型压缩)
  - [3.1. 量化](#31-量化)
    - [3.1.1 LLM-Viewer 的应用案例：量化的 Roofline 分析](#311-llm-viewer-的应用案例量化的-roofline-分析)
    - [3.1.2 量化用于压缩预训练 LLMs](#312-量化用于压缩预训练-llms)
      - [量化感知训练（`QAT`）](#量化感知训练qat)
      - [训练后量化（PTQ）](#训练后量化ptq)
    - [3.1.3 参数高效微调的量化（Q-PEFT）](#313-参数高效微调的量化q-peft)
    - [3.1.4 关于 LLM 量化的讨论](#314-关于-llm-量化的讨论)
- [4. Algorithmic Methods for Fast Decoding](#4-algorithmic-methods-for-fast-decoding)
  - [4.1. Minimum Parameter Used Per Token Decoded](#41-minimum-parameter-used-per-token-decoded)
    - [4.1.1 Early Exiting](#411-early-exiting)
    - [4.1.2 Contextual Sparsity](#412-contextual-sparsity)
    - [4.1.3 Mixture-of-Expert Models](#413-mixture-of-expert-models)
    - [4.1.4 Roofline Model Analysis for Dynamic Parameter](#414-roofline-model-analysis-for-dynamic-parameter)
  - [4.2. Maximum Tokens Decoded Per LLM Forward](#42-maximum-tokens-decoded-per-llm-forward)
    - [4.2.1 Speculative Decoding](#421-speculative-decoding)
    - [4.2.2 Parallel Decoding](#422-parallel-decoding)
- [4. 快速解码的算法方法](#4-快速解码的算法方法)
  - [4.1 每个词元解码时使用最少的参数](#41-每个词元解码时使用最少的参数)
    - [4.1.1 提前退出](#411-提前退出)
    - [4.1.2 上下文稀疏性](#412-上下文稀疏性)
    - [4.1.3 专家混合模型](#413-专家混合模型)
    - [4.1.4 动态参数的 Roofline 模型分析](#414-动态参数的-roofline-模型分析)
  - [4.2 最大解码令牌数每次 LLM 前向传播](#42-最大解码令牌数每次-llm-前向传播)
    - [4.2.1 推测解码](#421-推测解码)
    - [4.2.2 并行解码](#422-并行解码)
- [5. Compiler/System Optimization](#5-compilersystem-optimization)
  - [5.1. Operator Fusion](#51-operator-fusion)
  - [5.2. Memory Management and Workload Offloading](#52-memory-management-and-workload-offloading)
  - [5.3. Parallel Serving](#53-parallel-serving)
- [5. 编译器/系统优化](#5-编译器系统优化)
  - [5.1. 算子融合](#51-算子融合)
  - [5.2. 内存管理与工作负载卸载](#52-内存管理与工作负载卸载)
  - [5.3. 并行服务](#53-并行服务)
- [6. Hardware Optimization](#6-hardware-optimization)
  - [6.1. Spatial Architecture](#61-spatial-architecture)
- [8. Conclusion](#8-conclusion)
- [6. 硬件优化](#6-硬件优化)
  - [6.1. 空间架构](#61-空间架构)
- [8. 结论](#8-结论)

## Abstract

The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn’t been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on Roofline model for systematic analysis of LLM inference techniques. This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory-bound, how much memory and computation they need, and how to choose the right hardware. We systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as model compression (e.g., quantization), algorithm improvements (e.g., speculative decoding), and both system and hardware-level enhancements (e.g., operator fusion). Our survey stands out by analyzing these methods with Roofline model, helping us understand their impact on memory access and computation. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment. The analyze tool, LLM-Viewer, is open-sourced.

## 摘要

在大型语言模型 (LLM) 高效推理领域，尽管其发展迅速且充满活力，但目前尚缺乏一个简洁的框架来**分析各种 `LLM` 推理方法**，从而清晰地理解这一领域。我们的综述区别于传统的文献回顾，不仅总结了现有研究的现状，还基于 `Roofline` 模型提出了一个用于系统分析 `LLM` 推理技术的框架。该框架能够识别在硬件设备上部署 LLM 时的瓶颈，并为诸如为什么 LLM 受到内存限制、它们需要多少内存和计算资源，以及如何选择合适的硬件等实际问题提供清晰的理解。

我们系统地整理了高效 LLM 推理领域的最新进展，涵盖了**模型压缩**（例如量化）、**算法改进**（例如推测解码）、以及**系统**和**硬件层面的增强**（例如操作融合）等关键领域。与其他综述不同，我们通过 Roofline 模型分析这些方法，揭示它们在内存和计算方面的性能影响。这种独特的视角不仅展示了当前的研究现状，还为实际应用提供了深刻的见解，因此我们这项工作对新入门者和希望深入了解高效 LLM 部署的研究人员都极具参考价值。此外，我们还开源了用于分析的工具 LLM-Viewer。

## 1. Introduction

Large Language Models (LLMs) have become a cornerstone of AI advancement in recent years, reshaping the landscape of machine learning and natural language pro-cessing (NLP) [Zhao et al., 2023]. This trend can be traced to the success of revolutionary models like ChatGPT [Brown et al., 2020, Ouyang et al., 2022], which produce very human-like text through their exceptional understanding and generation abilities. Following ChatGPT, other notable LLMs such as OPT [Zhang et al., 2022], BLOOM [Scao et al., 2022], and Llama [Touvron et al., 2023a,b] have emerged, further solidifying the consensus that larger models often lead to enhanced capabilities. Therefore, models with tens of billions of parameters are becoming increasingly common. As a result of the vast size of these models, they present considerable inference challenges, not only for devices with limited computational capabilities, but also for the most advanced hardware. Because of their complexity and scale, as well as their energy and computational demands, these models are difficult to deploy in real-world situations. Additionally, the resource-intensive nature of these models raises concerns about energy consumption, scalability, and accessibility. The situation is particularly challenging for smaller organizations and communities with fewer computing resources than large corporations. Therefore, these challenges emphasize the need for innovative solutions to make LLM inference more universally accessible and sustainable.

Numerous methods have been developed to address the challenges of deploying LLM. The field of efficient LLM inference has grown exponentially in the last two years, presenting both opportunities and challenges. While the burgeoning volume of research demonstrates the field’s vibrancy, it can inadvertently mask key trends and slow advancements. A critical gap in existing literature is the absence of a systematic and practical framework for unified analysis and comprehensive solution development. To bridge this gap, our work offers a comprehensive overview of the current state of research in efficient LLM inference, with a unique focus on its practice-driven characteristics. Diverging from traditional literature reviews, our work not only discusses existing research but also introduces a specifically developed Roofline model. This model is designed to analyze bottlenecks in LLM deployments, a crucial step we believe is vital for practical application and optimization as shown in Figure 1. Our work is the first, to our knowledge, that provides such a tool for analyzing the intricacies of inferring LLMs on hardware devices, systematically collating the latest advancements in efficient LLM inference. We delve deep into deployment challenges, particularly emphasizing inference efficiency. Our discussion spans various areas, including model compression, decoding algorithm refinement, system-level and hardware-level enhancements, as illustrated in Figure 2. While there are concurrently related surveys in this domain, such as [Zhuet al., 2023] on LLM compression and [Miao et al., 2023a],[Ding et al., 2023] and [Wang et al., 2024a] on holistic LLM
serving, our work stands out by incorporating a Roofline model analysis.

In this paper, we first discuss the foundations of LLMs and develop a tool named LLM-Viewer, which uses Roofline model to analyze the bottleneck of deploying LLMs (Sec.2). LLM-Viewer can be used to analyze the deployment of any LLM architecture on various hardware platform, as shown in Figure 1. For the literature review, this survey categorizes strategies for improving LLM inference efficiency into four main areas: Model Compression (Sec.3), Algorithmic Methods for Fast Decoding (Sec.4), Compiler/System-Level Optimization (Sec.5), and Hardware-Level Optimization (Sec.6).

## 介绍

近年来，大型语言模型 (LLMs) 已成为人工智能进步的基石，重新定义了机器学习和自然语言处理 (NLP) 的格局 [Zhao et al., 2023]。这一趋势可以追溯到革命性模型 ChatGPT [Brown et al., 2020, Ouyang et al., 2022] 的成功，该模型通过其卓越的理解和生成能力产生了非常接近人类的文本。在 ChatGPT 之后，其他著名的 LLM，如 OPT [Zhang et al., 2022]、BLOOM [Scao et al., 2022] 和 Llama [Touvron et al., 2023a,b] 相继问世，进一步巩固了更大规模模型通常会带来更强大能力的共识。因此，拥有数百亿参数的模型正变得越来越普遍。由于这些模型的庞大规模，它们在推理时不仅对计算能力有限的设备提出了巨大挑战，对最先进的硬件设备也是如此。由于其复杂性和规模，以及它们对能源和计算的需求，这些模型在实际应用中难以部署。此外，这些模型的高资源需求引发了关于能源消耗、可扩展性和可访问性的担忧。对计算资源不如大公司丰富的小型组织和社区来说，这种情况尤其困难。因此，这些挑战突显了开发创新解决方案以使 LLM 推理更加普及和可持续的必要性。

为了解决 LLM 部署中的问题，已经有许多方法被提出。高效 LLM 推理这一领域在过去两年中发展迅猛，既带来了机遇也伴随着挑战。虽然研究数量的激增展示了该领域的活力，但也无意中掩盖了主要趋势和减缓进展。现有文献中的一个重要空白是**缺乏一个系统化且实用的框架，用于统一分析和制定全面解决方案**。为了填补这一空缺，我们的工作提供了一份详尽的 LLM 高效推理研究概览，并特别关注其实践导向的特性。**不同于传统的文献综述，我们不仅总结了现有研究，还引入了一个基于 Roofline 模型的框架，用于分析 LLM 部署中的瓶颈**。我们认为这是实现优化和实际应用的重要一步，如图 1 所示。这也是目前首个针对 LLM 在硬件设备上推理复杂性进行分析的工具，系统汇总了高效 LLM 推理领域的最新成果。
<img src="../images/llm_inference_unveiled_paper/figure1.png" width="60%" alt="Workflow of our designed LLM-Viewer.">

我们深入探讨了部署中的挑战，尤其是在推理效率方面。我们的讨论涉及多个领域，包括模型压缩、解码算法改进、系统和硬件层面的优化，如图 2 所示。尽管该领域也有相关的综述，如 [Zhuet al., 2023] 关于 LLM 压缩的研究，以及 [Miao et al., 2023a]、[Ding et al., 2023] 和 [Wang et al., 2024a] 关于 LLM 服务的研究，但我们的工作通过引入 Roofline 模型分析，展现了独特的价值。

<img src="../images/llm_inference_unveiled_paper/figure2.png" width="70%" alt="Workflow of our designed LLM-Viewer.">

> 高效LLM推理调查的思维导图。我们的综述与传统综述不同，重点关注 LLM 推理的实际应用。我们识别并分析了 LLM 推理过程中遇到的挑战，并引入了一种专门开发的 Roofline 模型，旨在找到推理过程中的瓶颈（第 2 节）。本综述将提升 LLM 推理效率的策略划分为四个核心方向：参数压缩（第 3 节）、快速解码算法设计（第 4 节）、系统层优化（第 5 节）以及硬件层优化（第 6 节），提供了一个系统的框架，帮助解决高效 LLM 部署的复杂问题。

本文首先介绍了 LLM 的基础，并开发了名为 `LLM-Viewer` 的工具，该工具**基于 Roofline 模型分析 LLM 部署的瓶颈**（第 2 节）。LLM-Viewer 可以用于分析任何 LLM 架构在各种硬件平台上的部署情况，如图 1 所示。在文献回顾中，本综述将 LLM 推理效率提升策略划分为四大领域：**模型压缩**（第 3 节）、**快速解码的算法方法**（第 4 节）、**系统级/编译器优化**（第 5 节）以及**硬件优化**（第 6 节）。

## 2. Delve into LLM Inference and Deployment
### 2.1. LLM Inference

Nowadays, the prevailing architecture adopted by most large language models (LLMs) is the Transformer decoder architecture. Here we will provide a concise overview of its fundamental structure, with the option to refer to this survey Zhao et al. [2023] for a more in-depth understanding. This structure comprises an embedding layer, a series of sequential Transformer layers, and a prediction head. Figure 3 demonstrated the architecture.

The embedding layer transform input tokens into the hidden states. The hidden states are sent to the Transformer layers. Each Transformer layer consists of two components.

Firstly, there is a masked multi-head attention module, denoted as MHA. Following MHA is a multi-layer perceptron submodule, labeled as MLP. The output from the last Transformer layer is then sent to the prediction head, which is responsible for predicting the next token after the input tokens.

Inference represents the process opposite to the training process. During training, a model learns from a vast dataset to capture the intricacies of language and context. The weights in model are updated. In contrast, during inference, a user inputs a prompt, and the LLM engages in a process of generating responses. This process involves the model utilizing its fixed pre-trained weights to comprehend the input text and produce text as output. The inference process of Large Language Models (LLMs) is divided into two stages: the Prefill Stage and the Decode Stage.

The Prefill Stage serves as the initial step in LLM inference. In this stage, the model takes a prompt sequence as input and engages in the generation of a key-value cache (KV cache) for each Transformer layer within the LLM. The KV cache plays a crucial role in storing and organizing information that the model deems relevant for subsequent token generation. Each Transformer layer is equipped with its own unique KV cache, and this prefilling process establishes the foundation for the subsequent decoding stage.

In the Prefill Stage, the Multi-Head Attention (MHA) creats key-value (KV) pairs that will be stored in the KV cache. Let’s denote the input to a Transformer layer as $X_{pre}\in R^{n\times d}$, where $d$ is the hidden size and $n$ is the length of prompt token sequence. The layers in the MHA have weights represented by $W_q$, $W_k$, $W_v$ and $W_o$. The query, key and value are computed through the following process:

$$\text{Query}: Q_{pre}=X_{pre} * W_{q} \\
\text{Key}: K_{pre}=X_{pre} * W_{k} \\ 
\text{Value}: V_{pre}=X_{pre} * W_{v}$$

The generated $K_{pre}$ and $V_{pre}$ are stored in the KV cache. The other computation in MHA can be formulated as:

$$O_{pre }=softmax((Q_{pre } * K_{pre }^{T})/sqrt(d)) * V_{pre } * W_{o}+X_{pre }$$

其中 MHA 的输出 $O_{pre }\in R^{n\times d}$ 被发送到 `MLP`。MLP 的输出作为下一个 Transformer 层的输入。

解码阶段代表了大型语言模型（LLM）推理过程的核心。在解码阶段，模型使用先前准备好的 $KV$ 缓存，并可能向其中添加新信息。这里的目标是生成 token （tokens），它们本质上是单词或单词的一部分。这是逐步发生的。每个新 token 的创建都受到之前生成的 token 的影响，就像逐字构建一个句子一样。

在解码阶段，MHA 加载先前存储的 **KV 缓存** $K_{cache}$ 和 $V_{cache}$。输入是 $X_{dec}\in R^{1\times d}$。**新的键值对被计算并连接到现有的缓存**：

$$\text{Query}: Q_{dec}=X_{dec}*W_{q} \\
\text{Key}: K_{cat }=[K_{cache }, X_{dec } * W_{k}] \\
\text{Value}: V_{cat }=[V_{cache }, X_{dec } * W_{v}]$$

这些新计算的 $X_{dec}\cdot W_{k}$ 和 $X_{dec}\cdot W_{v}$ 然后被附加到 $KV$ 缓存。MHA 中的其他计算如下进行：

$$O_{dec}=\text{softmax}(\frac{Q_{dec}\cdot K_{cat}^{T}}{sqrt(d)}) * V_{cat } * W_{o}+X_{dec}$$

其中 MHA 的输出 $O_{dec}\in R^{1\times d}$ 被发送到 MLP。最后一个 Transformer 层的输出被发送到最终预测层以预测下一个 token 。

### 2.2. Roofline Model

Assessing the efficiency at which LLMs deploy onto specific hardware involves a comprehensive consideration of both hardware and model characteristics. To conduct this evaluation, we employ the Roofline model. The Roofline model serves as an effective theoretical framework to assess the potential performance of deploying a model on particular hardware.

As shown in Figure 4, the execution of a neural network layer on hardware devices entails the transfer of data from memory (DDR or HBM) to on-chip buffers, followed by computations performed by on-chip processing units, ultimately outputting results back to memory. Therefore, evaluating performance requires simultaneous consideration of memory access and processing unit capabilities. If a layer involves extensive computations but minimal memory access, it is termed a computation bottleneck. This scenario leads to idle on the memory access. On the contrary, when a layer requires substantial memory access with fewer computational demands, it is referred to as a memory bottleneck. In this case， computational units remain underutilized. We can clearly distinguish between these two scenarios according to the Roofline model and provide performance upper bounds for different situations. 

There are two steps to using the Roofline model:

1. Plot the Roofline: Determine the peak computational

performance (operations per second, OPS) and peak memory bandwidth (bytes per second) specific to the target hardware device.2 Then create a graph with performance (OPS) on the y-axis and arithmetic intensity (OPs/byte) on the xaxis: Draw a horizontal line equal to the peak computational performance. This line represents the maximum achievable performance by the hardware device. And draw a diagonal line from the origin with a slope equal to the peak memory bandwidth. This line represents the maximum memory bandwidth available on the system, known as the memory Roofline. Figure 5 demonstrates the Roofline model of
Nvidia A6000 GPU.

2. Analyze performance for layers: Evaluate the performance of each layer in the model by quantifying both

the number of operations (OPs) and the volume of data accessed from memory (bytes). Calculate the arithmetic intensity (OPs/byte) of each layer by dividing the required operations by the amount of data transferred. According to the graph created in the first step, the theoretical max performance for each layer is determined by the position on the graph corresponding to the x-axis value of arithmetic intensity. It allows us to ascertain whether the system is memory-bound or compute-bound at this point, guiding the determination of the subsequent optimization strategy.

There are two scenarios where resources are not fully utilized: When the model’s computational intensity is below the turning point, residing in the red zone, it implies that the computational workload required per memory access is low. Even saturating the peak bandwidth does not fully utilize all computational resources. In such cases, the layer is constrained by memory access (memory-bound), and some computational units may remain idle. If the layer is memory-bound, consider optimization techniques such as quantization, kernel fusion and increasing batch size to alleviate the memory footprint. Conversely, if the model’s computational intensity is above the turning point, situatedin the green zone, it suggests that the model requires only a small amount of memory access to consume a significant amount of computational capability. It implies that the layer is constrained by computation (compute-bound), with some memory units potentially remaining idle. In this case, we should investigate strategies such as enabling low-bit computation to enhance computational efficiency. Detailed explanations of these methods will be provided in the subsequent sections.

As an example, Table 1 presents the analysis of layers in Llama-2-7b using the Roofline model on the Nvidia A6000 GPU. From the table, we observe that during the prefill stage, the majority of computations are compute-bound, leading to high performance. Conversely, in the decode stage, all computations are memory-bound, resulting in performance significantly below the computational capacity of the GPU’s computation units. During the user’s interaction with large models, the prefill stage executes only once, while the decode stage is repeatedly performed to generate a continuous output. Therefore, optimizing for the memoryboundcharacteristics of the decode stage becomes crucial for enhancing the inference performance of large models.

### 2.3. LLM-Viewer

There are multiple Transformer layers in LLMs, each containing various operations. Moreover, different LLMs have different sets of operations. Additionally, we need to track information like memory footprint to calculate the peak memory usage and total inference time. Hence, analyzing LLMs involves examining network-wide concerns. In this section, we propose a powerful tool, LLM-Viewer 3, to execute the network-wise analysis. It enables the analysis of LLM performance and efficiency on various hardware platforms, offering valuable insights into LLM inference and performance optimization.

The workflow of LLM-Viewer is depicted in Figure 1. It consists of the following steps: (1) Input the LLM and gather essential information about each layer, such as the computation count, input and output tensor shapes, and data dependencies. (2) Provide input for the hardware and generate a Roofline model that takes into account the computation capacity and memory bandwidth of the hardware. (3) Configure the inference settings, including the batch size, prompt token length, and generation token length. (4) Configure the optimization settings, such as the quantization bitwidth, utilization of FlashAttention, decoding methods, and other system optimization techniques. (5) The LLMViewer Analyzer utilizes the Roofline model and layer information to analyze the performance of each layer. It also tracks the memory usage of each layer and calculates the peak memory consumption based on data dependencies. By aggregating the results of all layers, the overall network performance of LLM can be obtained. (6) Generate a report that provides information such as the maximum performance and performance bottlenecks of each layer and the network, as well as the memory footprint. Analyzing curves, such as batch size-performance and sequence length-performance curves, can be plotted from the report to understand how different settings impact performance. (7)LLM-Viewer offers a web viewer that allows convenient visualization of the network architecture and analysis results. This tool facilitates easy configuration adjustment and provides access to various data for each layer.

## 2. 深入研究 LLM 推理和部署

### 2.1 LLM 推理

现在，大多数大型语言模型（LLM）都采用 Transformer 解码器架构。本文只提供基本结构的简要概述，更详细内容可参考 [Zhao et al. [2023]](https://arxiv.org/pdf/2303.18223) 这篇综述。该结构由**一个嵌入层**、**一系列连续的 Transformer 层**和**一个预测头**组成。图 3 展示了这一架构。

![LLMs 架构](../images/llm_inference_unveiled_paper/llama_architecture.png)

**Embedding 层将输入 token 序列（整数序列）转化为 embedding 向量/张量（大小为 $d$ 的向量）**，Embedding 向量/张量 被传递给 Transformer 层。每个 Transformer 层包含两个组件：首先是一个被称为 `MHA` 的掩码多头注意力模块，紧接着是一个被称为 `MLP` 的多层感知器子模块。最后一个 Transformer 层的输出被传递到**预测头（线性层 + softmax 层）**，负责在输入 token 之后预测下一个 token 。

推理是与训练过程相反的过程。在训练期间，模型通过庞大的数据集学习，捕捉语言和上下文的复杂性，模型的权重会被更新。而在推理阶段，用户输入一个提示词（`prompt`），模型根据预训练时学习到的固定权重进行理解，并**迭代式输出生成文本**。LLM 的推理过程主要分为两个阶段：**预填充阶段和解码阶段**。

预填充阶段是 LLM 推理的初始步骤。在此阶段，模型将提示序列作为输入，并为 LLM 的每一层 Transformer 生成键值缓存（KV 缓存）。KV 缓存在存储和组织模型认为对后续生成 token 重要的信息方面起着至关重要的作用。**每个 Transformer 层都有其独特的 KV 缓存**，预填充过程为随后的解码阶段奠定了基础。

**在预填充阶段，多头注意力（`MHA`）模块生成 `KV` 键值对并存储在 KV 缓存中**。设输入到 Transformer 层的输入为 $X_{pre}\in R^{n\times d}$，其中 $d$ 是隐藏维度，$n$ 是提示 token 序列的长度。`MHA` 模块的 $4$ 个线性层权重用 $W_q$，$W_k$，$W_v$ 和 $W_o$ 表示。查询、键和值（Q、K、V）的计算过程如下：

$$\text{Query}: Q_{pre}=X_{pre} * W_{q} \\
\text{Key}: K_{pre}=X_{pre} * W_{k} \\
\text{Value}: V_{pre}=X_{pre} * W_{v}$$

生成的 $K_{pre}$ 和 $V_{pre}$ 被存储在 KV 缓存中。其余的 `MHA` 计算如下：

$$O_{pre }=\text{softmax}(\frac{Q_{pre } * K_{pre }^{T}}{\sqrt{d}}) * V_{pre } * W_{o}+X_{pre }$$

MHA 的输出 $O_{pre }\in R^{n\times d}$ 将传递到 MLP。MLP 的输出作为下一层 Transformer 层的输入。

解码阶段是 LLM 推理的关键部分。在这一阶段，模型使用预填充阶段生成的 **KV 缓存**，同时逐步添加新信息。目标是**逐步生成新的 token **，每个新 token 的生成都会参考之前生成的 token ，从而逐字逐句地完成文本输出。


**在解码阶段**，MHA 加载先前存储的 KV 缓存 $K_{cache}$ 和 $V_{cache}$。输入为 $X_{dec}\in R^{1\times d}$。新的键值对被计算并连接到现有缓存：

$$\text{Query}: Q_{dec}=X_{dec}*W_{q} \\
\text{Key}: K_{cat }=[K_{cache }, X_{dec } * W_{k}] \\
\text{Value}: V_{cat }=[V_{cache }, X_{dec } * W_{v}]$$

这些新计算的 $X_{dec}\cdot W_{k}$ 和 $X_{dec}\cdot W_{v}$ 然后被附加到 $KV$ 缓存。MHA 中的其他计算如下进行：

$$O_{dec}=\text{softmax}(\frac{Q_{dec}\cdot K_{cat}^{T}}{\sqrt{d}}) * V_{cat } * W_{o}+X_{dec}$$

其中 MHA 的输出 $O_{dec}\in R^{1\times d}$ 被传递到 MLP。最后一个 Transformer 层的输出被发送到最终的预测层，以预测下一个 token 。

### 2.2. Roofline 模型

**评估大型语言模型（LLM）在特定硬件上的部署效率需要全面考虑硬件和模型的特性**。为此，我们采用 `Roofline` 模型。Roofline 模型是一个有效的理论框架，用于评估模型在特定硬件上的潜在性能。

如图 4 所示，神经网络层在硬件设备上的执行涉及将数据从内存（DDR 或 HBM）传输到片上缓冲区，随后由片上处理单元执行计算，最终将结果输出回内存。因此，**评估性能需要同时考虑内存访问和处理单元的能力**。如果某一层涉及大量计算但很少的内存访问，我们称之为计算瓶颈。在这种情况下，内存访问处于空闲状态。相反，当某一层需要大量内存访问但计算需求较少时，则称为内存瓶颈。这种情况下，计算单元未被充分利用。我们可以通过 Roofline 模型清晰地区分这两种情况，并为不同情况提供性能上限。

<img src="../images/llm_inference_unveiled_paper/figure4.png" width="70%" alt="Execution of an operation on hardware.">

使用 Roofline 模型有两个步骤：

1. **绘制 Roofline 图**：确定目标硬件设备的**峰值计算性能**（每秒操作数，OPS）和**峰值内存带宽**（每秒字节数）。然后创建一个图表，其中 y 轴为性能（OPS），x 轴为算术强度（OPs/byte）。绘制一条等于硬件设备峰值计算性能的水平线，这条线代表设备的最大可达性能。同时从原点绘制一条**斜率等于峰值内存带宽**的斜线，这条线代表系统可用的最大内存带宽，称为内存 Roofline。图 5 展示了 Nvidia A6000 GPU 的 Roofline 模型。

<img src="../images/llm_inference_unveiled_paper/figure5.png" width="65%" alt="a6000 roofline 模型">

2. **分析各层性能**：通过量化**每层的操作数**（OPs）和从**内存访问的数据量**（字节数）来评估模型各层的性能。通过将所需操作数除以传输的数据量来计算每层的算术强度（OPs/byte）。根据第一步创建的图表，确定每层在算术强度对应的 x 轴位置上的理论最大性能。这使我们能够判断系统在此点是内存受限（memory-bound）还是计算受限（compute-bound），从而指导后续优化策略的制定。

资源未充分利用的情况有两种：

1. **内存受限（memory-bound）**：当模型的计算强度低于拐点时，处于红色区域，意味着每次内存访问所需的计算量较低。即使达到了峰值内存带宽，计算资源也无法得到充分利用。在这种情况下，该层受到**内存访问限制（内存瓶颈）**，部分计算单元可能处于闲置状态。针对内存受限的情况，可以考虑以下优化技术来减小内存占用：
	- **量化**（quantization）：通过将模型权重或激活值降低为低位表示（如 8 位或 4 位），减少内存带宽和存储需求，从而提高内存访问效率。
	- **内核融合**（kernel fusion）：将多个独立的计算内核操作合并为一个，以减少内存的读写次数，并降低总的内存访问需求。
	- **增大批处理大小**（increasing batch size）：通过增大一次性处理的输入数据量，增加计算密度，从而更有效地利用内存和计算资源。
2. **计算受限（compute-bound）**：当模型的计算强度高于拐点，位于绿色区域时，表明模型仅需少量的内存访问即可消耗大量的计算能力。这意味着该层受到计算限制，部分内存资源可能未被充分利用。在这种情况下，可以通过以下方法提高计算效率：
	- **启用低比特计算单元**（low-bit computation）：使用低位的表示形式（如 4 位或 8 位）进行计算，以减少处理单元的计算负担，提升计算效率。（后续章节将对这些方法进行详细解释）

举个例子，表 1 展示了使用 Roofline 模型在 Nvidia A6000 GPU 上分析 Llama-2-7b 各层的结果。我们观察到，在预填充阶段，大多数计算是计算瓶颈，导致性能较高。相反，在解码阶段，所有计算都受到内存瓶颈限制，导致性能显著低于 GPU 计算单元的计算能力。在用户与大型模型交互时，预填充阶段只执行一次，而解码阶段则需要反复执行以生成连续输出。因此，**优化解码阶段的内存受限特性对于提升大型模型的推理性能至关重要**。

<img src="../images/llm_inference_unveiled_paper/table1.png" width="55%" alt="使用 Nvidia A6000 GPU 的 Roofline 模型对 Llama-2-7b 中的层进行分析。在此示例中，序列长度为 2048，批处理大小为 1。">

在大型语言模型（LLM）中，存在多个 Transformer 层，每层执行不同的操作，并且不同的 LLM 使用不同的操作组合。此外，我们还需要跟踪如内存使用等信息，以计算**最高内存消耗和总推理时间**。因此，分析 LLM 需要考虑整个网络的多个因素。为此，我们提出了一款强大的工具—`LLM-Viewer`，用于进行全网络的分析。它可以**帮助评估 LLM 在不同硬件平台上的表现和效率**，从而提供有关推理过程和性能优化的宝贵见解。

LLM-Viewer 的工作流程如图 1 所示，主要包括以下几个步骤：

（1）输入 LLM 并收集每层的基本信息，例如计算次数、输入和输出张量的形状，以及数据依赖关系。
（2）提供硬件信息，生成考虑计算能力和内存带宽的 Roofline 模型。
（3）配置推理设置，如批量大小、提示令牌长度和生成令牌长度。
（4）配置优化设置，例如量化位宽、FlashAttention 的使用、解码方法和其他系统优化技术。（5）LLMViewer 分析器利用 Roofline 模型和层信息来评估每层的性能，同时跟踪每层的内存使用，并根据数据依赖关系计算峰值内存消耗。通过汇总所有层的结果，可以得到 LLM 的整体性能表现。
（6）生成报告，展示每层和整个网络的最大性能及瓶颈，并提供内存使用情况。报告中还可以绘制性能曲线，如批量大小与性能关系图和序列长度与性能关系图，以帮助理解不同设置对性能的影响。
（7）LLM-Viewer 提供了一个 Web 查看器，方便可视化网络架构和分析结果，使配置调整更加便捷，并提供对每层各种数据的访问。

## 3. Model Compression

The formidable size and computational demands of Large Language Models (LLMs) present significant challenges for practical deployment, especially in resource-constrained environments. To alleviate these limitations, the most straightforward solution is to compress the LLMs. In this section, we review the concept of neural network compression for LLMs. This exploration encompasses a thorough examination of well-established techniques, including but not limited to quantization, pruning, knowledge distillation, and low-rank factorization. In each subsection, we will utilize LLM-Viewer to analyze the impact of network compression on LLM inference. Based on our analysis, we will provide optimization recommendations.

### 3.1. Quantization

In the realm of LLM compression, quantization has become a pivotal technique for mitigating the substantial storage and computational overhead associated with these models. Essentially, quantization involves transforming the floating-point values in original LLMs into integers or other discrete forms, a process that considerably reduces both storage requirements and computational complexity [Gholami et al., 2022]. While some degree of precision loss is inherent in this process, carefully designed quantization techniques can achieve significant model compression with minimal impact on accuracy. Quantization in the context of LLMs can be primarily categorized into two directions: Quantization for Compressing Pre-trained LLMs and Quantization for Parameter-Efficient Fine-Tuning (QPEFT). 

The first caxtegory encompasses approaches that apply quantization to LLMs for using the quantized LLMs as pre-trained models. This category can be further divided into two subcategories: Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT integrates quantization into the model’s training process or during the fine-tuning/re-training of a pre-trained LLM, allowing the model to adapt to the quantization from the onset. In contrast, PTQ applies quantization to a model after it has completed its training phase, offering a more straightforward approach to model compression without the need for retraining. These distinct methodologies highlight the versatility of quantization techniques in addressing the specific needs and constraints of LLM deployment.

#### 3.1.1 A Use Case of LLM-Viewer:Roofline Analysis for Quantization

Here we provide an example of how to use our LLM-Viewer (Section 2.3) to analyze the bottlenecks of LLM deployments. In LLMs, tensors consist of weights and activations, with activations including temporary activations and KV cache. (1) LLM weights must be stored in memory. For example, Llama-13b [Touvron et al., 2023a], which has 13 billion weights, occupies approximately 26GB of memory in FP16 format. (2) temporary activations are generated during inference. For example, the inputs of each transformer layer are kept in memory until the residual addition is executed. (3) for auto-regressive LLMs, caching key and value activations (KV cache) into memory is necessary for subsequent token generation. We utilize LLM-Viewer to analyze the effects of quantization on these tensors from three perspectives: computation, memory consumption, and memory access.

**Computation**: The latest computing devices, such as NVIDIA GPUs, generally support FP32, FP16, and INT8 data types for computation. Hardware devices generally perform better when processing data with smaller bit widths. NVIDIA’s A6000 GPU, for example, is capable of performing twice as fast as FP16 with 155 TOP/s and 310 TOP/s, respectively. In the Roofline model, when enabling quantization for faster computation, the roofline height increases, indicating improved performance for computebound layers. As shown in Figure 6, the max performance improved when using INT8 computation. However, to utilize the computational power of INT8, all input operands must be in INT8 format. Consequently, if only the weights are quantized to INT8 while the activations remain in FP16 format, the INT8 computational power cannot be utilized.

Instead, the INT8 weights would need to be converted to FP16 for multiplication with FP16 activations. Furthermore, when tensors are quantized to a bitwidth that is not supported by the hardware, they need to be converted to higher bit widths for computation. For example, the NVIDIA H100 GPU does not support INT4 computation. Consequently, if the weight or activation is quantized to INT4, it would require conversion to a higher bit width, such as INT8 or FP16, for computation.

**Memory Consumption**: The memory consumption reduction resulting from quantizing different tensors varies, as shown in Figure 74. Notably, the memory usage of temporary activations is relatively low, especially during the decode stage. This can be attributed to their short lifespan, allowing their memory to be released once their purpose is fulfilled. On the other hand, the memory allocated for the KV cache behaves differently. It cannot be freed until the entire process of generating a complete answer is finished, which entails multiple inference passes through the network. Additionally, the memory consumption of the KV cache increases as the batch sizes grow larger and the input sequences become longer. This is because the model needs to store a greater number of key-value (KV) pairs to facilitate its operations。

Memory Access: Quantizing tensors in LLM can significantly reduce memory access, resulting in fewer data bytes to be moved for the same amount of computation. This increase in arithmetic intensity contributes to the Roofline model, leading to three scenarios: 

(1) After quantization, the arithmetic intensity remains within the memory-bound range. With the improvement in arithmetic intensity, the average data access per computation is reduced, alleviating the pressure on data memory access. Consequently, the theoretical performance is enhanced. This can greatly boost the performance during the memory-bound decode stage.

(2) The arithmetic intensity transitions from being memorybound to compute-bound. This shift also reduces the pressure on data memory access, resulting in improved theoretical performance. (3) Both before and after quantization, the arithmetic intensity remains within the compute-bound range. In this case, there is no performance improvement. For example, this scenario can occur during the computebound prefill stage or when the batch size is large in the decode stage.

As depicted in Figure 8, when the batch size is small, the layers in the network are memory-bound both before and after quantization. Therefore, quantization can enhance performance and reduce the network’s inference time. However, when the batch size is large, compressing the network’s weights from 4 bits to 2 bits or 1 bit does not lead to a decrease in the inference time. This is because, at this point, the network is already compute-bound, and quantizing the weights becomes ineffective. Similar to the previous scenario, the behavior of the system can exhibit saturation effects in prefill stage. As shown in Figure 9, when the sequence length is relatively small, the prefill stage is memory-bound. In this case, applying quantization can enhance the performance by reducing the memory access requirements of the network. However, as the sequence length increases, the prefill stage becomes more compute-bound. Consequently, quantizing the weights may not yield significant improvements in performance when the network is already compute-bound during the prefill stage with large sequence lengths.

#### 3.1.2 Quantization for Compressing Pre-trained

##### LLMs In Quantization-Aware Training (QAT) 

LLMs In Quantization-Aware Training (QAT)[Choi et al., 2018,Courbariaux et al., 2015, Dong et al., 2019], the quantization process is seamlessly integrated into the training of Large Language Models (LLMs), enabling them to adapt to low-precision representations and thus mitigating precision loss. LLM-QAT [Liu et al., 2023b] innovatively addresses the challenge of training data acquisition for LLMs through data-free distillation, which leverages outputs from a pretrained model to obviate the need for extensive data collection. Furthermore, LLM-QAT expands quantization beyond weights and activations to include key value (KV) caches, enhancing throughput and supporting longer sequence dependencies. Its successful distillation of large Llama models to 4-bit quantized weights and KV caches underscores the potential for accurately quantized 4-bit LLMs.

To attain lower-bit quantization, such as below 2-bit,Kim et al. [2023b] introduce Token-Scaled Logit Distillation (TSLD) for ternary QAT in LLMs. This method employs an adaptive knowledge distillation technique that modifies Logit Knowledge Distillation based on token confidence, providing tailored guidance during LLM QAT. Furthermore, Shang et al. [2024] focus on salient weights with their concept of partially binarized matrices in PB-LLM. By preserving these crucial weights in higher bits, PB-LLM effectively maintains the reasoning capacity of heavily quantized LLMs. Additionally, PB-LLM explores minimizing quantization error by determining the optimal scaling factors for binarized LLMs, a vital step in preserving the effectiveness of models under aggressive quantization.

##### Post-Training Quantization (PTQ)

Post-Training Quantization (PTQ) represents a crucial technique in optimizing Large Language Models (LLMs), entailing the quantization of model parameters post the LLM’s training phase. The primary goal of PTQ is to reduce both the storage requirements and computational complexity of the LLM, without necessitating alterations to the model’s architecture or embarking on a retraining process.

This approach stands out for its simplicity and efficiency, particularly in achieving significant model compression. In the context of LLMs, which typically contain billions of parameters, Quantization-Aware Training (QAT) often becomes impractical due to excessive training costs. Hence, PTQ emerges as a more viable solution for these large-scale models. However, it’s crucial to acknowledge that PTQ can lead to a certain degree of precision loss as a consequence of the quantization process. Despite this, PTQ serves as an effective method to enhance the efficiency of LLMs, offering a straightforward solution that avoids major modifications or extensive additional training.

In PTQ, various approaches focus on weight-only quantization to enhance efficiency. For instance, `LUTGEMM`[Park et al., 2023] optimizes matrix multiplications in LLMs using weight-only quantization and the BCQ format, thereby reducing latency and improving computational efficiency. LLM.int8() [Dettmers et al., 2022] employs 8-bit quantization, which halves GPU memory usage during inference and maintains precision through vectorwise quantization and mixed-precision decomposition. This method enables efficient inference in models up to 175 billion parameters. ZeroQuant [Yao et al., 2022] combines a hardware-friendly quantization scheme with layer-by-layer knowledge distillation, optimizing both weights and activations to INT8 with minimal accuracy loss. Addressing higher compression targets, GPTQ [Frantar et al., 2022] introduces a layer-wise quantization technique based on approximate second-order information, achieving a reduction to 3-4 bits per weight with minimal accuracy loss. Additionally, the study by Dettmers and Zettlemoyer [2023] explores the balance between model size and bit precision, particularly for zero-shot performance, finding that 4-bit precision generally offers the optimal balance. Innovations like AWQ [Kim et al., 2023c, Lin et al., 2023] highlight that protecting a small percentage of salient weights can significantly reduce quantization error. AWQ uses an activationaware approach, focusing on weight channels with larger activation magnitudes, and incorporates per-channel scaling for optimal quantization. OWQ [Lee et al., 2023] analyzes how activation outliers amplify quantization error, introducing a mixed-precision scheme to assign higher precision to weights affected by these outliers. `SpQR` [Dettmers et al.,2023b] takes a unique approach by isolating outlier weights for storage in higher precision, while compressing the remainder to 3-4 bits. This technique allows for more efficient compression while maintaining near-lossless performance. QuantEase [Behdin et al., 2023] suggests using a coordinate descent approach to optimize all of the weights in network, improving the efficiency of quantization. To achieve even lower-bit quantization (e.g., below 2-bit), QuIP [Chee et al., 2023] introduces an innovative approach that accounts for the even distribution of weight magnitudes and the significance of accurately rounding directions unaligned with coordinate axes. QuIP comprises an adaptive rounding procedure that minimizes a quadratic proxy objective, essential for optimizing the quantization process. Additionally, it employs efficient pre- and postprocessing techniques that ensure weight and Hessian incoherence through multiplication by random orthogonal matrices, crucial for maintaining quantization effectiveness.

Further advancing PTQ methods, Li et al. [2023a] are inspired by the observation that aligning the quantized activation distribution with its floating-point counterpart can restore accuracy in LLMs. Their proposed ’Norm Tweaking’ strategy involves a meticulous calibration data generation process and a channel-wise distance constraint. This approach updates the weights of normalization layers, leading to enhanced generalization capabilities. [Shang et al.,2024] propose partial-binarized LLM (PB-LLM) by introducing binarization [Hubara et al., 2016] into LLM quantization to push weight quantization under 2 bits. Following PB-LLM, BiLLM [Huang et al., 2024] pushes weight quantization to almost 1 bit. Apart from efforts that focus solely on weight quantization in LLMs, numerous PTQ approaches focus on weights and activations quantization. SmoothQuant [Xiao et al.,2023a] addresses the challenge of quantizing activations, which can be complex due to the presence of outliers. It introduces a per-channel scaling transformation that effectively smooths out activation magnitudes, rendering the model more receptive to quantization. Recognizing the intricacies of quantizing activations in LLMs, `RPTQ` [Yuanet al., 2023c] highlights the uneven ranges across channels and the prevalence of outliers. RPTQ’s innovative approach involves clustering channels for quantization, thereby reducing discrepancies in channel ranges. This method smartly integrates channel reordering into layer normalization and linear layer weights to minimize overhead.

OliVe [Guo et al., 2023a] adopts an outlier-victim pai(OVP) quantization strategy, focusing on local handling of outliers with low hardware overhead and significant performance benefits. This approach stems from the understanding that outliers are crucial, while adjacent normal values are less so. Building on this, Outlier Suppression+ extends the concept by addressing asymmetrically distributed harmful outliers in specific channels. It introduces channel-wise shifting and scaling operations to balance the outlier distribution and reduce the impact of problematic channels, considering both the nature of the outliers and the subsequent quantization errors. ZeroQuant-FP [Wu et al., 2023d] delves into floating-point (FP) quantization, specifically exploring FP8 and FP4 formats. This study finds that FP8 activation quantization in LLMs outperforms the traditional INT8 format, while FP4 weight quantization shows comparable efficacy to INT4. ZeroQuant-FP addresses the divergence between weights and activations by standardizing all scaling factors as powers of 2 and restricting them within a single compute group, ensuring consistency and efficiency in the quantization process. Li et al. [2023c] propose FPTQ, in which they employ a layerwise strategy to cope with different levels of quantization difficulty.Particularly, they devises an offline logarithmic activation equalization to render a quantization-friendly distribution for previously intractable layers.

Since the end of 2023, the length of tokens has been significantly increasing, causing the KV cache to consume more memory. For instance, Google Gemini 1.5 [Sundar Pichai, 2024] can handle up to 1 million tokens in production, and LLMs processing books, large images or videos will require tens of thousands of tokens. As a result, the optimization of KV Cache Quantization has become increasingly important. Several recent papers in 2024 have focused on improving KV cache quantization. For example, Hooper et al. [2024] propose a solution for achieving 10 million context length LLM inference with KV Cache Quantization. KIVI [Liu et al., 2024b] pushes the quan-tization of KV cache to 2-bit. Yue et al. [2024] proposes WKVQuant as to jointly optimize the quantization of both the weights and the KV cache in LLMs, making W4KV4 have the same performance as W4. As shown in Figure 11, we use LLM-Viewer to analyze the memory reduction of KV cache quantization. We can observe that when the sequence length is larger than 50k, the KV cache takes most of the memory and its quantization can significantly decrease the memory consumption.

#### 3.1.3 Quantization for Parameter Efficient FineTuning (Q-PEFT)

Parameter Efficient Fine-Tuning (PEFT) is an important topic for LLMs. One of the most popular approaches is lowrank adaptation (LoRA) [Hu et al., 2021, Valipour et al.,2022], where the key insight is to decompose the adapter weights into the multiplication of two low-rank (and thus parameter-efficient) matrices. LoRA has claimed comparable performance to full fine-tuning while using much fewer learnable parameters. Please refer to the review paper [Huet al., 2023] for more details about this adaptor.

In addition to the well-defined quantization paradigms, a novel paradigm in LLM efficiency is emerging: Quantization for Parameter-Efficient Fine-Tuning (Q-PEFT). This approach integrates quantization into the fine-tuning process of LLMs, offering a unique and efficient method, particularly relevant in the era of large models. Pioneering works in this paradigm, such as PEQA [Kim et al., 2023a], DFT [Li et al., 2023e], and QLORA [Dettmers et al., 2023a] demonstrate the feasibility and effectiveness of this approach. PEQA employs a dual-stage process where the first stage involves quantizing the parameter matrix of each fully connected layer into a matrix of low-bit integers coupled with a scalar vector. The second stage focuses on finetuning the scalar vector for specific downstream tasks, allowing for more efficient task-specific adjustments. DFT adopts the efficient Lion optimizer, which only keeps track of the momentum and has consistent update magnitudes for each parameter, an inherent advantage for robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient flow and parameter update scheme for the quantized weights. On the other hand, QLORA introduces novel concepts such as a new data type, double quantization, and paged optimizers. These innovations aim to conserve memory efficiently while maintaining LLM fine-tuning performance. Notably, QLORA facilitates large model fine-tuning on a single GPU, achieving state-of-the-art results on the Vicuna benchmark, a testament to its effectiveness in balancing memory efficiency and model performance.

However, a limitation of QLoRA is its restriction to at most 4-bit quantization during fine-tuning; lower-bit quantization, such as 2-bit, can significantly deteriorate the performance. Addressing this challenge, several studies have ventured into the realm of Q-PEFT to enable lower-bit quantization. LQ-LoRA [Guo et al., 2023b] introduces an iterative algorithm that decomposes each pretrained matrix into a high-precision, low-rank component and a memoryefficient quantized component. During fine-tuning, only the low-rank component is updated, keeping the quantized component fixed. This method presents an integer linear programming approach for the quantization component, allowing dynamic configuration of quantization parameters like bit-width and block size within a given memory budget. Another notable approach, Loft-Q [Li et al., 2023d], simultaneously quantizes an LLM and establishes a suitable low-rank initialization for LoRA fine-tuning. This strategy effectively bridges the gap between the quantized and fullprecision models, significantly enhancing generalization in downstream tasks. QA-LoRA [Xu et al., 2023c] leverages the benefits of quantizing the LLM’s weights into low-bit integers, facilitating an efficient fine-tuning stage. Additionally, it produces a lightweight, fine-tuned model, circumventing the accuracy loss often associated with PTQ.

#### 3.1.4 Discussion on LLM Quantiztaion

Figure 10 presents a timeline of LLM quantization techniques, highlighting the evolution from Post-Training Quantization (PTQ) as the initial mainstream approach to the rising prominence of Quantization-Aware Training (QAT) and Quantization for Parameter-Efficient FineTuning (Q-PEFT). This shift underscores the community’s adaptation in response to the performance bottlenecks encountered with PTQ, marking QAT and Q-PEFT as the burgeoning areas of focus in the quest for efficient LLM inference.

## 3. 模型压缩

LLMs 由于其庞大的规模和内存、计算需求，在实际部署中，尤其是在资源有限的环境下，面临着显著的挑战。**压缩 LLM** 是解决这些挑战的直接办法。在本节中，我们将探讨 LLM 的神经网络压缩技术，包括**量化、剪枝、知识蒸馏和低秩分解**等成熟方法。每个小节中，我们都会使用 LLM-Viewer 来分析网络压缩对 LLM 推理性能的影响，并根据分析结果提供优化建议。

### 3.1. 量化

在 LLM 压缩领域，量化已经成为减轻这些模型存储和计算开销的关键技术。**量化主要是将原始 LLM 中的浮点值转换为整数或其他离散形式，其可以显著降低存储需求和计算复杂度 [Gholami et al., 2022]**。虽然这一过程中不可避免地会有一些精度损失，但通过精心设计的量化方法，可以在对模型准确性影响很小的情况下实现显著的压缩。针对 LLM 的量化主要有两个方向：**压缩预训练 LLM 的量化**和**参数高效微调（QPEFT）的量化**。

对预训练的 `LLM` 进行量化，方便使用量化后的模型，这一类又可以细分为：
- 量化感知训练（`QAT`）: QAT 是将量化集成到模型的训练过程或预训练模型的微调过程中，从而使模型从一开始就适应量化。
- 训练后量化（`PTQ`）: PTQ 是在模型完成训练后进行量化，这种方法更为直接，无需重新训练即可实现模型压缩。

#### 3.1.1 LLM-Viewer 的应用案例：量化的 Roofline 分析

下面我们展示了如何使用 LLM-Viewer（第 2.3 节）来分析 LLM 部署中的瓶颈。在 LLM 中，**张量包括权重和激活，激活包括临时激活和 KV 缓存**。
（1）LLM 的权重必须存在内存中存储。例如，Llama-13b [Touvron et al., 2023a] 拥有 130 亿个权重，在 `FP16` 格式下占用约 26GB 的内存。
（2）在推理过程中，会生成临时激活。例如，每个 Transformer 层的输入会一直保存在内存中，直到执行残差加法。
（3）对于自回归 LLM，需要将 KV 键值对（KV 缓存）缓存到内存中，以便生成后续的令牌。

我们将利用 LLM-Viewer 从**计算、内存消耗和内存访问**三个方面来分析量化对这些张量的影响。

<img src="../images/llm_inference_unveiled_paper/figure6.png" width="65%" alt="演示 Nvidia A6000 GPU 针对不同计算数据类型的 Roofline 模型">

**计算**：现代计算设备，如 NVIDIA GPU，通常支持 FP32、FP16 和 INT8 数据类型。**处理较小位宽的数据通常能获得更好的性能**。例如，NVIDIA 的 A6000 GPU 在 FP16 计算时的性能是 FP32 的两倍，分别为 155 TOP/s 和 310 TOP/s。**在 Roofline 模型中，启用量化可以提高计算强度，进而提升计算密集层的性能**。如图 6 所示，使用 INT8 计算时的最大性能有所提升。**但是，要发挥 INT8 的计算优势，所有输入操作数必须为 INT8 格式**。因此，如果仅量化权重为 INT8，而激活仍保持 FP16 格式，INT8 的计算能力是无法得到有效利用的。相反，INT8 权重需要转换为 FP16 才能与 FP16 激活进行乘法。此外，当张量的位宽超出硬件支持范围时，需要转换为更高的位宽进行计算。例如，NVIDIA H100 GPU 不支持 INT4 计算。因此，如果权重或激活被量化为 INT4，则需要转换为更高的位宽，如 INT8 或 FP16，才能进行计算。

<img src="../images/llm_inference_unveiled_paper/figure7.png" width="65%" alt="Llama-2-13b 不同量化设置的相对内存消耗。Tmp Act 表示临时激活">

**内存消耗**：量化对不同张量的内存消耗减少程度各异，如图 7 所示。特别是，临时激活的内存使用量较低，特别是在解码阶段，这主要因为它们的生命周期较短，一旦用途完成即可释放内存。而 KV 缓存的内存分配则表现不同。KV 缓存不能释放，直到生成完整的答案，这需要多次通过网络进行推理。此外，KV 缓存的内存消耗随着批量大小和输入序列长度的增加而增加，因为模型需要存储更多的关键值（KV）对以支持操作。



**内存访问**：量化张量可以显著减少内存访问量，从而减少相同计算量所需移动的数据字节数。这种计算强度的提升对 Roofline 模型有贡献，导致三种情况：
* **计算强度仍在内存绑定范围内**。随着计算强度的提高，每次计算的平均数据访问量减少，减轻了对数据内存访问的压力，因此理论性能得到提升。这在内存绑定的解码阶段可以显著提升性能。
* **计算强度从内存绑定转变为计算绑定**。这种转变也减少了对数据内存访问的压力，提升了理论性能。
* **计算强度在量化前后始终保持计算绑定状态**。在这种情况下，性能没有提升。例如，这种情况可能发生在计算绑定的填充阶段或解码阶段批量大小较大时。

<img src="../images/llm_inference_unveiled_paper/figure8.png" width="65%" alt="Inference time of decoding stage for different quantization settings on Llama-2-13b. (Sequence length=1024)">

如图 8 所示，**当批量大小较小时，网络中的层在量化前后都处于内存绑定状态，因此量化可以提升性能并减少网络推理时间**。然而，当批量大小较大时，将网络权重从 4 位压缩到 2 位或 1 位并不会减少推理时间，因为此时网络已经处于计算绑定状态，量化权重变得无效。
<img src="../images/llm_inference_unveiled_paper/figure9.png" width="65%" alt="Llama-2-13b 上不同量化设置的预填充阶段的推理时间。（批次大小 = 1）">
如图 9 所示，**类似地，在填充阶段，当序列长度较小时，填充阶段处于内存绑定状态，这时应用量化可以通过减少内存访问需求来提升性能**。但随着序列长度增加，填充阶段变得更加计算绑定，因此当网络在填充阶段已经是计算绑定状态时，量化权重可能不会显著提高性能。

#### 3.1.2 量化用于压缩预训练 LLMs

##### 量化感知训练（`QAT`）

在量化感知训练（`QAT`）[Choi et al., 2018；Courbariaux et al., 2015；Dong et al., 2019]中，量化过程被无缝地集成到大型语言模型（LLMs）的训练中，使这些模型能够适应低精度表示，从而减轻了精度损失。`LLM-QAT` [Liu et al., 2023b] 创新性地解决了 `LLMs` 训练数据获取的问题，直接**利用预训练模型的输出**来消除大量数据收集的需要。此外，LLM-QAT 将量化范围扩展到包括关键值（KV）缓存在内的更多内容，从而提升了处理速度并支持更长的序列依赖。它成功地将大型 Llama 模型蒸馏为**4位量化的权重和KV 缓存**，显示了 4 位量化 LLMs 的准确性潜力。

为了实现更低位的量化，如低于 2 位，Kim et al. [2023b] 提出了用于 LLMs 的三元量化感知训练的新方法—— token 缩放对数蒸馏（`TSLD`）。这种方法利用适应性知识蒸馏技术，根据 token 的置信度调整对数知识蒸馏，为LLM QAT提供了个性化的指导。除此之外，Shang et al. [2024] **关注于重要的权重**，提出了 `PB-LLM` 中的部分二值化矩阵概念。通过在高位保留这些重要权重，PB-LLM 能够有效地保持在高度量化LLMs中的推理能力。此外，PB-LLM 还研究了通过确定最佳缩放因子来最小化量化误差，这是在高量化情况下保持模型有效性的关键步骤。

##### 训练后量化（PTQ）

训练后量化（`PTQ`）是优化大型语言模型（`LLMs`）的一项关键技术。它是**在模型训练结束后对参数进行量化**，旨在减少存储需求和计算负担，而不需要对模型架构做任何改动或重新训练。

这种方法简单高效，特别适合需要大幅度压缩模型的场景。对于通常包含数十亿参数的 LLMs 来说，量化感知训练（QAT）的训练成本过高，PTQ 因此成为一种更实际的替代方案。不过，量化过程可能带来一定程度的精度下降，但 PTQ 提供了一种无需重大修改或重新训练的简洁解决方案，能显著提高 LLMs 的效率。

**`PTQ` 的多种方案集中在仅对权重进行量化上**，以提升效率。例如:
1. `LUTGEMM` [Park et al., 2023] 通过权重量化和 `BCQ` 格式优化 `LLM` 中的矩阵运算，**降低了延迟并提升了计算效率**。LLM.int8() [Dettmers et al., 2022] 采用 8 位量化，将推理时的 GPU 内存占用减半，同时通过向量量化和混合精度保持精度，实现了在最大 1750 亿参数模型中的高效推理。
3. `GPTQ` [Frantar et al., 2022] 则使用基于二阶信息的逐层量化，成功将每个权重量化至 3-4 位，几乎没有精度损失。
4. Dettmers 和 Zettlemoyer [2023] 研究了模型规模与位精度的平衡，尤其在零样本推理中，发现 4 位精度是最佳选择。
5. `AWQ` [Kim et al., 2023c；Lin et al., 2023] 等创新方法表明，保护少量关键权重能显著减少量化误差。`AWQ` 采用激活感知的策略，**专注于激活值较大的权重通道，并通过每通道缩放实现最佳量化效果**。
6. `OWQ` [Lee et al., 2023] 则研究了激活异常如何加剧量化误差，提出一种混合精度方案，为受异常值影响的权重提供更高的精度。
7. `SpQR` [Dettmers et al., 2023b] 采取独特的方式，将异常权重隔离出来并用更高的精度存储，其他权重则压缩至 3-4 位，达到了更加高效的压缩效果，同时性能几乎没有损失。
8. `QuantEase` [Behdin et al., 2023] 提议使用坐标下降算法来优化网络中的权重，提升量化效率。为实现低于 2 位的量化，QuIP [Chee et al., 2023] 提出了一种新方法，考虑了权重大小的均匀分布，并关注那些不与坐标轴对齐方向的舍入精度
9. `QuIP` 包含一个自适应舍入过程，能够优化量化过程，并利用高效的预处理和后处理技术，通过随机正交矩阵的乘法操作确保权重和 Hessian 的不相关性，从而保障量化效果。
10. **“归一化调整”策略**: [Li 等人[2023a]](https://arxiv.org/pdf/2309.02784) 发现，通过**将量化的激活分布与浮点版本对齐，可以显著提高 LLM 的准确性**。“归一化调整”策略通过精确生成校准数据并设定通道的距离约束，来更新归一化层的权重，从而提升模型的泛化能力。
11. `PB-LLM`[Shang et al., 2024]部分二值化 LLM: 将权重量化推进至低于 2 位，基于二值化方法 [Hubara et al., 2016]。
12. `BiLLM`[Huang et al., 2024] : 紧随 `PB-LLM` 之后, 进一步将量化推进至几乎 1 位。

**`PTQ` 方法除了关注 LLM 权重量化，还有许多 PTQ 方法关注权重与激活同时量化**。比如：
1. `ZeroQuant` [Yao et al., 2022] 结合硬件友好的量化方法与逐层知识蒸馏，将权重和激活量化为 INT8，且几乎不损失精度。
2. `SmoothQuant` [Xiao et al., 2023a] 针对激活量化中的异常值问题，提出了一种逐通道缩放的方法，平滑激活幅度，提升模型对量化的适应能力。
3. `RPTQ` [Yuan et al., 2023c] 关注激活量化的复杂性，指出通道范围不均匀和异常值的普遍存在问题。RPTQ 通过将通道进行聚类量化，减少通道范围差异，并将通道重排序与层归一化和线性层权重结合，最大限度降低开销。
4. `OVP` 异常值-受害者配对量化策略，由 OliVe [Guo et al., 2023a] 提出，其专注于局部处理异常值，在硬件开销较低的情况下带来显著的性能提升。该策略基于异常值的重要性，而忽略了相邻的普通数值。进一步的研究，如 Outlier Suppression+，则解决了某些通道中不对称分布的异常值问题，通过逐通道的移位和缩放操作，均衡异常值的分布，减少问题通道对量化误差的影响。
5. `ZeroQuant-FP` [Wu et al., 2023d] 探讨了**浮点量化**，特别是 FP8 和 FP4 格式的应用，研究表明 FP8 激活量化在 LLMs 中优于传统的 INT8 格式，而 FP4 权重量化则表现得与 INT4 类似。ZeroQuant-FP 通过将缩放因子统一为 2 的幂，并将其限制在同一个计算组内，确保了权重和激活的一致性和量化过程的高效性。
6. `FPTQ` 采用了分层方法来应对不同层的量化难度，由 Li 等人 [2023c] 提出，特别是通过离线的对数激活均衡策略，成功处理了以往难以量化的层。

自 2023 年底以来，token 的长度快速增长，KV 缓存因此占用了更多的内存。例如，Google Gemini 1.5 [Sundar Pichai, 2024] 能够在生产环境中处理多达 100 万个 token，而处理书籍、图像或视频的 LLMs 通常需要数万个 token。因此，**KV 缓存量化的优化变得至关重要**。2024 年的一些最新研究专注于这一问题，例如，Hooper 等人 [2024] 提出了一种实现 1000 万上下文长度 LLM 推理的 KV 缓存量化方案。KIVI [Liu et al., 2024b] 将 KV 缓存量化压缩至 2 位。Yue 等人 [2024] 的 `WKVQuant` 通过联合优化权重和 KV 缓存量化，使得 `W4KV4` 的表现与 `W4` 相同。

<img src="../images/llm_inference_unveiled_paper/figure11.png" width="60%" alt="Llama-2-13b 上不同量化设置的解码阶段内存消耗">

如图 11 所示，通过使用 LLM-Viewer，我们分析了 KV 缓存量化对内存占用的影响，**结果显示当序列长度超过 `50k` 时，KV 缓存占用了大部分内存，而量化处理能够显著降低内存消耗。**

#### 3.1.3 参数高效微调的量化（Q-PEFT）

参数高效微调（`PEFT`）是大型语言模型（`LLMs`）领域的重要话题。其中最受欢迎的方法之一是**低秩适应**（`LoRA`）[Hu et al., 2021, Valipour et al., 2022]，其关键思想是将适配器权重分解为两个低秩（因此是参数高效的）矩阵的乘积。LoRA 声称在使用更少可学习参数的情况下，能够与完整微调相媲美。更多关于此适配器的详细信息，请参考综述文章 [Hu et al., 2023]。

除了已有的量化范式外，LLM 效率领域正在兴起一种新的范式：参数高效微调的量化（`Q-PEFT`）。这种方法将量化整合到 LLM 的微调过程中，提供了一种独特而高效的方式，尤其在大模型时代尤为相关。诸如 `PEQA` [Kim et al., 2023a]、DFT [Li et al., 2023e] 和 `QLORA` [Dettmers et al., 2023a] 等开创性工作展示了这种方法的可行性和有效性。
1. `PEQA` 采用了双阶段流程，第一阶段是将每个全连接层的参数矩阵量化为低位整数矩阵，并配以标量向量；第二阶段则专注于为特定下游任务微调标量向量，使得任务特定的调整更加高效。DFT 采用高效的 Lion 优化器，该优化器仅跟踪动量，并为每个参数提供一致的更新幅度，这对稳健的量化有着天然优势；此外，它将所有模型状态量化并以整数值存储，提出了一种针对量化权重的梯度流和参数更新方案。
2. `QLORA` 引入了新的数据类型、双重量化和分页优化器等概念。这些创新旨在有效节省内存，同时保持 LLM 微调的性能。尤其值得注意的是，QLORA 使得在单个 GPU 上进行大模型的微调成为可能，并在 Vicuna 基准测试上取得了最先进的结果，展示了其在平衡内存效率和模型性能方面的有效性。但是，QLoRA 在微调过程中最多只能进行 4 位量化；对于更低位的量化（如 2 位），性能可能会显著下降。
3. `LQ-LoRA` [Guo et al., 2023b] 引入了一种迭代算法，将每个预训练矩阵分解为一个高精度低秩分量和一个高效量化分量。在微调过程中，仅更新低秩分量，而保持量化分量不变。该方法采用了整数线性规划的方法来处理量化分量，允许在给定的内存预算内动态配置量化参数，如位宽和块大小。
4. `Loft-Q` [Li et al., 2023d]: 量化 LLM 的同时为 LoRA 微调建立了一个合适的低秩初始化策略。该策略有效地弥合了量化模型与全精度模型之间的差距，显著提高了下游任务的泛化能力。
5. `QA-LoRA` [Xu et al., 2023c]: 利用了将 LLM 权重量化为低位整数的优势，促进了高效的微调过程。此外，它还生成了轻量化、微调后的模型，避免了通常与后训练量化（PTQ）相关的精度损失。

#### 3.1.4 关于 LLM 量化的讨论

![2022 年至 2024 年 LLM 方法量化的时间线。红色突出显示的方法表示它们属于参数有效微调量化 (Q-PEFT)，绿色突出显示的方法表示它们属于 QAT 相关方法，其他则是基于 PTQ 的方法](../images/llm_inference_unveiled_paper/figure10.png)

图 10 展示了 LLM 量化技术的发展时间轴，重点突出了从后训练量化（PTQ）作为最初的主流方法，逐渐演变为量化感知训练（QAT）和参数高效微调的量化（Q-PEFT）。这种转变反映了研究社区为应对 PTQ 性能瓶颈而作出的调整，标志着 QAT 和 Q-PEFT 成为在追求高效 LLM 推理过程中日益受到关注的领域。

## 4. Algorithmic Methods for Fast Decoding

LLMs have achieved astonishing performance in various text generation tasks. They typically contain the decoder stage that generates tokens one after another, following an autoregressive relation with all preceding tokens. During the decoding of every token, decoder weights have to be repeatedly loaded into memory. As the parameter size of LLM becomes colossal, the decoding process becomes heavily memory-bound [de Jong et al., 2023] and experiences low hardware utilization, leading to severely long latency [Kim et al., 2023d]. This is particularly problematicin real-world applications like ChatBot, where quick and even real-time responses are crucial. Therefore, there is a strong need to optimize the decoding process to improve performance in such applications.

This section focuses on the discussion of prior efforts to reduce the LLM inference cost from an algorithm perspective. Specifically, this section intends to develop the discussion from two directions:
- In section 4.1, for every single token decoded (fixed #tokens decoded), how to utilize the minimum number of parameters of the LLM.
- In section 4.2, for every single forward propagation of the LLM (fixed #parameters used) how to decode the maximum number of tokens.

### 4.1. Minimum Parameter Used Per Token Decoded

Interestingly, Simoulin and Crabbe´ [2021] has shown that although language models tend to have a huge number of parameters, not all parameters are needed to generate the accurate tokens. LLM inference latency can be reduced by selecting only a subset of necessary parameters to use (load) per input token and still preserving the decoded token’s accuracy. In this section, we look at input-dependent dynamic weights dropping scheme for LLM from three different perspectives: 4.1.1 looks at early exiting, or dynamically choosing weights in the layer, depth, dimension; 4.1.2 introduces methods that dynamically detects sparsity in the width dimension of the LLM, pruning out heads and MLP columns; 4.1.3 present Mixture-of-Experts (MoE) which pretrains a sparse model and chooses the correct experts for the different input during runtime.

#### 4.1.1 Early Exiting

Early exiting (or layer skipping) has been a well-explored idea in various network architectures, particularly for the encoder-only models [Baier-Reinio and Sterck, 2020, Houet al., 2020, Li et al., 2021, Liu et al., 2020, 2022, Schuster et al., 2021, Schwartz et al., 2020, Stickland and Murray, 2019, Xin et al., 2020, Zhou et al., 2020, Zhu, 2021]. 

Early exiting for decoder architecture requires consistency and quality retaining on the sequence level where each token depends on the previous tokens, which are considerations lacking in the previous abundant encoder-only earlyexiting literature. The decoder contains layers of identical structure. Benefiting from this trait, the output hidden states of every layer can be used to pass in the LM Head to get a probability distribution prediction of the next token decoded. Geva et al. [2022] and Simoulin and Crabbe´ [2021] observe that for some tokens, the hidden states saturate during the intermediate layers. In other words, early exiting in the middle would, for some tokens, output the correct top-1 prediction as running through the full model. This observation lays the basis for the success of decoder early exiting methods.

Elbayad et al. [2020] conducts an early effort for efficient machine translation tasks to use early exiting on the decoder architecture. It proposes a general approach to follow. Shown in Figure 12 (b), during the forward propagation, after every layer, there is an internal confidence function, usually a fixed metric or an MLP with a small number of layers, that computes a confidence score based on the hidden states on how likely it is to saturate at the current layer. The score is used to decide whether to exit through some carefully designed criteria. The LM Head is then used to output the next token-predicted probability distribution. Due to the high similarity of the newer follow-up works, we extend the discussion by looking at the key challenges of designing early exiting schemes for language models, where they introduce different novel techniques. 

**Modeling the Confidence of Saturation**. CALM [Schuster et al., 2022] studies three different ways to output the confidence score to exit: the softmax response, or the difference between the top two values after the softmax; the saturation of hidden states, or the cosine similarity between the current layer’s hidden states with the last layer; the output of a linear classifier inserted to every layer. The linear classifier is trained by simply using a cross-entropy loss to align MLP output when inputting the hidden states with whether the top-1 token decoded as exiting on the current layer matches the top-1 token decoded of the full model. The experiments presented suggest that despite not being the most accurate predictor, the classifier method reaches the optimal trade-off between additional FLOPs overhead with prediction accuracy on score generating. Building up from CALM, [Bae et al., 2023] observed that when consistently exiting from shallow layers will result in an abnormally long length. Also, the confidence score computation on every layer injects high overhead and diminishes the benefit of early exiting. Therefore, it proposes to only have
two choices for early exiting: either exit from the so-called ”shallow module” or a group of shallow layers, or go all the way to the full model, or ”deep module”, drastically reducing the number of classifiers needed inside the model. Such design enables it to achieve more speedup than CALM, reaching 2x for certain tasks. On the other hand, ConsistentEE Zeng et al. [2023] proposes a different method to predict when to exit. It uses an RL policy network that is iteratively trained with the per-layer output classifier head. The policy networks are trained with the goal of balancing the optimization of both efficiency (the early layer receives rewards) and accuracy (the reward function has a term that is the early exit output CE loss).

**Early Exit Criteria**. CALM Schuster et al. [2022] proposes a distribution-free calibration technique that uses the fixed sequence testing procedure (Family-wise Error Rate procedure) to output the suitable threshold. The threshold is exponentially decreasing to allow more aggressive exiting for tokens later in the sequence. Bae et al. [2023], on the other hand, observes that the pattern of confidence criteria resembles a beta distribution and uses the on-the-fly data to update a beta distribution model through MLE and use such probability model to guide its decision. Zeng et al. [2023] bypasses this issue by letting the policy network directly output the exit decision.

**Hidden States Propagation**. Hidden states of the skipped layers can pose a technical challenge. As shown in the 12 (b), the token position at ”school” exits later than previous tokens. However, the last self-attention layer doesn’t have the previous key-value pairs of the previous early exited tokens. Elbayad et al. [2020] and Schuster et al. [2022] proposes the ”hidden states propagation” technique. For example, the hidden states of token ”Max” at the exited layer l1 are stored. When the later token ”school” reaches deeper layer l2, the hidden state for ”Max” is copied for all layers between l1 and l2, and the key-value pairs are then computed on the copied hidden states. Basically, to approximate the deep layer’s hidden state with the ones from the early layer. Later works Bae et al. [2023] and Ding et al. [2023] found that state propagation leads to performance degradation. Since LLM inferences are dominated mostly by memory loading, computation is relatively ”free”. These two methods proposed to recompute the later hidden states directly on the fly. Chen et al. [2023b] proposes to run the full large model in parallel to the early exit stream to efficiently parallel the computation of the missing kv cache. Din et al. [2023] conducts a systematic study on using a linear network to jump across layers for transformer architecture and shows that linear layers can be added to effectively bridge the performance gap between directly copying and computing the hidden states with low memory and compute cost.

#### 4.1.2 Contextual Sparsity

While early exiting aims to select parameters on the depth dimension, some techniques have also been proposed to exploit the dynamic sparsity on the width dimension. Deja Vu Liu et al. [2023c] conducts a comprehensive study on dynamic sparsity on the LLM width dimension. The paper reveals that contextual sparsity can go up as high as 80%, meaning that the majority of the weights can be left out while still preserving the original model performance. However, the chosen weights are dynamic and different for different input tokens. The paper formulates this problem as a nearly neighbor search problem that for a given hidden state from the embedding layer of previous layers, how to find the attention heads and the MLP columns that are the most similar to these tokens. To save compute, the paper proposes to train a small MLP network as the Sparse Predictor in front of the Multi-Head Attention (MHA) and the Feed-Forward Networks (FFN) of the LLM, shown in Figure 12 (c). 

By using only a subset of weights and reducing the memory IO overhead, Deja Vu manages to achieve over 2x speedup of LLM inference. Building on Deja Vu, PowerInfer (Song et al. [2023]) brings the contextual sparsity finding to the LLM inference across heterogeneous devices (CPUs and GPUs). PowerInfer discovers a substantial portion of weights are heavily used and activated in the input-independent setting, thus stored on the GPU memory, while others are on the CPU memory. Then, to specifically find the weights to use for a given input token, it trains a smaller sparse prediction than Deja Vu. To make the sparse predictor, it initializes the sparse predictor to have a dynamic structure and iteratively trains and modifies the sparse predictor. To better do inference of the model deployed on the mixed CPU and GPU environment, it introduces a novel memory placement scheme and implements a vector-based sparse computation library. 

Concurrently, MatFormer (Devvrit et al. [2023]) studies the problem of LLM deployment on various heterogenous devices of different hardware capabilities. They added dynamic structure only on the FFN, which occupies 60% of total weights. The model is specially trained so that during inference, based on the target hardware properties, MLP layers are sampled on the row dimension to give a model of various sizes with reasonable performance. To diversify the model size selection, it imposes a Mix’n’Match method to choose different settings for different layers, so combined would give a more variable model size.

#### 4.1.3 Mixture-of-Expert Models

Language Models, especially transformer architecture, exhibit strong power-law scaling (Kaplan et al. [2020], Hoffmann et al. [2022]) of performance when the training dataset is scaled up. On the other hand, though bringing strong performance gain, the large parameter count makes
training and inference of the model inefficient. 

**The mixture of expert (MoE) technique is a well-studied topic (Yukselet al. [2012]) that effectively decouples the parameter count of the model and the computation FLOPs required by the model training and inference, thus bringing huge gains of efficiency under certain conditions**. Further, MoE is shown to effectively scale up the language model size and increase its performance without the concern of increasing compute during inference (Lepikhin et al. [2020], Fedus et al.[2021]). Shown in Figure 12 (d), an expert network is inserted into the transformer architecture to replace the FFN layers. Also, a gating function is introduced between the Multi-Head Attention and the expert network which aims to select the best-fit expert or experts for the given input token. For in-depth analysis and discussion about the MoE scaling generalization, routing algorithms, training techniques, etc., we refer the readers to the survey on Sparse Expert Models (Fedus et al. [2022]). 

Although both rely on the input token to determine sparse structure, We deliberately separate MoE and the contextual sparsity techniques because the latter operates on pre-trained dense language models and exploits the sparsity from the dense neural networks, while the prior trains a sparse model from the beginning. More recently, MoE techniques have achieved substantial success. Sparse Mixer (Lee-Thorp and Ainslie [2022]) brings 89% and 98% speedup in both training and inference to BERT (Devlin et al. [2019]) models. Du et al. [2022] uses only 49% FLOPs but beats GPT-3 (Brown et al. [2020]) in performance. ST-MoE (Zoph et al. [2022]) brings MoE to the encoder-decoder models, even becoming the state-of-theart model for many reasoning and generation tasks. STMoE, using 20x and 40x fewer FLOPs in training and inference, beats 540B PaLM (Chowdhery et al. [2022]) in performance. Mixtral 8x7B (Jiang et al. [2024]), while only actively using 13B parameters during inference, performs on par with Llama2-70B models (Touvron et al. [2023b]) across various evaluation benchmarks.

Besides, various attempts have been made to optimize MoE model inference. Kossmann et al. [2022] builds an efficient compiler library RECOMPILE for MoE models that introduce dynamic recompiling and optimization according to varying inference batch sizes. Rajbhandari et al.[2022] extends the ZeRO distributed inference method to MoE models. Jawahar et al. [2023] conducts Neural Architecture Search (NAS) on the expert network architecture. Yi et al. [2023] deploys large MoE language models on the edge devices. It optimizes the deployment around the find ing that some neurons are much more heavily used in the MoE models than others.

#### 4.1.4 Roofline Model Analysis for Dynamic Parameter

Reducing The Minimum Parameter Used Per Token Decoded methods simultaneously decrease computational and memory access overhead. From the viewpoint of roofline model, these methods result in small changes to the arithmetic intensity of each operator and the type of bound.

For Early Exiting or Layer Skipping methods, entire Transformer layers are skipped, leading to a proportional reduction in overall computation, memory access, and inference time. In other words, the inference time decreases proportionally to the number of layers skipped in the network. However, for methods like Contextual Sparsity and Mixture of Experts, the arithmetic intensity varies across different operations. Consequently, dynamically choose to activate these layers leads to varying reductions in computation and memory access, resulting in different impacts on the overall inference time.

### 4.2. Maximum Tokens Decoded Per LLM Forward

Propagation Another angle to reduce the latency of LLM inference is to relax the LLM from the limitation of autoregressive decoding and have more than one token decoded per one LLM forward propagation. We look at two ways to achieve it: computationally efficient draft model to propose candidates for the next few token positions, while the LLM is used to evaluate the draft model’s proposed draft tokens, instead of generating next tokens. On the other hand, 4.2.2 presents works that enable the LLM to directly decode multiple tokens from a single forward propagation. Due to some methods combining the benefits from both directions and lying in the middle, we manually add a distinction just for the sense of nomenclature that speculative decoding methods here all have the draft model to be in the transformer architecture.

#### 4.2.1 Speculative Decoding

Due to the demanding memory loading challenges and autoregressive properties, LLMs are inefficient in inference. However, models that are much smaller in size are shown (Kim et al. [2023e]) to have the ability to decode the correct sequences as the LLM, as long as some key tokens in the sequence of the small model generation are corrected. Then, shown in Figure 13 (a), when the small model is asked to infer (speculate) and output a sequence of draft tokens, memory loading of model weights is less of a problem, resulting in much higher utilization in hardware computation units. To ensure the quality of the text generated by the small model, the LLM can ”periodically” evaluate and correct tokens from the small model’s draft. Then, although the large model needs to sometimes evaluate the wrong draft tokens, potentially leading to larger FLOPs spent than LLM autoregressive decoding, the memory loading of weights is parallelized on the token dimension and drastically reduces the memory IO overhead. Since the LLM inference is memory bottlenecked, the speculative decoding will potentially reduce the LLM inference latency greatly.

**LLM Distribution Preserving** During early exploration of this idea, two different paths emerged concurrently. Kim et al. [2023e] proposed to have the small model speculate and generate draft tokens until the token decoded confidence falls below a threshold. Then, the small model ”fallback” to the large model to evaluate the draft tokens generated and hand over to the small model. Some of the tokens are rejected, so the large model asks the small model to ”roll back” these wrong tokens and resume speculating. 

In the paper’s setting, all decoding is ”greedy”. The paper show that the large and small model pair can generate text with quality on par with the original large model autoregressive generated text. However, Leviathan et al. [2023]and Chen et al. [2023a], upon the small model speculate paradigm, points out a technique of resampling that at the position where the LLM rejects the small model’s prediction that provably enables the large and the small model predictions to be in the same probability distribution as the large model’s autoregressive generation. The following techniques generally follow the paradigm of speculating then evaluating and resampling to preserve the LLM autoregressive decoding quality while enabling speedup.

**Building a Tree of Draft Tokens** Since the LLM generates in the autoregressive order, every token is dependent on all previous tokens generated, and the length of the accepted tokens in the small model’s draft is usually modest and bounded. It is exponentially more difficult to speculate on tokens more distant in the future. For example, if the small model is asked to output the length m draft sequence, and the LLM accepts n, n < m, the (m - n) tokens are automatically discarded. Thus, the speedup ratio of speculative decoding is modest, since every LLM forward leads to only a limited number of tokens being decoded. There are two ways to improve the speedup of speculative decoding. First, Sun et al. [2023b], Miao et al. [2023b], and Xu et al. [2023a] all proposed to boost the draft on the batch size direction, or letting the small model sample multiple plausible draft sequences for the LLM to evaluate in parallel. 

Specifically, Sun et al. [2023b] proposes a way and theoretical guarantees for the LLMs to batch verify and resample from the multiple small model drafts so that the LLM distribution is preserved and no loss of generation quality is incurred. The paper first connects speculative decoding to the broader problem of discrete optimal transport. The small model is asked to sample multiple draft sequences using topk sampling. Based on the properties of the discrete optimal transport, finding the optimal method to evaluate and resample becomes finding the optimal transport path. On the other hand, besides from maintaining the speculative decoding consistency of draft trees, Miao et al.[2023b] constructs the token tree not based on the top predictions from the small draft model, but based on multiple diversely trained small draft models, each running in parallel and output diverse but powerful draft sequences. 

The paper proposes a novel draft token tree construction algorithm that builds a tree of candidate tokens based on the diverse draft sequences through predefined expanding and merging schemes. Then, the large model is asked to parallel verify the constructed tree using a carefully designed tree attention to maximize the reuse of the key-value cache and maintain a tree-based causal mask. Xu et al. [2023a] innovatively applies the benefit of speculative decoding to edge devices. 

The paper builds an LLM serving engine for the edge, where a smaller draft LLM is sitting consistently in memory, while a larger robust LLM is occasionally loaded in memory to do verification. To boost the acceptance rate from the large LLM, it also constructs a tree using topk tokens. To cater to the edge hardware characteristics, it implements a tree-based parallel verification decoder equipped with masking and a customized large-small LLM computation pipeline to avoid memory contention.

**Knowledge Distillation and Self-Speculative Decoding** Another way to improve the acceptance rate is to improve the small draft model’s ability to align with the LLM’s generation distribution, which can be done through finetuning the small models on corpus generated by the large models with knowledge distillation. Zhou et al. [2023c] establishes a mathematical connection between the acceptance rate and natural divergence between the small model and the LLM: minimizing the divergence is maximizing the acceptance rate. The paper also studies a range of different knowledge distillation losses and shows that adding knowledge distillation brings consistent 10-45% improvement in latency speedup. However, the paper generally finds that the optimal knowledge distillation loss choices vary model by model and should be tuned as a hyperparameter. Liu et al.[2023a] also shows that knowledge distillation boosts the small model training. Besides, the paper brings speculative decoding to the cloud online learning settings. LLM inference is memory-bottlenecked, which means that there is always a surplus in computation resources. The compute can be used to train a draft model continously on server, which brings two benefits: 

1) Continuously training with knowledge distillation boosts its acceptance rate and, thus, reduces the LLM inference latency; 

2) serving input is constantly shifting in domains, and continuous training helps the draft models maintain the strong performance in different domains. Zhang et al. [2023] avoids storing a separate draft model by selectively sampling a smaller draft model from the large model itself. Before deployment, the paper utilizes a Bayesian optimization method to search for a draft model by skipping intermediate layers within the pretrained large model. Besides, it proposes an adaptive threshold selection technique tailored for the decoding of the draft model sampled from the large models.

#### 4.2.2 Parallel Decoding

Alternatively, abundant works have been proposed to enable the large model to directly perform parallel decoding without the help of a small transformer model.

**Simultaneously Predicting Multiple Future Tokens** A wide variety of works are exploring the subject of enabling multiple token predictions directly from one forward pass of the Large Language Model. Stern et al. [2018] pioneers the design of inserting a linear projecting layer between the last hidden states output and the input of the language modeling head to enable multiple future tokens to be projected solely based on the current token’s last hidden states as input. Evaluation is subsequently made by the LLM to decide whether to accept or reject these projected tokens. The proposed technique focuses on the sequence-to-sequence models that have the decoder structure. More recently, Cai et al.[2024] extends the previous work to the decoder-only language models as shown in Figure 13 (b). Besides the last layer projection, to further improve the decoded acceptance rate, the paper proposes to add a tree-based decoding structure and the associate attention mask design to propose multiple drafts simultaneously for the large model to evaluate.

Besides, concurrently Monea et al. [2023] proposes to add several dummy tokens at the end of the input sequence are called ”lookahead embeddings” in work. During the forward pass of each layer, the information of previous prompt tokens and already decoded tokens can be used to parallel decode several consecutive future tokens. To enable this design, the work trains a separate embedding layer that specifically serves these lookahead embeddings. Li et al.[2024] also aims to do parallel decoding with LLM evaluation. Like previous works, it also adds a lightweight structure FeatExtrapolator. Differently, the structure takes both the previous token’s last layer hidden states and the actual decoded token embedding as input and output the hidden states prediction of the next layer. The LM head of the LLM is used, and several tokens are sampled, which are then used to build a decoding tree for the LLM to evaluate in parallel. 

**Retrieval of Frequent N-grams**. Besides directly using the LLM to output several following tokens, some works use the frequently appeared n-grams in natural language to enable multiple future tokens to be generated within one forward pass of the large model. LLMA (Yang et al. [2023]) first observes that the generation tasks tend to ask the LLM to repeat tokens that appeared in the previous contexts. Based on this information, the paper set out to use the decoded tokens and the prompt to do prefix matching with a set of reference documents so that if a repetition occurs, tokens that are repeated can be directly copied to the current place. Then, an LLM will evaluate these found candidate tokens from the previous context to decide whether to use them. He et al. [2023] further extends LLMA and proposes to first construct a database of common phrases based on the LLM pretrained or finetuned dataset and corpus. Then, during decoding, the previous context prompts or tokens are used as the query to be used to retrieve into the constructed database. The candidates retrieved are organized into a prefix tree structure or a trie, which the LLM can then evaluate efficiently. Lan et al. [2023] similarly follows to use the retrieval methods to speed up inference. In contrast, it adds an extra attention layer at the end of the LLM to use the current context represented by the hidden states of the current token as the query to attend to relevant phrases retrieved from the documents of reference and select top phrases based on the attention scores. 

**Hierarchical Structure In Language**. Hierarchical Structure exists in language. For writing a long piece of article, the usual approach is to first write out the general outline of the paper, as in the format of bulletin points. Then, for every bulletin point, arguments can be extended to encapsulate the full intent of the bulletin point. Based on the observation that arguments for different bulletin points are relatively independent in semantics, some methods are proposed to parallelize the generation process for different bulletin points. Skeleton-of-Thoughts (Ning et al. [2023]) proposed to first ask the LLM to generate concise bulletin points for an article, and then collect these bulletin points on the batch axis and feed them into the LLM again as a prompt to ask the LLM to expand the arguments for each bulletin points in parallel. The achieved speedup is approximately 2x, but with the caveat that the method cannot easily generalize to all text generation tasks. More recently, APAR(Liu et al. [2024a]) extends upon this direction. The paper adds specific soft tokens that explicitly inform the LLM of the hierarchical information during the generation. The LLM is further instruct-tuned to incorporate the added special tokens, and the generation is boosted with the Medusa(Cai et al. [2024]) technique to achieve 4x speedup on text generation with the hierarchical structure. 

**Jacobi and Gaussian-Seidel Iterative**.Algorithms Song et al. [2021] pioneers the study of using parallelizable methods to approximate the results from iterative and sequential inferences of fully connected networks or CNNs. Though seemingly in-viable, the paper finds that neural networks can tolerate numerical approximation errors and the data patterns that neural networks learn expose parallel structures to some extent, which makes it possible in some scenarios to parallelize the sequential inference of neural networks. Jacobi and Gaussian-Seidel Algorithms were previously proposed to solve a system of non-linear equations (Ortega and Rheinboldt [2000]) and are shown to effectively parallelize the sequential neural network inference. Santilli et al. [2023] extends the Jacobi and GaussianSeidel algorithms to parallelize the autoregressive decoding in the Machine Translation tasks. Specifically, this work is built on top of the previous Non-Autoregressive Transformers architecture (which we will cover later in the chapter) to enhance the parallel decoding with GS-Jacobi algorithms. The parallel decoding process stops when a [EOS] token is found in the decoded text. Concurrently, Lookahead decoding (Fu et al. [2023a]) shown in Figure 13 (c) extends this method to parallelize the LLM generation of subsequent tokens. Besides using the vanilla Jacobi iterative algorithm, it also boosts its speed with a retrieval-based algorithm to reuse the previously seen n-grams. In addition, it parallelizes the lookahead step and LLM verification step by introducing a carefully designed attention mask to the original LLM model to further improve decoding efficiency.

**Non-Autoregressive Transformers**. For Machine Translation tasks that require autoregressive decoding of the sequence-to-sequence model, Non-Autoregressive Transformers (NAT) has been proposed to iteratively decode all of the output tokens together, as shown in Figure 13 (d).NAT has been relatively well-explored (Gu et al. [2017], Wang et al. [2019], Li et al. [2019], Sun et al. [2019b], Wei et al. [2019], Shao et al. [2020], Lee et al. [2018],Ghazvininejad et al. [2019], Guo et al. [2020], Gu and Kong [2020], Savinov et al. [2021]), and we point the readers to the following survey paper that covers specifically NAT models Xiao et al. [2023c] for an in-depth review and analysis on the subject. Coarsely, the speedup of text decoding comes from making a single forward pass of the decoder output more than one token. The input sequence is first fed into the encoder, which outputs the hidden states that extract the input semantics. 

The output hidden states of the encoder are then used as the condition for the decoder pass. To speed up the text generation, the decoder side relaxes the autoregressive constraints and takes a sequence full of dummy tokens [pad] as the input to start the iterative parallel decoding process. During each iteration, based on the condition set by the encoder output hidden states, some tokens can be confidently predicted, which are unmasked. The sequence is mixed with unmasked decoded tokens and the remaining masked tokens are fed to the decoder again until every token is decoded. The length of the sequence fed into the decoder, or fertility, is usually learned either inside the encoder as a special [CLS] token or by a specialized fertility predictor between the encoder and the decoder. More recently, Savinov et al. [2021] treats the decoder as a diffusion model and trains it to denoise the noisy initial sequence based on the conditions given. However, because of the requirement to use encoder hidden states as the condition for parallel decoding, NAT methods face natural difficulties in extending directly to decoder-only architectures.

## 4. 快速解码的算法方法

大规模语言模型（ LLMs ）在各种文本生成任务中取得了惊人的表现。它们通常包含解码阶段，通过与之前所有生成的词元进行自回归关系，一个接一个地生成词元。在每个词元解码过程中，解码器的权重必须反复加载到内存中。**随着 LLM 的参数规模变得庞大，解码过程主要瓶颈是内存带宽[de Jong et al., 2023](https://arxiv.org/pdf/2212.08153)，硬件的利用率很低，导致延迟严重[Kim et al., 2023d](https://arxiv.org/pdf/2302.14017)**。这在诸如 ChatBot 这样的实际应用中尤其成问题，因为这些应用需要快速甚至是实时的响应。因此，迫切需要优化解码过程，以提高此类应用的性能。

本节集中讨论了从算法角度减少 LLM 推理成本的先前努力。具体而言，本节旨在从两个方向展开讨论：

- 在第 4.1 节中，**对于每个解码的词元（固定的解码词元数），如何利用 LLM 的最少参数**。
- 在第 4.2 节中，**对于每次 LLM 的前向传播（固定的使用参数数），如何解码最多的词元**。

### 4.1 每个词元解码时使用最少的参数

Simoulin 和 Crabbe[2021] 的研究表明，尽管语言模型的参数量庞大，但**生成准确词元并不需要使用所有参数**。通过动态选择并加载必要的参数子集，可以减少 LLM 推理的延迟，同时保持解码词元的准确性。本节将从三个角度探讨 LLM 的输入依赖型动态权重丢弃方法：
- 4.1.1 讨论提前退出机制，在层、深度和维度上动态选择权重；
- 4.1.2 介绍在宽度维度上检测稀疏性的方法，剪除多余的头部和 MLP 列；
- 4.1.3 介绍专家混合模型（`MoE`），它通过预训练生成稀疏模型，并在推理时为不同输入选择合适的专家。

#### 4.1.1 提前退出

提前退出（或跳过层）的概念已经在许多网络架构中得到了广泛研究，尤其是仅编码器的模型 [Baier-Reinio 和 Sterck, 2020，Hou 等, 2020，Li 等, 2021，Liu 等, 2020, 2022，Schuster 等, 2021，Schwartz 等, 2020，Stickland 和 Murray, 2019，Xin 等, 2020，Zhou 等, 2020，Zhu, 2021]。解码器架构中的提前退出与仅编码器的提前退出不同，要求每个词元都依赖于之前的词元，因此需要在序列级别保持一致性和质量。解码器由多个结构相同的层组成，利用这一特性，每层的隐藏状态可以传递给 LM Head ，用于预测下一个词元的概率分布。Geva 等人 [2022] 和 Simoulin 和 Crabbe´ [2021] 观察到，某些词元在中间层时隐藏状态就已经趋于稳定，也就是说，对于这些词元，中途退出和完整模型运行的预测结果是相同的。这一发现为解码器提前退出方法的有效性提供了依据。

Elbayad 等人[2020] 率先提出在机器翻译任务中使用提前退出的方法。它提出了一种通用的处理流程，如图 12（b）所示，在前向传播的每一层之后，有一个置信度函数（通常是固定指标或一个小型 MLP ），用于根据隐藏状态计算当前层的饱和可能性。该置信度分数用于判断是否可以依据设计好的标准退出模型。随后， LM Head 将输出下一个词元的预测概率分布。由于后续工作的设计有很大的相似性，我们通过进一步讨论语言模型中的提前退出方案设计的挑战，并引入了各种创新技术。

![Illustration of Input-Dependent Dynamic Network Technique](../images/llm_inference_unveiled_paper/figure12.png)

**饱和度置信度的建模**。CALM [Schuster et al., 2022] 探讨了三种计算退出置信度得分的方法：softmax 响应（即 softmax 后前两项的差值）；隐藏状态的饱和度（通过计算当前层和最后一层隐藏状态之间的余弦相似度）；以及每层插入的线性分类器的输出。该线性分类器通过交叉熵损失进行训练，目标是让输入隐藏状态时的 MLP 输出与当前层退出时解码的 top-1 词元与完整模型解码的 top-1 词元一致。实验表明，虽然分类器方法并不是最精准的预测工具，但它在 FLOPs 开销和得分生成准确性之间找到了最佳平衡。[Bae et al., 2023] 基于 CALM 的研究进一步指出，总是从浅层退出可能导致异常长的序列生成。此外，每层计算置信度分数的开销较大，抵消了提前退出的性能优势。因此，该方法提出了两个退出选择：一是从“浅模块”或一组浅层退出，二是运行完整模型“深模块”，大幅减少了模型中需要的分类器数量。这种设计比 CALM 提供了更高的加速效果，在某些任务中达到了 2 倍速度提升。另一边，ConsistentEE [Zeng 等人, 2023] 提出了一种不同的退出预测方法，使用强化学习（RL）策略网络，通过逐层输出分类器头部进行迭代训练。该策略网络的目标是平衡效率（早层获得奖励）和准确性（奖励函数中包括提前退出时的 CE 损失项）。

**提前退出标准**。CALM Schuster 等人 [2022] 提出了一种无需分布的校准技术，利用固定序列测试程序（族内错误率程序）来设定适当的退出阈值。该阈值随着序列的推进呈指数下降，以便在后期允许更加激进的退出策略。Bae 等人 [2023] 观察到置信度分布类似于 beta 分布，并使用实时数据通过最大似然估计（MLE）更新 beta 分布模型，从而通过这种概率模型来引导退出决策。Zeng 等人 [2023] 通过让策略网络直接输出退出决定，避免了此问题。

**隐藏状态传播**。跳过的层的隐藏状态会带来技术上的挑战。如图 12（b）所示，“school”词元的退出时间晚于之前的词元。然而，最后一个自注意力层并没有之前提前退出的词元的键值对。为了解决这个问题，Elbayad 等人 [2020] 和 Schuster 等人 [2022] 提出了“隐藏状态传播”技术。例如，“Max”词元在退出层 l1 的隐藏状态会被保存，当后续的“school”词元到达更深的 l2 层时，l1 层的隐藏状态会被复制到 l1 和 l2 之间的所有层，随后基于复制的隐藏状态计算键值对。这种方式实质上是用早期层的隐藏状态来近似深层的隐藏状态。然而，后续研究 [Bae et al., 2023] 和 [Ding et al., 2023] 发现状态传播会导致性能下降。由于 LLM 的推理主要受限于内存加载，计算负担较小，因此这两项研究建议直接实时计算后续的隐藏状态。Chen 等人 [2023b] 提议让完整的大模型与提前退出的推理流并行运行，从而更高效地并行计算缺失的键值缓存。Din 等人 [2023] 进行了一项系统研究，探讨如何在 Transformer 架构中通过线性网络跳跃层，并展示了通过添加线性层，可以在保持低内存和计算成本的同时，有效填补复制和重新计算隐藏状态之间的性能差距。

#### 4.1.2 上下文稀疏性

虽然**早期退出技术侧重于在深度维度上选择参数，但一些技术也致力于利用宽度维度上的动态稀疏性**。Deja Vu [Liu et al., 2023c] 对 LLM 宽度维度的动态稀疏性进行了深入研究。研究表明，上下文稀疏性可以达到 80%，即大多数权重可以被忽略，同时仍能保持模型的原始性能。然而，这些权重是动态的，对于不同的输入词元会有所不同。论文将这个问题描述为近邻搜索问题，即**在给定的隐藏状态下，如何找到最相似的注意力头和 MLP 列**。为了节省计算资源，论文提出**在多头注意力（MHA）和前馈网络（FFN）前面训练一个小型 MLP 网络作为稀疏预测器**，如图 12（c）所示。

通过只使用一部分权重并减少内存 I/O 开销，Deja Vu 实现了 LLM 推理速度提升超过 2 倍。基于 Deja Vu 的研究，PowerInfer [Song et al., 2023] 将上下文稀疏性应用于异构设备（如 CPU 和 GPU）上的 LLM 推理。PowerInfer 发现，在输入无关的设置下，大量权重被频繁使用并激活，因此被存储在 GPU 内存中，而其他权重则存储在 CPU 内存中。为了专门为特定输入词元找到使用的权重，它训练了一个比 Deja Vu 更小的稀疏预测器。为制作稀疏预测器，它初始化了一个动态结构的稀疏预测器，并进行迭代训练和调整。为了在混合 CPU 和 GPU 环境中更好地进行推理，它引入了一种新型的内存分配方案，并实现了基于向量的稀疏计算库。

与此同时，MatFormer [Devvrit et al., 2023] 研究了 LLM 在各种异构设备上的部署问题。他们在 FFN 上添加了动态结构，FFN 占总权重的 60%。模型经过特别训练，以便在推理过程中，根据目标硬件的特性，MLP 层在行维度上进行采样，从而生成具有合理性能的不同大小模型。为了增加模型尺寸的多样性，他们采用了 Mix’n’Match 方法，为不同层选择不同的配置，以此组合出更具变化性的模型尺寸。

#### 4.1.3 专家混合模型

`transformer` 架构的语言模型，在训练数据集规模扩大时表现出强大的幂律缩放效应（Kaplan et al. [2020], Hoffmann et al. [2022]）。但是，大量的参数也使得模型的训练和推理效率变得低下。

**专家混合（MoE）**技术是一种经过深入研究的方法（Yukselet al. [2012]），它**通过将模型参数量与训练和推理所需的计算 FLOPs 解耦**，从而在特定条件下实现显著的效率提升。`MoE` 证明能够有效扩大语言模型的规模，并在不增加推理计算负担的情况下提高其性能（Lepikhin et al.[2020], [Fedus et al.[2021]](https://arxiv.org/pdf/2101.03961)）。如图12（d）所示，专家网络被插入到 `transformer` 架构中，替代了 `FFN` 层。同时，在多头注意力和专家网络之间引入了一个门控函数，用于选择最适合当前输入词元的专家。有关 MoE 的扩展通用性、路由算法、训练技术等的详细分析，请参考稀疏专家模型的综述（[Fedus et al. [2022]](https://arxiv.org/pdf/2209.01667)）。

**尽管 MoE 和上下文稀疏性技术都依赖于输入词元来确定稀疏结构**，但我们有意将它们分开讨论。上下文稀疏性技术操作于预训练的稠密语言模型，并利用稠密神经网络中的稀疏性，而 **MoE 从一开始就训练一个稀疏模型**。最近，MoE 技术取得了显著成功。例如，Sparse Mixer（Lee-Thorp 和 Ainslie [2022]）为 BERT（Devlin et al. [2019]）模型在训练和推理中带来了 89% 和 98% 的速度提升。Du et al. [2022] 使用了仅 49% 的 FLOPs，却在性能上超越了 GPT-3（Brown et al. [2020]）。ST-MoE（Zoph et al. [2022]）将 MoE 引入编码器-解码器模型中，成为许多推理和生成任务的最先进模型。ST-MoE 在训练和推理中使用了 20 倍和 40 倍更少的 FLOPs，在性能上超越了 540B PaLM（Chowdhery et al. [2022]）。Mixtral-8x7B（Jiang et al. [2024]）在推理时仅活跃使用了 13B 参数，但在各种评估基准中表现与 Llama2-70B 模型（Touvron et al. [2023b]）相当。

此外，为优化 MoE 模型推理，各种尝试也在进行中。Kossmann et al. [2022] 为 MoE 模型构建了一个高效的编译器库 RECOMPILE，实现了根据不同推理批量大小进行动态重新编译和优化。Rajbhandari et al. [2022] 将 ZeRO 分布式推理方法扩展到了 MoE 模型。Jawahar et al. [2023] 对专家网络架构进行了神经架构搜索（NAS）。Yi et al. [2023] 在边缘设备上部署了大型 MoE 语言模型，并针对发现 MoE 模型中的某些神经元比其他神经元使用更多的情况优化了部署。

#### 4.1.4 动态参数的 Roofline 模型分析

减少每个解码词元所需的最小参数的方法同时减少了计算和内存访问的开销。从 Roofline 模型的角度来看，这些方法对每个操作的算术强度和类型界限的影响较小。

对于早期退出或层跳过方法，整个 `transformer` 层被跳过，从而使整体计算、内存访问和推理时间成比例减少。换句话说，推理时间会与跳过的层数成正比地减少。

然而，对于上下文稀疏性和专家混合这样的方法，不同操作的算术强度会有所不同。因此，动态选择激活这些层会导致计算和内存访问的减少程度不同，从而对整体推理时间产生不同的影响。


### 4.2 最大解码令牌数每次 LLM 前向传播

另一种降低 LLM 推理延迟的方法是解除 LLM 的自回归解码限制，**使其能够在一次前向传播中解码多个令牌**。我们探索了两种实现方法：一种是使用计算效率高的草稿模型（`draft model`）提出**接下来的数个候选 tokns**，LLM 则用来评估这些候选 `tokens`，而不是直接生成下一个令牌。另一方面，4.2.2 节介绍了一些技术，使得 LLM 能够通过一次前向传播直接解码多个令牌。由于一些方法结合了这两种方向的优点并介于其中, 我们为了命名的清晰性手动添加了一个区分，即这里的推测解码方法都具有基于 `transformer` 架构的草稿模型。

#### 4.2.1 推测解码

由于内存加载的挑战和自回归的特性，LLM 的推理效率较低。然而，有研究（Kim 等人 [2023e]）表明，即使是更小的模型，只要对小模型生成序列中的一些关键令牌进行纠正，它们也能解码出与 LLM 相同的正确序列。如图 13(a) 所示，当小模型被要求推测并生成一系列草稿令牌时，模型权重的内存加载问题较小，从而显著提高了硬件计算单元的利用率。为了保证小模型生成的文本质量，LLM 可以“定期”评估并修正小模型的草稿令牌。尽管大模型有时需要评估错误的草稿令牌，可能导致比 LLM 自回归解码更多的 FLOPs 开销，但由于在令牌维度上并行处理内存加载，内存 IO 开销显著减少。**因为 LLM 推理受限于内存瓶颈，推测解码有可能大幅降低 LLM 的推理延迟**。

![并行解码方法演示](../images/llm_inference_unveiled_paper/figure13.png)

**LLM 分布保持**: 

在对这思想的早期探索中，出现了两条不同的路径。Kim 等人[2023e] 提议让小模型推测并生成草稿令牌，直到令牌解码的置信度低于阈值。之后，小模型将“回退”到大模型来评估这些草稿令牌，并将其交给小模型。一些令牌被拒绝，大模型要求小模型“回滚”这些错误的令牌并继续推测。

在研究的设置中，所有解码过程都是“贪婪”的。研究表明，大模型和小模型的组合可以生成与原大模型自回归生成文本质量相当的文本。然而，Leviathan 等人 [2023] 和 Chen 等人 [2023a] 在小模型推测范式上提出了一种重新采样技术，这种技术在 LLM 拒绝小模型预测的位置，能够使大模型和小模型的预测与大模型自回归生成保持一致。接下来的技术通常遵循推测、评估和重新采样的范式，以保持 LLM 自回归解码的质量，同时提高速度。

**建立草稿令牌树**

由于 LLM 按自回归顺序生成令牌，因此每个令牌都依赖于之前生成的所有令牌，而小模型草稿中的接受令牌长度通常较为适中且受限。推测较远未来的令牌会变得指数级更困难。例如，如果小模型被要求生成长度为 $m$ 的草稿序列，而 LLM 只接受了 $n$ 个令牌（其中 $n < m$），则 ($m - n$) 个令牌会被自动丢弃。因此，推测解码的加速效果有限，因为每次 LLM 前向传播只能解码出有限数量的令牌。有两种方法可以提高推测解码的加速效果。首先，Sun 等人 [2023b]、Miao 等人 [2023b] 和 Xu 等人 [2023a] 都提出了通过增加批量大小，或让小模型并行采样多个可能的草稿序列供 LLM 评估来提升加速比率。具体来说：
- Sun 等人 [2023b] 提出了一种方法和理论保障，**使 LLM 可以批量验证和重新采样多个小模型草稿，从而保持 LLM 的分布，并确保生成质量不受影响**。论文首先将推测解码与离散最优传输这一更广泛的问题联系起来。小模型通过 `topk` 采样生成多个草稿序列。根据离散最优传输的特性，找到评估和重新采样的最佳方法就是寻找最优的传输路径。
- 另一方面，Miao 等人 [2023b] 构建的令牌树不是仅基于小模型的 top 预测，而是基于多个经过多样化训练的小模型，每个模型并行运行并生成多样化而有效的草稿序列。他们提出了一种**新颖的草稿令牌树构建算法**，通过预定义的扩展和合并方案，基于这些多样化的草稿序列构建候选令牌的树。然后，大模型使用精心设计的树形注意力机制并行验证构建的树，以最大化关键值缓存的重用，并保持树形因果掩码。
- Xu 等人 [2023a] 创新性地将推测解码的好处应用于边缘设备，构建了一个边缘 LLM 服务引擎。在这个引擎中，较小的草稿 LLM 始终驻留在内存中，而较大的健壮 LLM 则偶尔被加载到内存中进行验证。为了提高大 LLM 的接受率，它还使用 topk 令牌构建了一个树形结构。为适应边缘硬件的特性，它实现了一种树形并行验证解码器，配备了掩码功能和定制的大、小 LLM 计算管道，以避免内存竞争。

**知识蒸馏与自我推测解码**

**另一种提高接受率的方法是通过对小模型进行知识蒸馏，以使其更好地与 LLM 的生成分布对齐**。这可以通过在大模型生成的语料上对小模型进行微调来实现。Zhou 等人 [2023c] 建立了接受率与小模型和 LLM 之间自然发散度的数学关系：最小化发散度即是最大化接受率。论文还探讨了各种知识蒸馏损失，显示知识蒸馏可以带来 10-45% 的延迟加速改进。不过，论文发现不同模型的最佳知识蒸馏损失选择会有所不同，需作为超参数进行调整。Liu 等人 [2023a] 也表明知识蒸馏可以提升小模型训练效果。此外，论文将推测解码引入了云端在线学习环境中。由于 LLM 推理存在内存瓶颈，这意味着计算资源通常过剩。这些计算资源可以用来在服务器上持续训练草稿模型，带来两个好处：
1. 持续的知识蒸馏训练能够提高接受率，从而减少 LLM 推理的延迟；
2. 输入的领域不断变化，而持续训练帮助草稿模型在不同领域中维持强劲性能。Zhang 等人 [2023] 通过从大模型中选择性地抽样出较小的草稿模型，避免了存储独立草稿模型的需要。在部署前，论文利用贝叶斯优化方法在预训练的大模型中跳过中间层，以寻找草稿模型。此外，论文还提出了一种适用于大模型中抽样草稿模型的自适应阈值选择技术。

#### 4.2.2 并行解码

此外，还有许多研究提出了使大模型直接进行并行解码的方案，而无需依赖小型 `transformer` 模型。

**同时预测多个未来令牌**: 很多研究探索了如何通过一次前向传播使大型语言模型能够直接预测多个令牌。

Stern 等人 [2018] 首创性地设计了在最后的隐状态输出与语言建模头输入之间插入线性投影层，以便仅基于当前令牌的最后隐状态来预测多个未来令牌。之后，LLM 会对这些投影的令牌进行评估，以决定是否接受或拒绝这些预测令牌。这项技术主要针对具有解码器结构的序列到序列模型。最近，Cai等人[2024] 将之前的工作扩展到了仅解码器的语言模型，如图 13 (b) 所示。除了最后一层投影外，为了进一步提高解码接受率，论文提出了添加基于树的解码结构及其相关注意力掩码设计，从而同时提出多个草稿供大模型评估。

**通过“前瞻嵌入”进行并行解码**: Monea 等人 [2023] 提出了在输入序列的末尾添加多个虚拟令牌，这些虚拟令牌被称为“前瞻嵌入”。在每层的前向传播过程中，可以使用先前提示令牌和已解码令牌的信息来并行解码几个连续的未来令牌。为实现这一设计，论文训练了一个专门用于这些前瞻嵌入的嵌入层。Li 等人 [2024] 也旨在通过 LLM 评估实现并行解码。与之前的工作类似，它还添加了一个轻量级结构 FeatExtrapolator。不同之处在于，该结构将先前令牌的最后层隐状态和实际解码令牌嵌入作为输入，并输出下一层的隐状态预测。LLM 的头部用于进行多个令牌的采样，这些令牌随后用于构建解码树供 LLM 并行评估。

**检索频繁的 N-grams**: 除了直接使用 LLM 输出多个后续令牌外，还有一些工作利用自然语言中频繁出现的 n-grams，使得在一次前向传播中生成多个未来令牌。LLMA（Yang 等人 [2023]）首先观察到生成任务往往要求 LLM 重复先前上下文中出现的令牌。基于这一信息，论文提出使用解码的令牌和提示进行前缀匹配，利用一组参考文档，这样在出现重复时，可以直接将重复的令牌复制到当前位置。然后，LLM 会评估这些从先前上下文中找到的候选令牌，以决定是否使用它们。He 等人 [2023] 进一步扩展了 LLMA，提出首先基于 LLM 预训练或微调的数据集和语料库构建常用短语的数据库。然后，在解码过程中，将先前的上下文提示或令牌用作查询，以检索构建的数据库。检索到的候选项被组织成前缀树结构或 Trie，LLM 可以高效地对其进行评估。Lan 等人 [2023] 同样使用检索方法加速推理。与之前不同的是，它在 LLM 的末尾添加了一个额外的注意力层，以使用当前令牌的隐状态表示的上下文作为查询，关注从参考文档中检索到的相关短语，并根据注意力得分选择前几个短语。

**语言中的层次结构**: 语言中存在层次结构。在撰写长篇文章时，通常的做法是首先列出文章的总体大纲，例如以项目符号的形式。然后，对于每个项目符号，可以扩展论点以涵盖项目符号的全部意图。基于观察到不同项目符号的论点在语义上相对独立，一些方法提出了并行生成不同项目符号的过程。Skeleton-of-Thoughts（Ning 等人 [2023]）提出首先要求 LLM 为文章生成简洁的项目符号，然后将这些项目符号按批次收集，再次输入 LLM 作为提示，以要求 LLM 并行扩展每个项目符号的论点。实现的加速约为 2 倍，但需注意这种方法无法轻易推广到所有文本生成任务。最近，APAR（Liu 等人 [2024a]）在这一方向上进行了扩展。论文添加了特定的软令牌，明确告知 LLM 层次结构信息，在生成过程中进一步指令调优 LLM，以结合添加的特殊令牌，并利用 Medusa（Cai 等人 [2024]）技术实现了文本生成速度的 4 倍提升。

**Jacobi 和高斯-赛德尔迭代算法**: Song 等人 [2021] 开创性地研究了使用可并行化的方法来近似完全连接网络或卷积神经网络（CNN）的迭代和顺序推理结果。尽管看似不可行，但该论文发现神经网络能够容忍数值近似误差，并且神经网络学习的数据模式在某种程度上暴露了并行结构，这使得在某些情况下能够并行化神经网络的顺序推理。Jacobi 和高斯-赛德尔算法此前被提议用来求解非线性方程组（Ortega 和 Rheinboldt [2000]），并且被证明可以有效地并行化顺序神经网络推理。Santilli 等人 [2023] 扩展了 Jacobi 和高斯-赛德尔算法，以并行化机器翻译任务中的自回归解码。具体而言，这项工作是在之前的非自回归变换器架构（我们将在本章后面介绍）基础上构建的，通过 GS-Jacobi 算法来增强并行解码。并行解码过程在解码文本中找到 [EOS]  token 时停止。同时，Lookahead 解码（Fu 等人 [2023a]）如图 13（c）所示，将此方法扩展到并行化 LLM 后续 token 的生成。除了使用原始的 Jacobi 迭代算法外，它还通过基于检索的算法加速其速度，以重用先前见过的 n-gram。此外，通过在原始 LLM 模型中引入精心设计的注意力掩码，它还并行化了前瞻步骤和 LLM 验证步骤，以进一步提高解码效率。

**非自回归 transformer**。对于需要自回归解码的序列到序列模型的机器翻译任务，提出了非自回归变换器（NAT），如图 13（d）所示，NAT 旨在迭代解码所有输出 token 。NAT 已经得到相对充分的研究（Gu 等人 [2017]，Wang 等人 [2019]，Li 等人 [2019]，Sun 等人 [2019b]，Wei 等人 [2019]，Shao 等人 [2020]，Lee 等人 [2018]，Ghazvininejad 等人 [2019]，Guo 等人 [2020]，Gu 和 Kong [2020]，Savinov 等人 [2021]），我们推荐读者参考以下专门涵盖 NAT 模型的综述论文 Xiao 等人 [2023c]，以获取有关该主题的深入审查和分析。大致来说，文本解码的加速来自于使解码器输出的单次前向传播生成多个 token 。首先将输入序列送入编码器，编码器输出的隐藏状态提取输入语义。

编码器的输出隐藏状态随后用作解码器传递的条件。为了加速文本生成，解码器一方放宽了自回归约束，并以充满虚拟 token  [pad] 的序列作为输入，开始迭代并行解码过程。在每次迭代中，根据编码器输出隐藏状态设置的条件，可以自信地预测一些 token ，这些 token 将被解码。序列与未掩盖的解码 token 混合，并将剩余的掩盖 token 再次输入解码器，直到每个 token 被解码。输入到解码器的序列长度，或称为繁殖度，通常是在编码器内部作为特殊 [CLS]  token 学习的，或者由编码器和解码器之间的专用繁殖度预测器预测。最近，Savinov 等人 [2021] 将解码器视为扩散模型，并训练其根据给定的条件去噪声初始序列。然而，由于要求使用编码器隐藏状态作为并行解码的条件，NAT 方法在直接扩展到仅解码器架构时面临天然的困难。

## 5. Compiler/System Optimization

After model compression and algorithm optimization for LLMs, the next step is to compile and deploy them on hardware devices. To ensure efficient inference of LLMs, there are various compiler optimizations that can be employed. Moreover, due to the increasing scale of LLMs, multiple hardware devices may be required for deployment and execution, forming a complex inference infrastructure system. As a result, system-level optimization for efficient inference has become a hot topic. In this section, we will explore some widely used compiler optimization and system optimization techniques. These include operator fusion, memory management, workload offloading, and parallel serving.

### 5.1. Operator Fusion

Operator fusion is an important compile-time optimization technique in deep learning frameworks to improve computational efficiency. It combines together multiple operators or layers that are directly connected in the computation graph. This eliminates redundant data movement and intermediate representations. For example, a linear operator followed by a SiLU operator can be fused together into a single operator. As shown in Figure 14, this avoids having to store and load the intermediate activations between each operator, reducing both the memory consumption and the memory access. As shown in Figure 15, the roofline model suggests that kernel fusion can increase arithmetic intensity and enhance inference performance in memory-bound areas. However, when operators are already in a computebound area, memory fusion provides little benefit.

While operator fusion can provide significant performance benefits in many cases, it is not applicable to all operators. Operator fusion may not be possible or beneficial for certain operators: (1) Operator fusion requires that the intermediate results of the fused operations are not needed elsewhere in the computation graph. If a subsequent operation depends on the output of an intermediate operation, fusion is not possible without introducing additional complexity or recomputation. (2) Operator fusion can potentially increase the on-chip buffer requirements of the fused operation. If the available on-chip buffer is limited, it may not be feasible to fuse certain operations. (3) Some frameworks or hardware architectures may have limitations or restrictions on which operations can be fused together, depending on their implementation details.

Some compilation tools, such as TVM [Chen et al.,together and replacing them with a fused operator. However, for LLMs, automatically detecting and fusing operators is both unnecessary and complex because LLMs have a fixed architecture. Instead, specific fusion patterns can be used to improve efficiency. For instance, the attention mechanism is an essential part of LLMs. Automatically fusing attention mechanism can be a complex task for compilation tools. FlashAttention [Dao, 2023, Dao et al., 2022] and Flash-Decoding [Dao et al., 2023] proposed fusing the matrix multiplications and softmax operator in self-attention into one operator. This fusion technique eliminates the need to store and load the intermediate attention matrix, which can be very large when the sequence length or batchsize is large. As shown in Figure 16, fusing them can significantly decrease the memory access and inference time. We can observe that there are differences between the prefill stage and decode stage. In the decode stage, the memory access reduction is the same as inference time reduction. However,in the prefill stage, inference time reduction is lower than memory access reduction. This is because some operations in the prefill stage are compute-bound, so reducing memory access by operator fusion provides little benefit.

DeepSpeed-inference [Aminabadi et al., 2022] introduces a technique called Deep-Fusion. It specifically fuses four main regions within a transformer layer: the QKV GeMM and input layer normalization; transposition and attention operations; post-attention layer normalization and intermediate GeMM; bias addition and residual addition. xFormers [Lefaudeux et al., 2022] offers various fused kernels that can enhance the performance of transformers. These include fused softmax, fused linear layer, fused layer norm, and fused SwiGLU. TensorRT-LLM [Vaidya et al.,2023] is another framework that offers a wide range of high-performance fused kernels. It incorporates a powerful pattern-matching algorithm that can detect potential fusions in various LLMs.

In addition to kernel fusion, we can enhance the performance of the LLM by further optimizing operators’ implementation. For example, FlashDecoding++ [Hong et al., 2023] proposes using asynchronized softmax and flat GEMM optimization with double buffering to improve efficiency.

### 5.2. Memory Management and Workload Offloading

When using an LLM to generate responses, the number of input and output tokens can change each time. The length of the user’s input prompt may vary, affecting the length of the sequence in the prefill phase. Additionally, the sequence length increases incrementally during the decode phase as tokens are generated. This means that the shapes of the activations are not fixed like in a normal neural network. How to manage the memory efficiently as the tensor sizes change is a problem. PagedAttention [Kwon et al., 2023] efficiently handles the KV cache by dividing it into blocks. The KV cache of each sequence is divided into blocks, with each block containing the keys and values for a fixed number of tokens. To manage these blocks, a table is used to map the logical blocks of a sequence to the physical blocks in GPU memory. This mapping is similar to how virtual memory works in a CPU’s memory management system.

When the GPU has limited memory capacity and the network is too large to fit, it may be necessary to employ workload offloading to store the network in alternative memory spaces. As depicted in Figure 17, a computer system
consists of various memory spaces, including CPU’s DDR, GPU’s GDDR/HBM, and hard disk. However, these different memory spaces have distinct access bandwidths. Figure 18 illustrates that when the data is offloaded to CPU’s DDR and transferred to the GPU for computation when needed, it is better than performing the computation on the CPU. When the batch size is large enough, the arithmetic intensity increases significantly, allowing the GPU to fully utilize its computation capacity and achieve good results. DeepSpeed-inference [Aminabadi et al., 2022] introduces ZeRO-Inference, which offloads the weights of large models to CPU memory. This mechanism performs well with large batch sizes because the increased batch size increase the computation requirement and make the computation latency overlap the latency of fetching model weights, thereby improving overall efficiency. Huggingface Accelerate [HuggingFace, 2022] can also move certain modules to the CPU or disk if there is not enough GPU space to store the entire model. FlexGen [Sheng et al., 2023] provides a way to explore different ways of offloading computations considering constraints imposed by available hardware resources from the GPU, CPU, and disk. To find the best strategy in terms of throughput, FlexGen employs a linear programming-based search algorithm. Alizadeh et al.[2023] takes advantage of the larger capacity of flash memory compared to DRAM. It efficiently performs inference by storing model parameters in flash memory and transferring them to DRAM when needed.

### 5.3. Parallel Serving

Parallel serving handles multiple user requests to a server at the same time. One goal is to respond to each request quickly. To achieve this, we need to reduce the time it takes to respond to each user, known as the response latency. Another important factor to consider is throughput, which is the number of requests the server can process in a given time. By increasing the server’s throughput capacity, we can serve more users simultaneously, leading to better overall system performance. By increasing the server’s throughput capacity, more users can be served simultaneously, resulting in improved system performance. The serving system should be optimized to maximize throughput, while still ensuring that the response latency is within acceptable limits. Batching is a fundamental approach to improve throughput by processing multiple user requests together. Figure 19 shows that increasing the batch size during the decode stage significantly enhances throughput. However, increasing batch size can increase the response latency and memory consumption.

Several techniques have been proposed to optimize the batching method. For example, ORCA [Yu et al., 2022] introduces continuous batching (also known as iterative or rolling batching) to combine inferences from different
users. SARATHI [Agrawal et al., 2023] employs chunkedprefills and decode-maximal batching. It combines prefill chunks and decode requests to create batches, which increases the arithmetic intensity and improves throughput. Similarly, DeepSpeed-FastGen [Holmes et al., 2024] and LightLLM [ModelTC, 2024] also employ a split and fuse technique.

## 5. 编译器/系统优化

在对大语言模型（LLMs）进行模型压缩和算法优化之后，下一步是将它们编译并部署到硬件设备上。为了确保 LLMs 的高效推理，有多种编译器优化技术可以使用。此外，由于 LLMs 的规模不断扩大，可能需要多个硬件设备进行部署和运行，这形成了一个复杂的推理基础设施系统。因此，系统级优化以实现高效推理已经成为一个重要的研究方向。本节将探讨一些常见的编译器和系统优化技术，包括算子融合、内存管理、工作负载卸载和并行服务。

### 5.1. 算子融合

算子融合是一种重要的编译时优化技术，用于提升深度学习框架的计算效率。它通过将计算图中直接连接的多个算子或层合并在一起，从而减少冗余的数据移动和中间表示。例如，可以将一个线性算子和一个 `SiLU` 算子融合成一个单一的算子。如图 14 所示，这样可以避免在每个算子之间存储和加载中间激活，从而减少内存消耗和内存访问。如图 15 所示，`Roofline` 模型表明，内核融合可以提高算术强度，增强内存受限区域的推理性能。然而，当算子已经处于计算受限区域时，内存融合几乎没有什么好处。

<img src="../images/llm_inference_unveiled_paper/figure14.png" width="60%" alt="线性算子与 SiLU 算子的算子融合的演示">
<img src="../images/llm_inference_unveiled_paper/figure15.png" width="60%" alt="运算符融合的内存受限情况和计算受限情况的演示。">

尽管算子融合可以显著提升性能，但并不是所有算子都适用。某些情况下，算子融合可能无法进行或并不有益：
1) 算子融合要求融合操作的中间结果不在计算图的其他部分使用。如果后续操作依赖于中间操作的输出，融合操作将变得复杂或者需要重新计算。
2) 算子融合可能增加片上缓冲区的需求。如果片上缓冲区有限，某些操作的融合可能不可行。
3) 不同的框架或硬件架构对可以融合的操作有不同的限制或要求，这取决于具体的实现细节。

一些编译工具，如 TVM [Chen et al.]，致力于将多个算子融合为一个。然而，**对于 LLMs，自动检测和融合算子既不必要也复杂，因为 LLMs 的架构是固定的**。相反，可以利用特定的融合模式来提升效率。例如，注意机制是 LLMs 的重要组成部分。自动融合注意机制对编译工具来说可能是一项复杂的任务。FlashAttention [Dao, 2023, Dao et al., 2022] 和 Flash-Decoding [Dao et al., 2023] **提出了将自注意力中的矩阵乘法和 softmax 操作融合为一个算子**。这种融合方法可以消除存储和加载中间注意力矩阵的需求，尤其是在序列长度或批量大小较大时，如图 16 所示，这可以显著减少内存访问和推理时间。我们可以看到，预填充阶段和解码阶段的效果有所不同。在解码阶段，内存访问减少与推理时间减少相同。然而，在预填充阶段，由于某些操作计算受限，通过算子融合减少内存访问的效果较为有限。

<img src="../images/llm_inference_unveiled_paper/figure16.png" width="60%" alt="Nvidia A6000 上 FlashAttention 的内存访问减少和推理时间减少">

DeepSpeed-inference [Aminabadi et al., 2022] 引入了名为 `Deep-Fusion` 的技术，专门融合了 `transformer` 层中的四个主要区域：
- `QKV GeMM` 和输入层归一化；
- 转置和注意力操作；
- 注意力后的层归一化和中间 GeMM；
- 偏置加法和残差加法。

xFormers [Lefaudeux et al., 2022] 提供了各种融合内核，以提高变换器的性能，包括融合 softmax、融合线性层、融合层归一化和融合 SwiGLU。TensorRT-LLM [Vaidya et al., 2023] 也是一个提供高性能融合内核的框架，采用强大的模式匹配算法来检测 LLMs 中的潜在融合机会。

除了内核融合，我们还可以通过优化算子的实现来进一步提升 LLM 的性能。例如，`FlashDecoding++` [Hong et al., 2023] 提出了使用异步 softmax 和双缓冲的平坦 GEMM 优化来提高效率。

### 5.2. 内存管理与工作负载卸载

在使用大语言模型（LLM）生成响应时，每次输入和输出 `token` 的数量可能会发生变化。用户的输入提示长度可能不一样，从而影响预填充阶段的序列长度。此外，解码阶段随着 token 的生成，序列长度也会逐渐增加。这意味着**激活的形状不像在普通神经网络中那样固定**。**如何在张量大小变化时有效管理内存是一个挑战**。`PagedAttention` [Kwon et al., 2023] 通过将 KV 缓存分块来高效处理。每个序列的 KV 缓存被分成多个块，每个块包含固定数量 token 的键和值。为了管理这些块，使用表将序列的逻辑块映射到 GPU 内存中的物理块，这类似于 CPU 中的虚拟内存系统。

<img src="../images/llm_inference_unveiled_paper/figure17.png" width="60%" alt="在典型的计算机架构中，内存系统由不同类型的内存空间组成。">

当 GPU 内存有限且网络规模过大时，可能需要将网络存储在其他内存空间中，称为工作负载卸载。如图 17 所示，计算机系统包含多种内存空间，如 CPU 的 DDR、GPU 的 GDDR/HBM 和硬盘，这些内存空间的访问带宽各不相同。

<img src="../images/llm_inference_unveiled_paper/figure18.png" width="60%" alt="Roofline model for different offload settings。">

如图 18 显示，当数据卸载到 CPU 的 DDR 上并在需要时转移到 GPU 进行计算时，效果优于在 CPU 上计算。**当批量大小足够大时，算术强度显著提高，GPU 能够充分利用其计算能力，从而实现良好的性能**。`DeepSpeed-inference`[Aminabadi et al., 2022] 引入了 `ZeRO-Inference`，将大模型的权重卸载到 CPU 内存中，这在大批量处理时表现良好，因为较大的批量需求提高了计算负荷，使得计算延迟与模型权重提取延迟重叠，从而提升了整体效率。Huggingface Accelerate [HuggingFace, 2022] 也可以将某些模块迁移到 CPU 或硬盘，以应对 GPU 空间不足的问题。FlexGen [Sheng et al., 2023] 提供了一种探索不同计算卸载方式的方法，考虑了 GPU、CPU 和硬盘的硬件资源限制。FlexGen 采用基于线性规划的搜索算法来找到最佳的吞吐量策略。Alizadeh et al. [2023] 利用闪存内存容量大于 DRAM 的优势，通过将模型参数存储在闪存中并在需要时转移到 DRAM 来高效执行推理。

### 5.3. 并行服务

**并行服务**旨在同时处理多个用户请求。其目标之一是快速响应每个请求，这要求降低每个用户的响应时间，即**响应延迟**。另一个关键因素是**吞吐量**，即服务器在特定时间内可以处理的请求数量。提高服务器的吞吐量可以让我们同时服务更多用户，从而提升整体系统性能。**服务系统应优化以最大化吞吐量，同时保持响应延迟在可接受范围内**。**批处理**是一种基本的提升吞吐量的方法，通过将多个用户请求一起处理来实现。如图 19 所示，在解码阶段增加批量大小显著提高了吞吐量。但需要注意的是，增加批量大小可能会提高响应延迟和内存消耗。

<img src="../images/llm_inference_unveiled_paper/figure19.png" width="60%" alt="并行服务设置会影响 Nvidia A6000 GPU 的吞吐量、延迟和内存使用率
(Llama-2-13b)">

为优化批处理方法，已经提出了多种技术。例如:
1. [ORCA [Yu et al., 2022]](https://www.usenix.org/system/files/osdi22-yu.pdf) 引入了**连续批处理**（也称为迭代批处理或滚动批处理），以合并来自不同用户的推理。
2. [SARATHI [Agrawal et al., 2023]](https://arxiv.org/pdf/2308.16369) 采用了**分块预填充和解码最大化批处理**。它将预填充块和解码请求结合形成批次，从而提高算术强度和吞吐量。同样，DeepSpeed-FastGen [Holmes et al., 2024] 和 `LightLLM`[ModelTC, 2024] 也采用了拆分和融合技术。

## 6. Hardware Optimization

Designing hardware to efficiently support inference for LLMs is a challenging task due to the varying arithmetic intensity5 under different inference stages and workload conditions. Specifically, the prefill stage usually leverages GEMM operators to process the batched tokens, which exhibits high arithmetic intensity. On the contrary, the decoding stage calculates output tokens one at a time, which necessitates the use of either GEMV operators or lean GEMM operators to process the attention and FFN layers. These operators are characterized by low arithmetic intensity. Furthermore, the arithmetic intensity can exhibit substantial variation depending on the batch sizes and sequence lengths. For instance, a large batch size could significantly alter the arithmetic intensity, and a long sequence length may increase the memory access overhead of KV-cache reading in each decoding step. This variability introduces additional complexity into the hardware design process, as different stages or configurations may necessitate distinct optimization strategies. Hence, it’s crucial to consider these factors when designing hardware to ensure efficient performance across a wide range of scenarios.

Considering these challenges, careful consideration and optimization of hardware designs are necessary. In this section, we will survey and analyze various hardware optimizations tailored for efficient LLM inference, with a focus on addressing the issues related to varying arithmetic intensity.

### 6.1. Spatial Architecture

The decoding process of LLM involves predicting words one at a time based on previously generated ones. However, this process can be costly, especially during tasks in long sequence generation. This is because the model needs to access large amount of weights and the key-value (KV) cache to generate each token, resulting in low arithmetic intensity.

There are several solutions that have been developed to address this issue. One such solution is the implementation of a ”Spatial Architecture”. In contrast to traditional computer architectures, spatial architectures utilize a different approach to computing. Instead of folding the computation process into multiple interactions between processing elements (PEs) and main memory, spatial architectures distribute the computation across multiple PEs. This design allows for the exploitation of parallelism, as each PE simultaneously performs a portion of the computation. Additionally, the intermediate data flows between the PEs, avoiding writing back to DRAM each time. 

In a spatial architecture, each PE is responsible for a specific portion of the computation. To facilitate efficient communication, data is typically moved between neighboring PEs. This allows for improved performance and efficient utilization of resources. In a spatial setup, each PE has its own direct access to memory. This enables multiple processing units to access memory simultaneously, improving the overall speed at which information can move in and out of memory. This results in enhanced memory bandwidth and overall LLM inference performance. As shown in Figure 20, as the total memory bandwidth increase, the performance for the linear layer in decoding stage can significantly increase.

In one case, Groq employs their LPU [Abts et al., 2022] to create a spatial system for LLM inference. This system achieves a remarkable speed of over 300 tokens per second per user on the Llama-2-70b model [Groq, 2023]. Another example is Graphcore’s Intelligence Processing Unit (IPU), which is another type of spatial architecture that efficiently executes LLMs [Graphcore, 2024].

## 8. Conclusion

In this work, we review on efficient large language model (LLM) inference. For this practice-driven topic, our comprehensive study goes beyond conventional literature reviews by providing both an overview of existing research and the development of a roofline model. Our first step is to develop a roofline model, which enables us to pinpoint bottlenecks in LLM deployments, allowing researchers to resort to more specific deployment strategies. By meticulously assembling the latest developments in the field, our survey spans an array of pivotal areas, including innovations in weight optimization techniques, enhancements in decoding algorithms, as well as advancements in hardware and system-level optimizations. It is important to note that this project will be updated and maintained.

## 6. 硬件优化

为大语言模型（LLM）设计高效的推理支持硬件是一项具有挑战性的任务，因为不同推理阶段和工作负载条件下的算术强度变化很大。具体而言，预填充阶段通常利用 GEMM 操作符来处理批量标记，这表现出高算术强度。相反，解码阶段逐个计算输出标记，这要求使用 GEMV 操作符或精简的 GEMM 操作符来处理注意力和前馈网络（FFN）层，这些操作符的算术强度较低。此外，算术强度还会根据批量大小和序列长度的变化显著波动。例如，大批量大小可能会显著改变算术强度，而较长的序列长度可能会增加每个解码步骤中 KV 缓存读取的内存访问开销。这种变异性给硬件设计过程带来了额外的复杂性，因为不同的阶段或配置可能需要不同的优化策略。因此，在设计硬件时，考虑这些因素以确保在各种场景下的高效性能至关重要。

考虑到这些挑战，必须仔细考虑和优化硬件设计。本节将调查和分析针对高效 LLM 推理的各种硬件优化，重点解决与变化算术强度相关的问题。

### 6.1. 空间架构

LLM 的解码过程涉及基于先前生成的标记逐个预测单词。然而，这个过程可能是昂贵的，特别是在长序列生成任务中。这是因为模型需要访问大量的权重和键值（KV）缓存来生成每个标记，导致算术强度较低。

为解决这一问题，已经开发了几种解决方案。其中之一是实现“空间架构”。与传统计算架构不同，空间架构采用不同的计算方法。它们不是将计算过程折叠到处理单元（PEs）和主内存之间的多次交互中，而是将计算分布到多个 PEs 上。这种设计可以利用并行性，因为每个 PE 同时执行计算的一部分。此外，PEs 之间的数据流动避免了每次都写回 DRAM。

在空间架构中，每个 PE 负责计算的特定部分。为了实现高效的通信，数据通常在相邻的 PEs 之间移动。这可以改善性能并有效利用资源。在空间设置中，每个 PE 直接访问内存。这使得多个处理单元可以同时访问内存，从而提高信息进出内存的整体速度。这导致了更高的内存带宽和整体 LLM 推理性能。如图 20 所示，随着总内存带宽的增加，解码阶段线性层的性能可以显著提高。

<img src="../images/llm_inference_unveiled_paper/figure20.png" width="60%" alt="不同带宽对 roofline 模型和 Llama-2-13b 解码时间的影响">

例如，Groq 使用其 LPU [Abts et al., 2022] 创建了一个空间系统来进行 LLM 推理，该系统在 Llama-2-70b 模型上实现了每秒超过 300 个标记的惊人速度 [Groq, 2023]。另一个例子是 Graphcore 的智能处理单元（IPU），这是一种高效执行 LLM 的空间架构 [Graphcore, 2024]。

## 8. 结论

在这项工作中，我们回顾了高效的大语言模型（LLM）推理。对于这一实践驱动的话题，我们的综合研究超越了传统的文献综述，通过提供现有研究的概述以及屋脊线模型的开发。我们的第一步是开发一个屋脊线模型，这使我们能够识别 LLM 部署中的瓶颈，帮助研究人员采取更具体的部署策略。通过精心整合该领域的最新进展，我们的调查涵盖了多个关键领域，包括权重优化技术的创新、解码算法的改进，以及硬件和系统级优化的进展。值得注意的是，本项目将进行更新和维护。